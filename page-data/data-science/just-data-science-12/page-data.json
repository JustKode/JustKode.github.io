{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/data-science/just-data-science-12","result":{"data":{"markdownRemark":{"html":"<h2>Generative Adversarial Network</h2>\n<p>안녕하세요? 오늘은 <strong>GAN</strong>, <strong>Generative Adversarial Network</strong>에 대해서 알아 보도록 하겠습니다.</p>\n<p><strong>Generative Adversarial Network</strong>을 직역하면 <strong>생성적 적대 네트워크</strong> 입니다. 말이 좀 어려워서 이게 뭘 이야기 하는지 직관적으로 파악할 수 힘든데, 쉽게 이야기 하면 <strong>경찰과 도둑</strong>을 생각 하시면 편합니다.</p>\n<p>특정 상황을 빗대어 비유 해 보겠습니다.</p>\n<blockquote>\n<p>어떤 위조 지폐를 만들어 내는 범죄자가 있고, 이를 잡으려는 경찰관이 있다고 가정 하겠습니다. 처음에는 범죄자가 위조 지폐를 찍어 내는 데 미숙한 나머지 쉽게 위조 지폐를 식별 할 수 있었습니다. 하지만, 나중에는 위조 지폐를 찍어내는 기술이 발달 하여, 실제 지폐와 유사한 지폐를 만들어 낼 수 있게 되었고, 경찰관은 이를 알아 볼 수 없었습니다. 하지만, 여기서 끝났을까요? 또, 경찰관 쪽의 기술이 발달 하여, 그렇게 어렵게 만들어 진 위조 지폐를 검출하는 기술이 만들어 지게 됩니다.</p>\n</blockquote>\n<p align=\"center\">\n       <img src=\"/post_image/just-data-science/12-01.png\" width=\"70%\"/>\n       <p align=\"center\" style=\"color:#888888; font-size: 12px;\">\n              출처: https://files.slack.com/files-pri/T25783BPY-F9SHTP6F9/picture2.png?pub_secret=6821873e68\n       </p>\n</p>\n<p>위와 같은 상황이 반복되게 된다면 어떻게 될까요? 그러면 나중에는 <strong>경찰관, 범죄자 모두 기술 경쟁</strong>으로 인해 서로 엄청난 기술이 만들어 지게 됩니다. <strong>GAN</strong>은 이러한 원리를 이용하여 학습 합니다.</p>\n<p>우리는 <strong>GAN</strong>에서 <strong>두 가지 모델</strong>을 이용하여 학습 합니다. 바로 <strong>Generator</strong>와 <strong>Discriminator</strong>입니다. <strong>Generator</strong>는 위의 사례에서 이야기 한 범죄자 역할을 합니다. 가짜 정보를 만들어 내는 역할을 하죠. <strong>Discriminator</strong>는 위의 사례에서 경찰관 역할을 담당 합니다. 가짜 정보를 검출 하는 역할을 합니다.</p>\n<p>손실 함수는 다음과 같습니다.</p>\n<p align=\"center\">\n       <img src=\"/post_image/just-data-science/12-02.png\" width=\"70%\"/>\n       <p align=\"center\" style=\"color:#888888; font-size: 12px;\">\n              손실 함수\n       </p>\n</p>\n<p>여기서 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>D</mi></mrow><annotation encoding=\"application/x-tex\">D</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span></span></span></span>는 <strong>Discriminator</strong>, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>G</mi></mrow><annotation encoding=\"application/x-tex\">G</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">G</span></span></span></span>는 <strong>Generator</strong>를 의미 합니다. <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>D</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">D(x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span>는 <strong>Discriminator</strong>가 실제 샘플 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">x</span></span></span></span>를 보고 판별하는 확률 예측값이고, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>G</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">G(z)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">G</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.04398em;\">z</span><span class=\"mclose\">)</span></span></span></span>는 <strong>Generator</strong>가 만들어 낸 값입니다. <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>D</mi><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">D(G(z))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">G</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.04398em;\">z</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span></span></span></span>는 <strong>Discriminator</strong>가 가짜 샘플 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>G</mi><mo stretchy=\"false\">(</mo><mi>z</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">G(z)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">G</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.04398em;\">z</span><span class=\"mclose\">)</span></span></span></span>를 보고 판별하는 확률 예측값입니다. 우리는 이 손실 함수를 다음과 같이 해석 할 수 있습니다.</p>\n<p>일단, <strong>Discriminator</strong> 입장 에서는, 가짜 데이터를 잘 찾을 수록, 더 좋은 <strong>Discriminator</strong> 이라고 볼 수 있습니다. 그러면 <strong>Discriminator</strong> 입장 에서는 위에 있는 손실 함수의 값을 높여야 하는 것이지요.</p>\n<p>역으로 <strong>Generator</strong> 입장에서는, 가짜 데이터로 더 잘 속일 수록, 더 좋은 <strong>Generator</strong>라고 볼 수 있습니다. 그러면 <strong>Generator</strong>는 위에 있는 손실 함수의 값을 낮춰야 하는 것이지요.</p>\n<p>이렇게 자강두천의 싸움이 지속 되면, <strong>Discriminator</strong>는 지도 학습의 방식으로, <strong>Generator</strong>는 비지도 학습의 방식으로 서로 더 강력한 모델을 만들어 나갈 수 있습니다.</p>\n<p align=\"center\">\n       <img src=\"/post_image/just-data-science/12-03.jpg\" width=\"50%\"/>\n       <p align=\"center\" style=\"color:#888888; font-size: 12px;\">\n              Discriminator, Generator 싸움 수준 실화냐? 가슴이 웅장해진다...\n       </p>\n</p>\n<p><strong>Generator</strong>는 어떤 식으로 데이터를 만들어 낼 까요? 입력 값에 <strong>정규 분포 값으로 초기화 된 벡터</strong>를 넣음으로 가능합니다. 데이터가 나올 확률은 학습 데이터에서 사용 한 확률 분포와 유사 합니다. <strong>GAN</strong>의 목표는 <strong>Generator</strong>에서 만들어 내는 데이터가, 실제 데이터의 확률 분포와 유사하게 만드는 것을 목표로 합니다.</p>\n<p align=\"center\">\n       <img src=\"/post_image/just-data-science/12-04.png\" width=\"70%\"/>\n       <p align=\"center\" style=\"color:#888888; font-size: 12px;\">\n              Ian Goodfellow의 논문에 수록된 그림.\n       </p>\n</p>\n<p>여기서 우리가 모델을 학습 시키는 데에 주의 해야 할 점이 있다면, 한 쪽의 성능이 우월하게 되면, <strong>Gradient Vanishing</strong> 문제로 인하여, 학습이 잘 진행이 되지 않는다는 단점이 있습니다. 그리하여, 우리는 <strong>학습 횟수 및 학습률</strong>을 잘 조정 해야 합니다.</p>\n<h2>Code Implementation</h2>\n<p>다음은 코드 구현을 진행 하여 보겠습니다. 일단 저번 <strong>MNIST</strong> 실습 때 이용 했던 것처럼, <strong>MNIST data</strong>를 가져 오도록 하겠습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn <span class=\"token keyword\">as</span> nn\n<span class=\"token keyword\">from</span> torchvision<span class=\"token punctuation\">.</span>datasets <span class=\"token keyword\">import</span> MNIST\n<span class=\"token keyword\">from</span> torchvision <span class=\"token keyword\">import</span> transforms\n<span class=\"token keyword\">from</span> torch<span class=\"token punctuation\">.</span>utils<span class=\"token punctuation\">.</span>data <span class=\"token keyword\">import</span> DataLoader\n<span class=\"token keyword\">from</span> torch<span class=\"token punctuation\">.</span>autograd <span class=\"token keyword\">import</span> Variable\n<span class=\"token keyword\">from</span> torchvision<span class=\"token punctuation\">.</span>transforms<span class=\"token punctuation\">.</span>functional <span class=\"token keyword\">import</span> to_pil_image\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>functional <span class=\"token keyword\">as</span> F\n<span class=\"token keyword\">import</span> matplotlib<span class=\"token punctuation\">.</span>pyplot <span class=\"token keyword\">as</span> plt\n\nBATCH_SIZE <span class=\"token operator\">=</span> <span class=\"token number\">64</span>\nimage_shape <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">28</span><span class=\"token punctuation\">,</span> <span class=\"token number\">28</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># MNIST 이미지 모양</span>\nEPOCH <span class=\"token operator\">=</span> <span class=\"token number\">200</span>\nlearning_rate <span class=\"token operator\">=</span> <span class=\"token number\">0.0002</span>\nlatent_dim <span class=\"token operator\">=</span> <span class=\"token number\">100</span>  <span class=\"token comment\"># 입력 값의 차원</span>\n\ncustom_train_transform <span class=\"token operator\">=</span> transforms<span class=\"token punctuation\">.</span>Compose<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n                                             transforms<span class=\"token punctuation\">.</span>ToTensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n                                             transforms<span class=\"token punctuation\">.</span>Normalize<span class=\"token punctuation\">(</span>mean<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> std<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\ncustom_test_transform <span class=\"token operator\">=</span> transforms<span class=\"token punctuation\">.</span>Compose<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n                                             transforms<span class=\"token punctuation\">.</span>ToTensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n                                             transforms<span class=\"token punctuation\">.</span>Normalize<span class=\"token punctuation\">(</span>mean<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> std<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">,</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\ntrain_dataset <span class=\"token operator\">=</span> MNIST<span class=\"token punctuation\">(</span><span class=\"token string\">\".\"</span><span class=\"token punctuation\">,</span> train<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> download<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> transform<span class=\"token operator\">=</span>custom_train_transform<span class=\"token punctuation\">)</span>\n\ntrain_loader <span class=\"token operator\">=</span> DataLoader<span class=\"token punctuation\">(</span>dataset<span class=\"token operator\">=</span>train_dataset<span class=\"token punctuation\">,</span>\n                          batch_size<span class=\"token operator\">=</span>BATCH_SIZE<span class=\"token punctuation\">,</span>\n                          shuffle<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n                          drop_last<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n                          num_workers<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n\n\ntest_dataset <span class=\"token operator\">=</span> MNIST<span class=\"token punctuation\">(</span><span class=\"token string\">\".\"</span><span class=\"token punctuation\">,</span> train<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span> download<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> transform<span class=\"token operator\">=</span>custom_test_transform<span class=\"token punctuation\">)</span>\n\ntest_loader <span class=\"token operator\">=</span> DataLoader<span class=\"token punctuation\">(</span>dataset<span class=\"token operator\">=</span>test_dataset<span class=\"token punctuation\">,</span>\n                         batch_size<span class=\"token operator\">=</span>BATCH_SIZE<span class=\"token punctuation\">,</span>\n                         shuffle<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span>\n                         num_workers<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n\nimg<span class=\"token punctuation\">,</span> label <span class=\"token operator\">=</span> train_dataset<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\nplt<span class=\"token punctuation\">.</span>title<span class=\"token punctuation\">(</span><span class=\"token string\">\"label: \"</span> <span class=\"token operator\">+</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">(</span>label<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>imshow<span class=\"token punctuation\">(</span>to_pil_image<span class=\"token punctuation\">(</span>img<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> cmap<span class=\"token operator\">=</span><span class=\"token string\">'gray'</span><span class=\"token punctuation\">)</span>\nplt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p align=\"center\">\n       <img src=\"/post_image/just-data-science/12-05.png\" width=\"40%\"/>\n       <p align=\"center\" style=\"color:#888888; font-size: 12px;\">\n              데이터를 잘 불러 온 모습.\n       </p>\n</p>\n<h2>Model</h2>\n<p>다음은 모델을 설계 할 시간입니다. 우리는 이를 학습 시킬 때, 두 가지 모델을 학습 시키기 때문에, 학습 시 두 가지 모델을 설계 하여야 합니다.</p>\n<h3>Generator</h3>\n<p>먼저 <strong>Generator</strong> 구현에 대해서 먼저 설명 드리겠습니다. 일단, 우리는 임의의 벡터를 입력 받아, 여러 개의 <strong>Affine</strong> 계층을 거친 후, 이미지와 비슷 한 사이즈로 만들어야 합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">Generator</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> input_noise<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span>Generator<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        \n        <span class=\"token keyword\">def</span> <span class=\"token function\">block</span><span class=\"token punctuation\">(</span>in_feat<span class=\"token punctuation\">,</span> out_feat<span class=\"token punctuation\">,</span> normalize<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            layers <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span>in_feat<span class=\"token punctuation\">,</span> out_feat<span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n            <span class=\"token keyword\">if</span> normalize<span class=\"token punctuation\">:</span>\n                layers<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>BatchNorm1d<span class=\"token punctuation\">(</span>out_feat<span class=\"token punctuation\">,</span> <span class=\"token number\">0.8</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n            layers<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>LeakyReLU<span class=\"token punctuation\">(</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span> inplace<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">return</span> layers\n\n        self<span class=\"token punctuation\">.</span>model <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Sequential<span class=\"token punctuation\">(</span>\n            <span class=\"token operator\">*</span>block<span class=\"token punctuation\">(</span>latent_dim<span class=\"token punctuation\">,</span> <span class=\"token number\">256</span><span class=\"token punctuation\">,</span> normalize<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            <span class=\"token operator\">*</span>block<span class=\"token punctuation\">(</span><span class=\"token number\">256</span><span class=\"token punctuation\">,</span> <span class=\"token number\">512</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            <span class=\"token operator\">*</span>block<span class=\"token punctuation\">(</span><span class=\"token number\">512</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1024</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">1024</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>prod<span class=\"token punctuation\">(</span>image_shape<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            nn<span class=\"token punctuation\">.</span>Tanh<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        <span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> z<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        img <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>model<span class=\"token punctuation\">(</span>z<span class=\"token punctuation\">)</span>\n        img <span class=\"token operator\">=</span> img<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span>img<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">*</span>image_shape<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> img</code></pre></div>\n<p>그 다음, <strong>Discriminator</strong>에서는, 이를 검출하기 위한 코드를 사용 합니다. 입력으로 이미지가 들어오면, 이를 진짜인지 아닌지를 검출 합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">Discriminator</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span>Discriminator<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n        self<span class=\"token punctuation\">.</span>model <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Sequential<span class=\"token punctuation\">(</span>\n            nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token builtin\">int</span><span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>prod<span class=\"token punctuation\">(</span>image_shape<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1024</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            nn<span class=\"token punctuation\">.</span>LeakyReLU<span class=\"token punctuation\">(</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span> inplace<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            nn<span class=\"token punctuation\">.</span>Dropout<span class=\"token punctuation\">(</span><span class=\"token number\">0.3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">1024</span><span class=\"token punctuation\">,</span> <span class=\"token number\">512</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            nn<span class=\"token punctuation\">.</span>LeakyReLU<span class=\"token punctuation\">(</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span> inplace<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            nn<span class=\"token punctuation\">.</span>Dropout<span class=\"token punctuation\">(</span><span class=\"token number\">0.3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">512</span><span class=\"token punctuation\">,</span> <span class=\"token number\">256</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            nn<span class=\"token punctuation\">.</span>LeakyReLU<span class=\"token punctuation\">(</span><span class=\"token number\">0.2</span><span class=\"token punctuation\">,</span> inplace<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            nn<span class=\"token punctuation\">.</span>Dropout<span class=\"token punctuation\">(</span><span class=\"token number\">0.3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">256</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            nn<span class=\"token punctuation\">.</span>Sigmoid<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        <span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> img<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        img_flat <span class=\"token operator\">=</span> img<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span>img<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n        validity <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>model<span class=\"token punctuation\">(</span>img_flat<span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">return</span> validity</code></pre></div>\n<h2>Training</h2>\n<p>다음은 학습을 실시 할 시간 입니다. <strong>Generator</strong>는 가짜 데이터가 <strong>Discriminator</strong>에 통과 된 결과값을 보고, 내가 제대로 속였는지, 안 속였는지에 대해서 <strong>손실 함수</strong>를 계산 합니다. <strong>Discriminator</strong>는 <strong>Generator</strong>에서 나온 가짜 데이터를 제대로 검출 했는지 안 했는지, 그 다음 실제 데이터를 진짜라고 판별 했는지, 안 했는지에 대해서 <strong>손실 함수</strong>를 계산 합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">adversarial_loss <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>BCELoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># Binary Cross Entropy</span>\n\ngenerator <span class=\"token operator\">=</span> Generator<span class=\"token punctuation\">(</span>latent_dim<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># Generator</span>\ndiscriminator <span class=\"token operator\">=</span> Discriminator<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># Discriminator</span>\n\n<span class=\"token comment\"># Optimizer</span>\noptimizer_G <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>optim<span class=\"token punctuation\">.</span>Adam<span class=\"token punctuation\">(</span>generator<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span>learning_rate<span class=\"token punctuation\">)</span>\noptimizer_D <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>optim<span class=\"token punctuation\">.</span>Adam<span class=\"token punctuation\">(</span>discriminator<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span>learning_rate<span class=\"token punctuation\">)</span>\n\nbatch_count <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n\nTensor <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>FloatTensor\n\n<span class=\"token keyword\">for</span> epoch <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>EPOCH<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">for</span> i<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>imgs<span class=\"token punctuation\">,</span> _<span class=\"token punctuation\">)</span> <span class=\"token keyword\">in</span> <span class=\"token builtin\">enumerate</span><span class=\"token punctuation\">(</span>train_loader<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        valid <span class=\"token operator\">=</span> Variable<span class=\"token punctuation\">(</span>Tensor<span class=\"token punctuation\">(</span>imgs<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>fill_<span class=\"token punctuation\">(</span><span class=\"token number\">1.0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> requires_grad<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 배치 사이즈로 Target: True 삽입</span>\n        fake <span class=\"token operator\">=</span> Variable<span class=\"token punctuation\">(</span>Tensor<span class=\"token punctuation\">(</span>imgs<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>fill_<span class=\"token punctuation\">(</span><span class=\"token number\">0.0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> requires_grad<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 배치 사이즈로 Target: False 삽입</span>\n        \n        <span class=\"token comment\"># Configure input</span>\n        real_imgs <span class=\"token operator\">=</span> Variable<span class=\"token punctuation\">(</span>imgs<span class=\"token punctuation\">.</span><span class=\"token builtin\">type</span><span class=\"token punctuation\">(</span>Tensor<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        \n        <span class=\"token comment\"># Train Generator</span>\n        optimizer_G<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        z <span class=\"token operator\">=</span> Variable<span class=\"token punctuation\">(</span>Tensor<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>normal<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>imgs<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> latent_dim<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        \n        gen_imgs <span class=\"token operator\">=</span> generator<span class=\"token punctuation\">(</span>z<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 이미지 생성</span>\n        g_loss <span class=\"token operator\">=</span> adversarial_loss<span class=\"token punctuation\">(</span>discriminator<span class=\"token punctuation\">(</span>gen_imgs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> valid<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># Generator Loss 계산, 속이면 속일 수록 낮아짐</span>\n\n        g_loss<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        optimizer_G<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        \n        <span class=\"token comment\"># Train Discriminator</span>\n        optimizer_D<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n        real_loss <span class=\"token operator\">=</span> adversarial_loss<span class=\"token punctuation\">(</span>discriminator<span class=\"token punctuation\">(</span>real_imgs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> valid<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># Discriminator Loss 계산, 진짜 레이블에 대해서</span>\n        fake_loss <span class=\"token operator\">=</span> adversarial_loss<span class=\"token punctuation\">(</span>discriminator<span class=\"token punctuation\">(</span>gen_imgs<span class=\"token punctuation\">.</span>detach<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> fake<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># Discriminator Loss 계산, 가짜 레이블에 대해서</span>\n        d_loss <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>real_loss <span class=\"token operator\">+</span> fake_loss<span class=\"token punctuation\">)</span>\n\n        d_loss<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        optimizer_D<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    \n    <span class=\"token comment\"># Epoch 마다 이미지 생성 시각화</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>\n        <span class=\"token string\">\"[Epoch %d/%d] [D loss: %f] [G loss: %f]\"</span>\n        <span class=\"token operator\">%</span> <span class=\"token punctuation\">(</span>epoch<span class=\"token punctuation\">,</span> EPOCH<span class=\"token punctuation\">,</span> d_loss<span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> g_loss<span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">)</span>\n    \n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"Epoch:\"</span><span class=\"token punctuation\">,</span> epoch<span class=\"token punctuation\">)</span>\n    fig<span class=\"token punctuation\">,</span> axes <span class=\"token operator\">=</span> plt<span class=\"token punctuation\">.</span>subplots<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">)</span>\n    z <span class=\"token operator\">=</span> Variable<span class=\"token punctuation\">(</span>Tensor<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>normal<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> latent_dim<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    gen_imgs <span class=\"token operator\">=</span> generator<span class=\"token punctuation\">(</span>z<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">4</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        axes<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>imshow<span class=\"token punctuation\">(</span>gen_imgs<span class=\"token punctuation\">.</span>detach<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>numpy<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n    plt<span class=\"token punctuation\">.</span>show<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>하지만, 데이터의 결과가 영 시원 찮은 것을 볼 수 있습니다. 일단, 데이터를 생성 하고, 데이터를 판별 하는데에 일반 <strong>DNN</strong>을 적용 했기 때문 입니다. 데이터를 생성하고, 데이터를 판별 할때 <strong>CNN</strong>을 사용 한 것을 <strong>DCGAN</strong>이라고 합니다. 아마, 그렇게 하면 성능이 더 잘 나올 것이라고 예상 됩니다.</p>\n<p align=\"center\">\n       <img src=\"/post_image/just-data-science/12-06.png\" width=\"40%\"/>\n       <p align=\"center\" style=\"color:#888888; font-size: 12px;\">\n              결과\n       </p>\n</p>\n<h2>마치며</h2>\n<p>이번 시간에는 <strong>GAN</strong>에 대해서 배워 보았습니다. 오늘, 이렇게 마지막 시간을 갖게 되었습니다. 사실, 찍먹 시리즈에서 제가 다룬건 겉핥기 수준에 불과 합니다. 실제 우리가 이를 이용하여 어플리케이션을 만들 때는 많은 수학적인 이해, 통찰력, 경험이 필요 합니다. 이를 통해 데이터 사이언스에 관심이 생기셨다면, 논문과, 다른 블로그나 영상을 통해 심층 적으로 공부 하시는 걸 추천 드립니다! (개인적으로 <strong>Andrew ng</strong>씨의 강의를 추천 드립니다.) 모두 부족한 제 포스트를 따라 공부해 주심에 감사 드립니다.</p>","id":"49a9d801-a31f-50ec-8e5b-d29b96e4a2de","frontmatter":{"date":"2021-08-07","path":"/data-science/just-data-science-12","title":"[찍먹 Data Science] 12. Generative Adversarial Network","tags":["Data-Science","Deep-Learning"],"keyword":"Data Science, 데이터 사이언스, GAN, Generative Adversarial Network, Pytorch GAN","summary":"서로가 적대적으로 학습하는 GAN에 대해서 알아 보자.","img":"/post_image/thumbnail/just-data-science-12.jpg","series":"찍먹 Data Science"}}},"pageContext":{"series":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"49a9d801-a31f-50ec-8e5b-d29b96e4a2de","excerpt":"Generative Adversarial Network 안녕하세요? 오늘은 GAN, Generative Adversarial Network에 대해서 알아 보도록 하겠습니다. Generative Adversarial Network…","frontmatter":{"date":"2021-08-07","tags":["Data-Science","Deep-Learning"],"path":"/data-science/just-data-science-12","title":"[찍먹 Data Science] 12. Generative Adversarial Network","img":"/post_image/thumbnail/just-data-science-12.jpg","summary":"서로가 적대적으로 학습하는 GAN에 대해서 알아 보자."}}},{"node":{"id":"8b853027-d41f-5306-8cb9-8242edaf29c1","excerpt":"Recurrent Neural Network 안녕하세요? 오늘은 RNN, Recurrent Neural Network에 대해서 알아 보도록 하겠습니다. RNN…","frontmatter":{"date":"2021-08-05","tags":["Data-Science","Deep-Learning"],"path":"/data-science/just-data-science-11","title":"[찍먹 Data Science] 11. Recurrent Neural Network","img":"/post_image/thumbnail/just-data-science-11.jpg","summary":"시계열 데이터를 처리하는 RNN을 알아보자."}}},{"node":{"id":"78aa3994-0aa2-55e7-b6cb-0ebc85c57591","excerpt":"Convolutional Neural Network 안녕하세요? 오늘은 CNN, Convolutional Neural Network에 대해서 알아 보도록 하겠습니다. 저번 시간에는 DNN…","frontmatter":{"date":"2021-07-31","tags":["Data-Science","Deep-Learning"],"path":"/data-science/just-data-science-10","title":"[찍먹 Data Science] 10. Convolutional Neural Network","img":"/post_image/thumbnail/just-data-science-10.jpg","summary":"국소적인 정보를 추출하는 CNN에 대해 알아 보자."}}},{"node":{"id":"e708c84f-fcff-5de6-9204-de9a7569750c","excerpt":"Deep Neural Network 안녕하세요? 오늘은 DNN, Deep Neural Network에 대해서 알아 보도록 하겠습니다. 여태까지 우리는 간단한 선형 모델에 대해서만 학습을 진행 하였습니다.  (혹은 ) 와 같이, 선형 연산을 통해서, 데이터에 대해서 예측하고, 분류 해 보는 실습을 진행 하였습니다. 하지만, 이러한 선형적인 모델이 비선형적인 문제를 해결 하려면 어떻게 해야 할까요? 일단 간단한 예제를 생각 해 보겠습니다. XOR…","frontmatter":{"date":"2021-07-29","tags":["Data-Science","Deep-Learning"],"path":"/data-science/just-data-science-9","title":"[찍먹 Data Science] 9. Deep Neural Network","img":"/post_image/thumbnail/just-data-science-9.jpg","summary":"딥러닝의 기초, DNN에 대해서 알아 보자."}}},{"node":{"id":"a272e5eb-daef-505b-91e4-ddb97a4caf45","excerpt":"Normalization, PCA 안녕하세요? Justkode 입니다. 이번 시간에는 Normalization (정규화)와, 차원 축소를 위한 PCA에 대해 공부 해 보는 시간을 가져 보도록 하겠습니다. 정규화와 차원 축소는 기계 학습에서 중요 한 요소 입니다. 학습에서 직접적인 영향을 주기 때문이죠. Normalization 저번에 Linear Regression의 Cost Function에 대한 미분을 하게 되면, Input Vector…","frontmatter":{"date":"2021-07-25","tags":["Data-Science","Machine-Learning"],"path":"/data-science/just-data-science-8","title":"[찍먹 Data Science] 8. Normalization, PCA","img":"/post_image/thumbnail/just-data-science-8.jpg","summary":"값의 범위를 줄이고, 차원을 효율 적으로 줄여보자."}}},{"node":{"id":"a15e6d2e-d923-5e8e-9bb9-ae1c7116c1bc","excerpt":"Linear Regression, Classification 안녕하세요? Justkode 입니다. 오늘은 선형 회귀와 분류에 대해 이론을 공부 해 보고, 실습을 진행 해 보는 시간을 가져 보도록 하겠습니다. Before we start 먼저 이번 실습에는  모듈이 필요합니다. 다음을 통해 설치 해 주세요. (단, numpy, pandas, matplotlib이 설치 되어 있다면 scikit-learn만 설치 하셔도 됩니다.) Linear…","frontmatter":{"date":"2021-07-21","tags":["Data-Science","Machine-Learning"],"path":"/data-science/just-data-science-6","title":"[찍먹 Data Science] 6. Linear Regression, Logistic Regression","img":"/post_image/thumbnail/just-data-science-6.jpg","summary":"선형 회귀와 로지스틱 회귀에 대해서 알아 보자."}}},{"node":{"id":"8276c1fb-09d2-5fea-a583-b11d7ca961a3","excerpt":"SVM, K-NN, Random Forest 안녕하세요? Justkode 입니다. 저번 시간에는 선형 회귀와 분류에 대해 공부 해 보았습니다. 그런데 Logistic Regression…","frontmatter":{"date":"2021-07-21","tags":["Data-Science","Machine-Learning"],"path":"/data-science/just-data-science-7","title":"[찍먹 Data Science] 7. SVM, K-NN, Random forest","img":"/post_image/thumbnail/just-data-science-7.jpg","summary":"자주 사용 되는 ML 모델을 사용 해 보자."}}},{"node":{"id":"5a9d9d37-ba85-593d-9e17-cd7082bc18c6","excerpt":"Maching Learning Basic. 안녕하세요? Justkode 입니다. 오늘은 머신 러닝의 기본 개념에 대해 공부 해 보는 시간을 가져 보도록 하겠습니다. 학습의 종류 머신 러닝 학습의 종류는 두 종류가 있습니다.  지도 학습 (Supervised Learning) 첫 번째는 지도 학습입니다. 지도 학습은 훈련 데이터로 Feature(특징)와 Target…","frontmatter":{"date":"2021-07-19","tags":["Data-Science","Machine-Learning"],"path":"/data-science/just-data-science-5","title":"[찍먹 Data Science] 5. Machine Learning Basic","img":"/post_image/thumbnail/just-data-science-5.jpg","summary":"머신 러닝의 기본 개념에 대해 알아보자."}}},{"node":{"id":"67ae44e3-5a48-57d9-87ea-fd77d9876c34","excerpt":"SQL 안녕하세요? Justkode 입니다. 오늘은 SQL에 대해서 간단하게 알아보는 시간을 가져보도록 하겠습니다. SQL은 Structured Query Language의 약자로, RDBMS의 데이터베이스를 주로 조회 하는데에 사용이 되는, 데이터베이스 관리 시스템(RDBMS)의 데이터를 관리하기 위해 설계된 특수 목적의 프로그래밍 언어입니다. 여기서 설명 하는 문법들은 많은 내용을 담지 않고, SQL…","frontmatter":{"date":"2021-07-11","tags":["Data-Science","Python"],"path":"/data-science/just-data-science-4","title":"[찍먹 Data Science] 4. SQL","img":"/post_image/thumbnail/just-data-science-4.jpg","summary":"SQL의 기본 문법에 대해 알아보자."}}},{"node":{"id":"72999af7-d2c7-5f95-8d6c-eb8d5219cd76","excerpt":"Matplotlib 안녕하세요? Justkode 입니다. 오늘은 Matplotlib에 대해서 알아보는 시간을 가져보도록 하겠습니다. Matplotlib는 데이터 분석을 위해 만들어진 라이브러리로 Numpy, Pandas…","frontmatter":{"date":"2021-07-08","tags":["Data-Science","Python"],"path":"/data-science/just-data-science-3","title":"[찍먹 Data Science] 3. Matplotlib","img":"/post_image/thumbnail/just-data-science-3.jpeg","summary":"데이터 시각화 모듈, Matplotlib을 알아보자."}}},{"node":{"id":"05ccbb5b-48d4-57d0-9f3b-ca1dae54ed34","excerpt":"Pandas 안녕하세요? Justkode 입니다. 오늘은 Pandas에 대해서 심층있게 알아보는 시간을 가져보도록 하겠습니다. Pandas는 데이터 분석을 위해 만들어진 라이브러리로 Numpy와 함께 많이 사용 됩니다. 주로 사용하는 데이터 구조는 Dataframe과 Series로, Table 정보와 같은 데이터를 처리 하는데 이점이 있습니다. Series and DataFrame 첫 번째로 Series입니다. Series는…","frontmatter":{"date":"2021-07-04","tags":["Data-Science","Python"],"path":"/data-science/just-data-science-2","title":"[찍먹 Data Science] 2. Pandas","img":"/post_image/thumbnail/just-data-science-2.jpg","summary":"데이터 분석에 쓰이는 Pandas를 알아보자."}}},{"node":{"id":"c29d290f-1001-5de9-8997-cb6fe729221b","excerpt":"Data Science And Math 안녕하세요? Justkode 입니다. 많은 Machine Learning과 Deep Learning…","frontmatter":{"date":"2021-06-30","tags":["Data-Science","Python"],"path":"/data-science/just-data-science-1","title":"[찍먹 Data Science] 1. Math, Numpy","img":"/post_image/thumbnail/just-data-science-1.jpg","summary":"간단한 수학 식을 Numpy로 구현해 보자"}}},{"node":{"id":"d0d8165b-3ac6-5f7f-b1e6-e8fb46731ec5","excerpt":"Data Science, 어디부터 해야 할까? 안녕하세요? Justkode 입니다. 일단, 제가 대학교 1,…","frontmatter":{"date":"2021-06-23","tags":["Data-Science"],"path":"/data-science/just-data-science-0","title":"[찍먹 Data Science] 0. Orientation","img":"/post_image/thumbnail/just-data-science-0.jpg","summary":"Data Science를 찍먹해 보자."}}}]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"0d127ed8-2483-5c3f-9041-36f402ee5ef0","excerpt":"Regularization 안녕하세요? 이번 시간에는 Regularization에 대해서 알아 보도록 하겠습니다. 우리가 모델을 만들 때, 많은 분들이 Overfitting을 경험 해 보셨을 것 입니다. 여러분들은 Overfitting을 경험할 때, 다음과 같은 것들을 시도 해 볼 것입니다. 모델의 크기를 줄인다. (네트워크의 깊이, 노드 갯수 등등) 데이터의 크기를 늘린다. 하지만, 이 외에도 다른 방법들이 있습니다. DropOut…","frontmatter":{"date":"2021-08-31","tags":["Data-Science","Deep-Learning"],"path":"/data-science/regularization","title":"Regularization: 모델의 과적합을 막는 방법","img":"/post_image/thumbnail/regularization.jpg","summary":"모델의 과적합을 막는 Regularization에 대해서 알아 보자."}}},{"node":{"id":"49a9d801-a31f-50ec-8e5b-d29b96e4a2de","excerpt":"Generative Adversarial Network 안녕하세요? 오늘은 GAN, Generative Adversarial Network에 대해서 알아 보도록 하겠습니다. Generative Adversarial Network…","frontmatter":{"date":"2021-08-07","tags":["Data-Science","Deep-Learning"],"path":"/data-science/just-data-science-12","title":"[찍먹 Data Science] 12. Generative Adversarial Network","img":"/post_image/thumbnail/just-data-science-12.jpg","summary":"서로가 적대적으로 학습하는 GAN에 대해서 알아 보자."}}},{"node":{"id":"8b853027-d41f-5306-8cb9-8242edaf29c1","excerpt":"Recurrent Neural Network 안녕하세요? 오늘은 RNN, Recurrent Neural Network에 대해서 알아 보도록 하겠습니다. RNN…","frontmatter":{"date":"2021-08-05","tags":["Data-Science","Deep-Learning"],"path":"/data-science/just-data-science-11","title":"[찍먹 Data Science] 11. Recurrent Neural Network","img":"/post_image/thumbnail/just-data-science-11.jpg","summary":"시계열 데이터를 처리하는 RNN을 알아보자."}}},{"node":{"id":"78aa3994-0aa2-55e7-b6cb-0ebc85c57591","excerpt":"Convolutional Neural Network 안녕하세요? 오늘은 CNN, Convolutional Neural Network에 대해서 알아 보도록 하겠습니다. 저번 시간에는 DNN…","frontmatter":{"date":"2021-07-31","tags":["Data-Science","Deep-Learning"],"path":"/data-science/just-data-science-10","title":"[찍먹 Data Science] 10. Convolutional Neural Network","img":"/post_image/thumbnail/just-data-science-10.jpg","summary":"국소적인 정보를 추출하는 CNN에 대해 알아 보자."}}}]}}}}},"staticQueryHashes":["234633779","63159454"]}