{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/data-engineering/spark-performance-tuning-1/","result":{"data":{"markdownRemark":{"html":"<p>오늘은 <strong>Spark</strong>의 성능 튜닝에 대해서 이야기 해 보겠습니다. <strong>Spark</strong>는 요약해서 말하면, **in-memory(RAM 위에서)**에서 작동 하는 <strong>분산 컴퓨팅</strong>을 쉽게 지원해 주는 프레임워크 입니다.</p>\n<p><strong>in-memory</strong> 연산은 <strong>빠르지만, 불안정</strong> 합니다. <strong>메모리 관리, CPU Core 수의 관리</strong>를 통해 <strong>Out of memory</strong>가 발생 하지 않는 선에서, <strong>Job</strong>이 성공적으로 수행 될 수 있도록 하여야 하며, <strong>적절한 캐싱 전략, 직렬화, Executor 파티션 갯수 선정</strong>을 통해, <strong>많은 컴퓨팅 자원을 점유하지 않도록</strong> 해야 합니다.</p>\n<p>이제 하나씩 알아 보도록 하겠습니다.</p>\n<h3>Executor</h3>\n<p><strong>Spark</strong>에서 <strong>Executor</strong>는 <strong>Spark Cluster</strong> 내에서 데이터를 처리하는 단위이며, 독립된 <strong>JVM</strong> 프로세스로 실행 됩니다. <strong>Executor</strong>는 클러스터 내, <strong>여러 개의 인스턴스에서 실행</strong> 될 수 있으며, 각 <strong>인스턴스</strong>는 <strong>다수의 CPU 코어로 작업을 병렬로 실행</strong> 될 수 있습니다. 또한, 각 <strong>인스턴스의 메모리</strong>는 다음과 같이 구성 되어 있습니다,</p>\n<p align=\"center\">\n    <img src=\"/post_image/spark-performance-tuning/1-2.jpeg\" max-width=\"400px\" />\n    <div align=\"center\" color=\"#aaaaaa\">Executor Memory</div>\n</p>\n<p><strong>Spark Executor Memory</strong>는 세 가지 영역으로 나눌 수 있습니다. 이 세 가지 영역은 다음과 같습니다.</p>\n<ul>\n<li><strong>Reserved Memory</strong>: Executor의 기타 기능 및 운영체제에서 사용 하는 메모리 등을 위해 예약된 메모리 영역입니다. 예로 <strong>JVM 내에서 사용</strong> 하는 메모리, Java의 <strong>Garbage Collection</strong>에서 발생 하는 메모리 등이 저장 됩니다. <strong>300MB</strong>가 고정 되어 있으며, <code class=\"language-text\">spark.testing.reservedMemory</code>로 설정을 변경 할 수 있으나, <strong>오로지 테스트 용도</strong> 입니다. <code class=\"language-text\">전체 Executor Memory - Reserved Memory</code>는 <strong>Usable Memory</strong>가 됩니다.</li>\n<li><strong>Spark Memory</strong>: Spark에서 사용하는 메모리 영역으로, <strong>RDD Cache, Broadcast 변수, 실행 계획</strong> 등이 이 영역에 저장 됩니다. <code class=\"language-text\">Usable Memory * spark.memory.fraction</code>이 <strong>Spark Memory</strong>가 됩니다.\n<ul>\n<li><strong>Storage Memory</strong>: <strong>Cache 데이터, Broadcast 변수</strong>가 저장 됩니다. 캐싱할 공간이 부족 하면, <strong>LRU(Least Recently Used) 방식으로 제거</strong>합니다. <code class=\"language-text\">Spark Memory * spark.memory.storageFraction</code> 이 <strong>Storage Memory</strong>가 됩니다.</li>\n<li><strong>Execution Memory</strong>: Spark가 task를 실행 하는 동안 생성되는 object들인 Map 수행 시 <strong>Shuffle Intermediate Buffer</strong>, Hash aggregation step에서의 <strong>Hash Table</strong> 등이 저장 됩니다. 메모리가 충분 하지 않으면, <strong>디스크로 Spill</strong>을 하기 때문에 <strong>디스크 I/O를 줄이기 위해서 고려</strong> 하여야 합니다 <code class=\"language-text\">Spark Memory * (1 - spark.memory.storageFraction)</code> 이 <strong>Execution Memory</strong>가 됩니다.</li>\n<li>참고 사항: <strong>Storage Memory와 Execution Memory는 필요 시 서로의 메모리를 점유</strong> 할 수 있습니다. 여기서 Storage Memory는 Execution Memory로부터 쫓겨날 수 있지만, Execution Memory는 Storage Memory로부터 쫓겨 날 수 없습니다.</li>\n</ul>\n</li>\n<li><strong>User Memory</strong>: <strong>사용자 코드에서 직접 할당</strong> 되는 메모리 영역입니다. Spark 내부 메타데이터, 사용자 정의 데이터 구조, UDF, RDD 종속성 정보 등, 요약하면 <strong>사용자가 정의한 데이터구조, UDF가 저장되는 공간</strong>입니다. <code class=\"language-text\">Usable Memory * (1 - spark.memory.fraction)</code> 이 <strong>User Memory</strong>가 됩니다.</li>\n</ul>\n<h3>Partition</h3>\n<p>그 다음으로는 Spark의 <strong>Partition</strong>에 대해서 알아 보도록 하겠습니다.</p>\n<p><strong>Spark</strong>에서 <strong>파티션</strong>은 <strong>데이터를 분할하는 단위</strong>로, <strong>Spark</strong>에서 처리 하는 최소 단위라고 볼 수 있습니다. **RDD(Resilient Distributed Dataset, 분산 데이터 셋)**를 구성하는 최소 단위 객체이며, 각 파티션은 클러스터의 <strong>노드</strong>에서 분산 되어 처리 됩니다.</p>\n<p>각 <strong>Partition</strong>은 서로 다른 노드에서 분산 처리 되며, <strong>1개의 CPU Core가, 1개의 Partion에 대해, 1개의 최소 연산인 Task</strong>를 수행 합니다.</p>\n<p>똑같은 크기의 데이터 입력 이라면 <strong>설정된 Partition 수에 따가 각 Partiton의 크기가 결정</strong>되는 것이죠!</p>\n<ul>\n<li>Partiton 수가 적어요 -> Partition의 크기가 커져요!</li>\n<li>Partiton 수가 많아요 -> Partition의 크기가 작아져요!</li>\n</ul>\n<p align=\"center\">\n    <img src=\"/post_image/spark-performance-tuning/1-1.gif\" max-width=\"400px\" />\n    <div align=\"center\" color=\"#aaaaaa\">Data Partitioning</div>\n</p>\n<p>파티션을 작게 만들면, 더 작은 파일 단위로 쪼개어, <strong>Task당 필요한 메모리를 줄이고, 병렬화의 정도를 늘릴 수 있기 때문</strong>에 좋지 않을까요? 이는 <strong>정답이 아닙니다.</strong> 너무 많은 파티션으로 쪼개게 되면, <strong>파티션 간 통신 비용</strong>이 증가 하게 되고, 저장 시에는 <strong>Small File Problem</strong>이 발생 하여, <strong>Disk I/O 비용</strong>이 증가 하게 되는 단점이 있습니다.</p>\n<p>그렇다면 크게 만드는 것이 답일까요? <strong>그것 또한 아닙니다.</strong> 너무 작은 수의 파티션은 <strong>적은 Task 수</strong>로 이어져, 병렬 처리에 불리합니다. 또한, <strong>Executor 메모리 사용량</strong> 증가, Partition의 데이터 분포가 고르지 않음으로 발생 하는, <strong>Data Skew</strong>현상 또한 발생 할 수 있습니다.</p>\n<p>Partition의 종류는 3개로 나눌 수 있습니다.</p>\n<ul>\n<li><strong>Input Partition</strong>: 처음 파일을 읽을 때 생성하는 Partition으로, 관련 설정 값은 <code class=\"language-text\">spark.sql.files.maxpartitionBytes</code> 입니다.</li>\n<li><strong>Output Partition</strong>: <strong>파일을 저장할 때 생성하는 Partition</strong>으로, 이 수가 <strong>HDFS 상의 마지막 경로의 파일 수</strong>를 지정합니다. <strong>파일 하나의 크기를 HDFS Blocksize에 일치</strong> 하도록 생성 하는 것이 권장 됩니다. <code class=\"language-text\">df.repartition(cnt)</code>, <code class=\"language-text\">df.coalesce(cnt)</code>를 통해 수정 할 수 있습니다.</li>\n<li><strong>Shuffle Partition</strong>: <strong>Join, groupBy 등의 연산</strong>을 수행할 때 사용 됩니다. <code class=\"language-text\">spark.sql.shuffle.partitions</code>로 설정을 변경 할 수 있습니다.</li>\n</ul>\n<p>이 설정들을 적절히 활용 하여, Shuffle 과정에서 <strong>Shuffle Spill</strong>이 일어 나지 않도록 최적화를 하는 것이 중요합니다.</p>\n<h3>그러면 파티션과 Executor 설정을 어떻게 해야 할까요?</h3>\n<p>일단 통상 적으로 알려진 사항은 <strong>Shuffle Partition</strong> 크기가 <strong>100 ~ 200MB</strong> 정도 나오게 끔 쿼리를 조정 해 주는 것이 좋습니다. <strong>Shuffle Partition</strong>이 너무 커져서 <strong>Memory Spill</strong>을 방지 하기 위함 입니다. 또한, 이를 적절하게 해결 할 수 있는 수준으로 <strong>Shuffle Partition</strong>을 구성 하는 것이 좋아 보입니다. 이를 위해서, 쿼리 중간에 <strong>파티셔닝이 진행</strong> 되어야 하는 경우, **Data Skew (Data가 한쪽으로 몰리는 현상)**를 막기 위한 조치들을 취해야 합니다. (Range Partitioning, Hash Partitioning, Custom Partitioning을 적절히 사용하여...)</p>\n<p>결론적으로는 최적화는 제일 먼저 쿼리를 통해, <strong>Shuffle Partiton 크기</strong>가 커지지 않도록 조정 하고, <strong>Partition 수</strong>를 <strong>Shuffle Spill</strong>이 발생하지 않으면서, 가용 가능한 <strong>CPU Core</strong> 리소스 안에서 증가 시키고, <strong>Core 당 메모리</strong> 또한, <strong>Shuffle Partition 크기</strong> 를 감당 할 수 있는 수준으로 조정 할 수 있게 하여야 합니다. 즉, <strong>Executor당 Core, Instance당 Memory, Instance 수</strong>를 잘 조정 하여야 한다는 이야기 입니다.</p>\n<p>다음 시간에는 쿼리 최적화를 하는 방법을 한 번 가져 보도록 하겠습니다. 감사합니다.</p>\n<h3>Reference</h3>\n<ul>\n<li><a href=\"https://spark.apache.org/docs/latest/configuration.html\">https://spark.apache.org/docs/latest/configuration.html</a></li>\n<li><a href=\"https://tech.kakao.com/2021/10/08/spark-shuffle-partition/\">https://tech.kakao.com/2021/10/08/spark-shuffle-partition/</a></li>\n<li><a href=\"https://velog.io/@busybean3/Apache-Spark-%EC%95%84%ED%8C%8C%EC%B9%98-%EC%8A%A4%ED%8C%8C%ED%81%AC%EC%9D%98-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EA%B4%80%EB%A6%AC%EC%97%90-%EB%8C%80%ED%95%B4%EC%84%9C\">https://velog.io/@busybean3/Apache-Spark-%EC%95%84%ED%8C%8C%EC%B9%98-%EC%8A%A4%ED%8C%8C%ED%81%AC%EC%9D%98-%EB%A9%94%EB%AA%A8%EB%A6%AC-%EA%B4%80%EB%A6%AC%EC%97%90-%EB%8C%80%ED%95%B4%EC%84%9C</a></li>\n</ul>","id":"fb3c924f-4a7c-50c2-8404-09a28f1e98ec","frontmatter":{"date":"2023-04-07","path":"/data-engineering/spark-performance-tuning-1","title":"Spark 성능 튜닝 - 1. Partition, Shuffle","tags":["Data-Engineering"],"keyword":"Spark, 성능 튜닝, Partition, Shuffle","summary":"Spark에 성능 튜닝을 시도 해 보자.","img":"/post_image/thumbnail/spark-performance-tuning-1.png","series":null}}},"pageContext":{"postPath":"/data-engineering/spark-performance-tuning-1","series":{"data":{"allMarkdownRemark":{"edges":[]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"fb3c924f-4a7c-50c2-8404-09a28f1e98ec","excerpt":"오늘은 Spark의 성능 튜닝에 대해서 이야기 해 보겠습니다. Spark는 요약해서 말하면, **in-memory(RAM 위에서)**에서 작동 하는 분산 컴퓨팅을 쉽게 지원해 주는 프레임워크 입니다. in-memory 연산은 빠르지만, 불안정 합니다. 메모리 관리, CPU Core 수의 관리를 통해 Out of memory가 발생 하지 않는 선에서, Job…","frontmatter":{"date":"2023-04-07","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-1","title":"Spark 성능 튜닝 - 1. Partition, Shuffle","img":"/post_image/thumbnail/spark-performance-tuning-1.png","summary":"Spark에 성능 튜닝을 시도 해 보자."}}},{"node":{"id":"7611f6af-dc48-595b-8866-23beee4d5cda","excerpt":"저번 시간에는 Spark On Kubernetes에 대한 이론을 배웠습니다. 오늘은 Spark On Kubernetes에 대한 실습을 진행 하도록 하겠습니다. 사전 준비 Docker Minikube (Kubernetes 1.20 버전 이상) kubectl Spark 3.0 버전 이상 최신 버전일 수록 좋습니다. 얼마 전에 구형 Docker가 깔려 있는 맥북에서 진행을 해 봤는데 Pod이 생성이 안되더군요.. Pyspark Image Build…","frontmatter":{"date":"2023-03-30","tags":["Data-Engineering","Cloud-Computing"],"path":"/data-engineering/spark-on-k8s-2","title":"Spark on Kubernetes - Practice","img":"/post_image/thumbnail/spark-on-k8s-2.png","summary":"Spark를 Kubernetes Cluster에서 동작 시켜 보자."}}},{"node":{"id":"3a543178-c1c9-5cfb-9c9e-b89aed5e8ead","excerpt":"안녕하세요? 오늘은 Kubernetes 환경에 JupyterHub를 설치 하는 방법에 대해서 알아 보도록 하겠습니다. Kubernetes Kubernetes는 컨테이너를 쉽고 빠르게 배포/확장하고 관리를 자동화해주는 Container Orchestration Tool 입니다. Kubernetes 시스템을 통해, 다음을 제공 받을 수 있습니다. 서비스 디스커버리와 로드 밸런싱: DNS 이름, 혹은 자체 IP…","frontmatter":{"date":"2023-03-25","tags":["Data-Engineering","Cloud-Computing"],"path":"/data-engineering/jupyterhub-on-k8s","title":"Jupyterhub on Kubernetes","img":"/post_image/thumbnail/jupyterhub-on-k8s.jpg","summary":"Jupyterhub를 Kubernetes Cluster에서 실행 해 보자."}}},{"node":{"id":"6108ea9c-22bb-5693-995b-aebd5cfb3bcf","excerpt":"Spark Apache Spark는 기존 Hadoop의 MapReduce 형태의 클러스터 컴퓨팅의 단점을 보완하기 위해 탄생한 프레임워크 입니다. 기존 하둡의 MapReduce에서는 Disk에서 데이터를 읽은 후, Mapping, Shuffling, Reducing의 과정을 거쳐서, 다시 Disk에 저장하는 형식으로 진행 되는데요, 이는 Disk I/O가 자주 발생 하기 때문에, 속도가 상대적으로 느리다는 단점이 있습니다. 하지만 Apache…","frontmatter":{"date":"2023-03-06","tags":["Data-Engineering","Cloud-Computing"],"path":"/data-engineering/spark-on-k8s-1","title":"Spark on Kubernetes - Concept","img":"/post_image/thumbnail/spark-on-k8s-1.jpeg","summary":"Spark를 Kubernetes Cluster에서 동작 시켜 보자."}}}]}}}}},"staticQueryHashes":["2876327880","63159454"],"slicesMap":{}}