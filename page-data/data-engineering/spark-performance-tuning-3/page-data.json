{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/data-engineering/spark-performance-tuning-3/","result":{"data":{"markdownRemark":{"html":"<p>오늘은 <strong>Spark</strong> 성능 튜닝에서 가장 중요한 <strong>SQL Tuning</strong>에 대해서 알아 보도록 하겠습니다.</p>\n<p>사실 파라미터(Shuffle Partition 갯수, Executor Instance, Core, Memory 조정) 튜닝 또한, 도움이 될 수 있겠습니다만, 그 전에 <strong>Execution Plan</strong>이 잘 짜여져 있지 않다면, 파라미터 튜닝이 큰 영향을 주지는 못할 것 입니다.</p>\n<h3>What is Execution Plan?</h3>\n<p><strong>Exection Plan</strong>은 뭘까요? 우리가 <strong>Spark SQL API</strong>인 <strong>Dataframe Aggregation Code</strong>를 작성 했다고 가정 하겠습니다. 그렇게 되면, Spark 내부의 <strong>Catalyst Optimizer</strong>는 다음과 같은 동작을 수행 합니다.</p>\n<ul>\n<li>DataFrame, SQL, RDD로 작성 된 코드를 최적화 되지 않은 <strong>Unresolved Logical Plan</strong>으로 변환 합니다.</li>\n<li>Schema 등을 확인 하여, 해당 <strong>Unresolved Logical Plan</strong>이 Spark 상에서 가동 가능 하다는 판단이 되면, 이를 <strong>Logical Plan</strong>으로 전환 하고, 내부에 구현되어 있는 <strong>Optimizing Rules</strong>에 기반하여, 이를 최적화된 <strong>Optimized Logical Plan</strong>으로 구현 합니다.</li>\n<li>최적화 된 <strong>Optimized Logical Plan</strong>을 각 Executor에 전달하여 수행 합니다.</li>\n</ul>\n<p align=\"center\">\n    <img src=\"/post_image/spark-performance-tuning/3-1.jpeg\" max-width=\"400px\" />\n    <div align=\"center\" color=\"#aaaaaa\">Execution Plan이 생성되는 과정</div>\n</p>\n<p>우리는 실제로 연산이 되는 것을 어떻게 확인 할 수 있을까요? 우리는 <code class=\"language-text\">explain()</code> 메서드를 통해, 논리적으로 어떻게 연산이 진행 되는지 확인 할 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token keyword\">import</span> <span class=\"token namespace\">spark<span class=\"token punctuation\">.</span>implicits<span class=\"token punctuation\">.</span></span>_\n\n<span class=\"token keyword\">val</span> data <span class=\"token operator\">=</span> Seq<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"1\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">20000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">\"1\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">30000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">\"2\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">\"2\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">20000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">\"3\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">\"3\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">val</span> testDf <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>createDataFrame<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>toDF<span class=\"token punctuation\">(</span><span class=\"token string\">\"id\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"score\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">val</span> resultDf <span class=\"token operator\">=</span> testDf<span class=\"token punctuation\">.</span>groupBy<span class=\"token punctuation\">(</span>col<span class=\"token punctuation\">(</span><span class=\"token string\">\"id\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>agg<span class=\"token punctuation\">(</span>sum<span class=\"token punctuation\">(</span><span class=\"token string\">\"score\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nresultDf<span class=\"token punctuation\">.</span>explain<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[id#16], functions=[sum(score#17)])\n   +- Exchange hashpartitioning(id#16, 200), ENSURE_REQUIREMENTS, [plan_id=11]\n      +- HashAggregate(keys=[id#16], functions=[partial_sum(score#17)])\n         +- LocalTableScan [id#16, score#17]</code></pre></div>\n<p>혹은 실제로 Physical Plan (Execution Plan)이 실행 된 이후, Spark Web UI의 SQL에서 확인 해 볼 수도 있습니다.</p>\n<p>실제로 Web UI를 잘 이용하면 좋은 것이, 각 연산 Job, Stage 별로 각 Executor 마다 얼마 만큼의 <strong>Data Input / Output</strong>이 발생 했고, <strong>Shuffle</strong>이 얼마나 발생 했는지, <strong>Data Spill</strong>은 얼마나 있었는지를 확인 할 수 있기 때문에, 튜닝 포인트를 찾을 때 유용 합니다.</p>\n<p align=\"center\">\n    <img src=\"/post_image/spark-performance-tuning/3-2.png\" width=\"400px\" />\n    <div align=\"center\" color=\"#aaaaaa\">Spark Web UI - SQL</div>\n</p>\n<p align=\"center\">\n    <img src=\"/post_image/spark-performance-tuning/3-2.png\" width=\"400px\" />\n    <div align=\"center\" color=\"#aaaaaa\">Spark Web UI - Stages</div>\n</p>\n<h3>How To Optimize Query</h3>\n<p>일단 일차적으로, 알고리즘 문제를 푼다고 생각하고, <strong>어떻게 해야 연산량을 줄일 수 있을까?</strong> 라는 생각으로 접근 하셔야 합니다. 도움이 될 만한 몇 가지 테크닉이 있습니다.</p>\n<h4>Select Column &#x26; Filtering First</h4>\n<p>Spark Application에서 <strong>Memory 점유율을 낮추기 위해</strong>, <strong>Shuffle Data의 크기를 줄이기 위해</strong> <strong>필요한 Column만 추출</strong> 하는 것이 중요 합니다., 또한, 데이터의 Record 갯수를 줄일 수 있는 <strong>Filtering</strong>은 맨 앞단에서 진행을 시켜 주는 것이 중요 합니다.</p>\n<h4>Adaptive Query Execution</h4>\n<p><strong>Adaptive Query Execution</strong>을 이용하면, Spark가 Runtime에 <strong>Execution Plan</strong>을 변경 하게 할 수 있습니다. 이는 <strong>Spark Application</strong>에서 Runtime 도중의 Data 통계를 수집하여 다음과 같은 효과를 얻을 수 있습니다.</p>\n<ul>\n<li>Query 수행 간 <strong>효율적인 Partition 갯수를 설정</strong>하여 줌</li>\n<li><strong>Skew(특정 Partition에 데이터가 몰리는 현상) data</strong> 같은 경우에 대해 <strong>Partition 증설</strong></li>\n<li>Join시 <strong>Broadcast Join</strong> 혹은 <strong>Sort Merge Join</strong>의 사용을 결정</li>\n</ul>\n<p>이는 <code class=\"language-text\">spark.sql.adaptive.enabled</code> 설정을 <code class=\"language-text\">true</code> 로 지정 하여 주면 됩니다.</p>\n<h4>Use Only Spark SQL API</h4>\n<p>가능한 UDF 함수를 사용 하지 않고, <strong>Spark SQL 내부의 함수</strong>를 이용하여, Logical Logic이 <strong>Optimizer를 태울 수</strong> 있도록 해 주는 것이 중요 합니다. 또한, 일반 <strong>Dataset, RDD</strong>를 사용 하는 것 보다는 <strong>Catalyst Optimzer</strong>에 최적화 된 <strong>DataFrame</strong>을 사용 하는 것이 <strong>Memory와 CPU의 효율성을 극대화</strong> 할 수 있습니다.</p>\n<h4>coalesce() vs repartition()</h4>\n<p>Partition 갯수를 늘리는 것이 아니라면, <code class=\"language-text\">coalesce()</code>를 사용 하는 것이 더 좋습니다. <code class=\"language-text\">coalesce()</code>는 Partition 갯수를 줄이는 데 있어, <code class=\"language-text\">repartiton()</code>의 최적화 된 버전입니다.</p>\n<h4>Etc</h4>\n<p>기타 중요한 테크닉은 다음과 같습니다.</p>\n<ul>\n<li>Network I/O를 많이 사용하게 되는 <strong>Shuffle 연산을 최소화</strong> 할 것.</li>\n<li>만들어진 <strong>DataFrame을 재사용</strong> 하게 된다면, <strong>Cache를 잘 사용</strong> 할 것.</li>\n</ul>\n<h3>Reference</h3>\n<ul>\n<li><a href=\"https://sparkbyexamples.com/spark/spark-execution-plan/\">https://sparkbyexamples.com/spark/spark-execution-plan/</a></li>\n<li><a href=\"https://sparkbyexamples.com/spark/spark-performance-tuning/\">https://sparkbyexamples.com/spark/spark-performance-tuning/</a></li>\n</ul>","id":"f2d9685d-9388-518e-b7b2-c03a527f3c85","frontmatter":{"date":"2023-08-22","path":"/data-engineering/spark-performance-tuning-3","title":"Spark 성능 튜닝 - 3. Spark SQL Tuning","tags":["Data-Engineering"],"keyword":"Spark, 성능 튜닝, spark cache, spark SQL, spark query","summary":"Spark의 쿼리를 튜닝 해 보자","img":"/post_image/thumbnail/spark-performance-tuning-3.jpeg","series":"Spark Performance Tuning"}}},"pageContext":{"postPath":"/data-engineering/spark-performance-tuning-3","series":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"f2d9685d-9388-518e-b7b2-c03a527f3c85","excerpt":"오늘은 Spark 성능 튜닝에서 가장 중요한 SQL Tuning에 대해서 알아 보도록 하겠습니다. 사실 파라미터(Shuffle Partition 갯수, Executor Instance, Core, Memory 조정) 튜닝 또한, 도움이 될 수 있겠습니다만, 그 전에 Execution Plan이 잘 짜여져 있지 않다면, 파라미터 튜닝이 큰 영향을 주지는 못할 것 입니다. What is Execution Plan? Exection Plan…","frontmatter":{"date":"2023-08-22","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-3","title":"Spark 성능 튜닝 - 3. Spark SQL Tuning","img":"/post_image/thumbnail/spark-performance-tuning-3.jpeg","summary":"Spark의 쿼리를 튜닝 해 보자"}}},{"node":{"id":"2f3abeaf-1ba1-5817-8329-0ef461b07b9c","excerpt":"오늘은 Spark 성능 튜닝에 필요한, 와  에 대해서 알아 보도록 하겠습니다. RDD는 Transformation (ex: , ,  등)을 이용 하여 새로운 RDD를 만들 수 있습니다. 하지만, Action (ex: , ,  등)이 호출 되기 전까지는, 실제 연산을 수행 하지 않죠. 다음 예제를 함께 봅시다! 하지만, 우리가 다음과 같은 상황을 가정해 보죠. 만약, 동일하게 Transformation 된 RDD에 대해, 여러 개의 Action…","frontmatter":{"date":"2023-05-24","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-2","title":"Spark 성능 튜닝 - 2. cache(), persist(), unpersist()","img":"/post_image/thumbnail/spark-performance-tuning-2.jpeg","summary":"Spark의 Performance 튜닝을 수행 해 보자."}}},{"node":{"id":"fb3c924f-4a7c-50c2-8404-09a28f1e98ec","excerpt":"오늘은 Spark의 성능 튜닝에 대해서 이야기 해 보겠습니다. Spark는 요약해서 말하면, **in-memory(RAM 위에서)**에서 작동 하는 분산 컴퓨팅을 쉽게 지원해 주는 프레임워크 입니다. in-memory 연산은 빠르지만, 불안정 합니다. 메모리 관리, CPU Core 수의 관리를 통해 Out of memory가 발생 하지 않는 선에서, Job…","frontmatter":{"date":"2023-04-07","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-1","title":"Spark 성능 튜닝 - 1. Partition, Shuffle","img":"/post_image/thumbnail/spark-performance-tuning-1.png","summary":"Spark에 성능 튜닝을 시도 해 보자."}}}]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"f2d9685d-9388-518e-b7b2-c03a527f3c85","excerpt":"오늘은 Spark 성능 튜닝에서 가장 중요한 SQL Tuning에 대해서 알아 보도록 하겠습니다. 사실 파라미터(Shuffle Partition 갯수, Executor Instance, Core, Memory 조정) 튜닝 또한, 도움이 될 수 있겠습니다만, 그 전에 Execution Plan이 잘 짜여져 있지 않다면, 파라미터 튜닝이 큰 영향을 주지는 못할 것 입니다. What is Execution Plan? Exection Plan…","frontmatter":{"date":"2023-08-22","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-3","title":"Spark 성능 튜닝 - 3. Spark SQL Tuning","img":"/post_image/thumbnail/spark-performance-tuning-3.jpeg","summary":"Spark의 쿼리를 튜닝 해 보자"}}},{"node":{"id":"2f3abeaf-1ba1-5817-8329-0ef461b07b9c","excerpt":"오늘은 Spark 성능 튜닝에 필요한, 와  에 대해서 알아 보도록 하겠습니다. RDD는 Transformation (ex: , ,  등)을 이용 하여 새로운 RDD를 만들 수 있습니다. 하지만, Action (ex: , ,  등)이 호출 되기 전까지는, 실제 연산을 수행 하지 않죠. 다음 예제를 함께 봅시다! 하지만, 우리가 다음과 같은 상황을 가정해 보죠. 만약, 동일하게 Transformation 된 RDD에 대해, 여러 개의 Action…","frontmatter":{"date":"2023-05-24","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-2","title":"Spark 성능 튜닝 - 2. cache(), persist(), unpersist()","img":"/post_image/thumbnail/spark-performance-tuning-2.jpeg","summary":"Spark의 Performance 튜닝을 수행 해 보자."}}},{"node":{"id":"9dbd0b47-b274-5813-960b-805e0f92e2d0","excerpt":"안녕하세요? 오늘은 HDFS의 Architecture에 대해서 알아 보도록 하겠습니다. Hadoop Distributed File System(HDFS) 는 상용 하드웨어에서 동작하게 만든 오픈소스 SW입니다. 장애 발생에 강하며, 저비용 하드웨어 안에서도, 잘 작동 하게 설계 되었습니다. 또한, 많은 데이터 셋을 지닌 어플리케이션에 적합하며, 높은 throughput을 가지고 있습니다. Assumptions and Goals HDFS…","frontmatter":{"date":"2023-05-16","tags":["Data-Engineering"],"path":"/data-engineering/hdfs-architecture","title":"HDFS Architectrue","img":"/post_image/thumbnail/hdfs-architecture.png","summary":"HDFS의 구조에 대해서 알아보자."}}},{"node":{"id":"c7182a1e-4786-5112-9048-54cca7467f6d","excerpt":"안녕하세요? 오늘은 Spark Structured Streaming에 대해서 알아 보도록 하겠습니다. Spark Structured Streaming이란? Spark Structured Streaming은, Spark SQL (Dataset/DataFrame) 엔진 기반의, 확장 가능하고, 내결함성이 있는 Stream Processing Engine 입니다. 이는 Batch 작업에서 구조화된 데이터를 처리 하는 것 처럼, Streaming…","frontmatter":{"date":"2023-05-13","tags":["Data-Engineering"],"path":"/data-engineering/spark-structured-streaming","title":"Spark Structured Streaming이란?","img":"/post_image/thumbnail/spark-structured-streaming.png","summary":"Spark로 Streaming Job을 수행 해 보자."}}}]}}}}},"staticQueryHashes":["2876327880","63159454"],"slicesMap":{}}