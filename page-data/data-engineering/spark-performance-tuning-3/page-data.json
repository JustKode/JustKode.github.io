{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/data-engineering/spark-performance-tuning-3/","result":{"data":{"markdownRemark":{"html":"<p>오늘은 <strong>Spark</strong> 성능 튜닝에서 가장 중요한 <strong>SQL Tuning</strong>에 대해서 알아 보도록 하겠습니다.</p>\n<p>사실 파라미터(Shuffle Partition 갯수, Executor Instance, Core, Memory 조정) 튜닝 또한, 도움이 될 수 있겠습니다만, 그 전에 <strong>Execution Plan</strong>이 잘 짜여져 있지 않다면, 파라미터 튜닝이 큰 영향을 주지는 못할 것 입니다.</p>\n<h3>What is Execution Plan?</h3>\n<p><strong>Exection Plan</strong>은 뭘까요? 우리가 <strong>Spark SQL API</strong>인 <strong>Dataframe Aggregation Code</strong>를 작성 했다고 가정 하겠습니다. 그렇게 되면, Spark 내부의 <strong>Catalyst Optimizer</strong>는 다음과 같은 동작을 수행 합니다.</p>\n<ul>\n<li>DataFrame, SQL, RDD로 작성 된 코드를 최적화 되지 않은 <strong>Unresolved Logical Plan</strong>으로 변환 합니다.</li>\n<li>Schema 등을 확인 하여, 해당 <strong>Unresolved Logical Plan</strong>이 Spark 상에서 가동 가능 하다는 판단이 되면, 이를 <strong>Logical Plan</strong>으로 전환 하고, 내부에 구현되어 있는 <strong>Optimizing Rules</strong>에 기반하여, 이를 최적화된 <strong>Optimized Logical Plan</strong>으로 구현 합니다.</li>\n<li>최적화 된 <strong>Optimized Logical Plan</strong>을 각 Executor에 전달하여 수행 합니다.</li>\n</ul>\n<p align=\"center\">\n    <img src=\"/post_image/spark-performance-tuning/3-1.jpeg\" max-width=\"400px\" />\n    <div align=\"center\" color=\"#aaaaaa\">Execution Plan이 생성되는 과정</div>\n</p>\n<p>우리는 실제로 연산이 되는 것을 어떻게 확인 할 수 있을까요? 우리는 <code class=\"language-text\">explain()</code> 메서드를 통해, 논리적으로 어떻게 연산이 진행 되는지 확인 할 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token keyword\">import</span> <span class=\"token namespace\">spark<span class=\"token punctuation\">.</span>implicits<span class=\"token punctuation\">.</span></span>_\n\n<span class=\"token keyword\">val</span> data <span class=\"token operator\">=</span> Seq<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"1\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">20000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">\"1\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">30000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">\"2\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">\"2\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">20000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">\"3\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token string\">\"3\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">val</span> testDf <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>createDataFrame<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>toDF<span class=\"token punctuation\">(</span><span class=\"token string\">\"id\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"score\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">val</span> resultDf <span class=\"token operator\">=</span> testDf<span class=\"token punctuation\">.</span>groupBy<span class=\"token punctuation\">(</span>col<span class=\"token punctuation\">(</span><span class=\"token string\">\"id\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>agg<span class=\"token punctuation\">(</span>sum<span class=\"token punctuation\">(</span><span class=\"token string\">\"score\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nresultDf<span class=\"token punctuation\">.</span>explain<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[id#16], functions=[sum(score#17)])\n   +- Exchange hashpartitioning(id#16, 200), ENSURE_REQUIREMENTS, [plan_id=11]\n      +- HashAggregate(keys=[id#16], functions=[partial_sum(score#17)])\n         +- LocalTableScan [id#16, score#17]</code></pre></div>\n<p>혹은 실제로 Physical Plan (Execution Plan)이 실행 된 이후, Spark Web UI의 SQL에서 확인 해 볼 수도 있습니다.</p>\n<p>실제로 Web UI를 잘 이용하면 좋은 것이, 각 연산 Job, Stage 별로 각 Executor 마다 얼마 만큼의 <strong>Data Input / Output</strong>이 발생 했고, <strong>Shuffle</strong>이 얼마나 발생 했는지, <strong>Data Spill</strong>은 얼마나 있었는지를 확인 할 수 있기 때문에, 튜닝 포인트를 찾을 때 유용 합니다.</p>\n<p align=\"center\">\n    <img src=\"/post_image/spark-performance-tuning/3-2.png\" width=\"400px\" />\n    <div align=\"center\" color=\"#aaaaaa\">Spark Web UI - SQL</div>\n</p>\n<p align=\"center\">\n    <img src=\"/post_image/spark-performance-tuning/3-2.png\" width=\"400px\" />\n    <div align=\"center\" color=\"#aaaaaa\">Spark Web UI - Stages</div>\n</p>\n<h3>How To Optimize Query</h3>\n<p>일단 일차적으로, 알고리즘 문제를 푼다고 생각하고, <strong>어떻게 해야 연산량을 줄일 수 있을까?</strong> 라는 생각으로 접근 하셔야 합니다. 도움이 될 만한 몇 가지 테크닉이 있습니다.</p>\n<h4>Select Column &#x26; Filtering First</h4>\n<p>Spark Application에서 <strong>Memory 점유율을 낮추기 위해</strong>, <strong>Shuffle Data의 크기를 줄이기 위해</strong> <strong>필요한 Column만 추출</strong> 하는 것이 중요 합니다., 또한, 데이터의 Record 갯수를 줄일 수 있는 <strong>Filtering</strong>은 맨 앞단에서 진행을 시켜 주는 것이 중요 합니다.</p>\n<h4>Adaptive Query Execution</h4>\n<p><strong>Adaptive Query Execution</strong>을 이용하면, Spark가 Runtime에 <strong>Execution Plan</strong>을 변경 하게 할 수 있습니다. 이는 <strong>Spark Application</strong>에서 Runtime 도중의 Data 통계를 수집하여 다음과 같은 효과를 얻을 수 있습니다.</p>\n<ul>\n<li>Query 수행 간 <strong>효율적인 Partition 갯수를 설정</strong>하여 줌</li>\n<li><strong>Skew(특정 Partition에 데이터가 몰리는 현상) data</strong> 같은 경우에 대해 <strong>Partition 증설</strong></li>\n<li>Join시 <strong>Broadcast Join</strong> 혹은 <strong>Sort Merge Join</strong>의 사용을 결정</li>\n</ul>\n<p>이는 <code class=\"language-text\">spark.sql.adaptive.enabled</code> 설정을 <code class=\"language-text\">true</code> 로 지정 하여 주면 됩니다.</p>\n<h4>Use Only Spark SQL API</h4>\n<p>가능한 UDF 함수를 사용 하지 않고, <strong>Spark SQL 내부의 함수</strong>를 이용하여, Logical Logic이 <strong>Optimizer를 태울 수</strong> 있도록 해 주는 것이 중요 합니다. 또한, 일반 <strong>Dataset, RDD</strong>를 사용 하는 것 보다는 <strong>Catalyst Optimzer</strong>에 최적화 된 <strong>DataFrame</strong>을 사용 하는 것이 <strong>Memory와 CPU의 효율성을 극대화</strong> 할 수 있습니다.</p>\n<h4>coalesce() vs repartition()</h4>\n<p>Partition 갯수를 늘리는 것이 아니라면, <code class=\"language-text\">coalesce()</code>를 사용 하는 것이 더 좋습니다. <code class=\"language-text\">coalesce()</code>는 Partition 갯수를 줄이는 데 있어, <code class=\"language-text\">repartiton()</code>의 최적화 된 버전입니다.</p>\n<h4>Etc</h4>\n<p>기타 중요한 테크닉은 다음과 같습니다.</p>\n<ul>\n<li>Network I/O를 많이 사용하게 되는 <strong>Shuffle 연산을 최소화</strong> 할 것.</li>\n<li>만들어진 <strong>DataFrame을 재사용</strong> 하게 된다면, <strong>Cache를 잘 사용</strong> 할 것.</li>\n</ul>\n<h3>Reference</h3>\n<ul>\n<li><a href=\"https://sparkbyexamples.com/spark/spark-execution-plan/\">https://sparkbyexamples.com/spark/spark-execution-plan/</a></li>\n<li><a href=\"https://sparkbyexamples.com/spark/spark-performance-tuning/\">https://sparkbyexamples.com/spark/spark-performance-tuning/</a></li>\n</ul>","id":"e4d8d6bf-2f58-5ff8-b46f-c47daed191dd","frontmatter":{"date":"2023-08-22","path":"/data-engineering/spark-performance-tuning-3","title":"Spark 성능 튜닝 - 3. Spark SQL Tuning","tags":["Data-Engineering"],"keyword":"Spark, 성능 튜닝, spark cache, spark SQL, spark query","summary":"Spark의 쿼리를 튜닝 해 보자","img":"/post_image/thumbnail/spark-performance-tuning-3.jpeg","series":"Spark Performance Tuning"}}},"pageContext":{"postPath":"/data-engineering/spark-performance-tuning-3","series":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"e4d8d6bf-2f58-5ff8-b46f-c47daed191dd","excerpt":"오늘은 Spark 성능 튜닝에서 가장 중요한 SQL Tuning에 대해서 알아 보도록 하겠습니다. 사실 파라미터(Shuffle Partition 갯수, Executor Instance, Core, Memory 조정) 튜닝 또한, 도움이 될 수 있겠습니다만, 그 전에 Execution Plan이 잘 짜여져 있지 않다면, 파라미터 튜닝이 큰 영향을 주지는 못할 것 입니다. What is Execution Plan? Exection Plan…","frontmatter":{"date":"2023-08-22","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-3","title":"Spark 성능 튜닝 - 3. Spark SQL Tuning","img":"/post_image/thumbnail/spark-performance-tuning-3.jpeg","summary":"Spark의 쿼리를 튜닝 해 보자"}}},{"node":{"id":"6466dd9e-f217-5113-909a-150f27b482f3","excerpt":"오늘은 Spark 성능 튜닝에 필요한, 와  에 대해서 알아 보도록 하겠습니다. RDD는 Transformation (ex: , ,  등)을 이용 하여 새로운 RDD를 만들 수 있습니다. 하지만, Action (ex: , ,  등)이 호출 되기 전까지는, 실제 연산을 수행 하지 않죠. 다음 예제를 함께 봅시다! 하지만, 우리가 다음과 같은 상황을 가정해 보죠. 만약, 동일하게 Transformation 된 RDD에 대해, 여러 개의 Action…","frontmatter":{"date":"2023-05-24","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-2","title":"Spark 성능 튜닝 - 2. cache(), persist(), unpersist()","img":"/post_image/thumbnail/spark-performance-tuning-2.jpeg","summary":"Spark의 Performance 튜닝을 수행 해 보자."}}},{"node":{"id":"6ac732dc-2528-5e90-b1f8-2d142ca3af25","excerpt":"오늘은 Spark의 성능 튜닝에 대해서 이야기 해 보겠습니다. Spark는 요약해서 말하면, **in-memory(RAM 위에서)**에서 작동 하는 분산 컴퓨팅을 쉽게 지원해 주는 프레임워크 입니다. in-memory 연산은 빠르지만, 불안정 합니다. 메모리 관리, CPU Core 수의 관리를 통해 Out of memory가 발생 하지 않는 선에서, Job…","frontmatter":{"date":"2023-04-07","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-1","title":"Spark 성능 튜닝 - 1. Partition, Shuffle","img":"/post_image/thumbnail/spark-performance-tuning-1.png","summary":"Spark에 성능 튜닝을 시도 해 보자."}}}]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"ac2612fc-bbdf-504f-94d9-d2bab40e68e6","excerpt":"안녕하세요? 박민재입니다. 오늘날의 Data Engineer…","frontmatter":{"date":"2025-03-26","tags":["Data-Engineering"],"path":"/data-engineering/dbt-intro","title":"dbt란 무엇인가?","img":"/post_image/thumbnail/dbt-intro.webp","summary":"SQL을 바탕으로 Data Transformation Pipeline을 구성해 주는 dbt를 알아보자."}}},{"node":{"id":"dec9e599-9444-556f-99ba-7a1dc27a4dbb","excerpt":"안녕하세요, 박민재입니다. 혹시 Data Discovery에 중요성을 느껴, DataHub를 사용하려고 하시는 분이 있나요? 아마 그렇다면 DataHub를 도입한 사례를 몇 개 읽어 보셨을꺼라 생각합니다. 대표적으로 국내 기업에서는 뱅크샐러드, 소카, 베이글코드 등에서 성공적으로 도입한 사례들을 회사 사이트에 올리는 경우를 확인 할 수 있었어요. SOCAR BankSalad BagelCode DataHub…","frontmatter":{"date":"2025-03-02","tags":["Data-Engineering"],"path":"/data-engineering/to-datahub-user","title":"DataHub 도입을 고려 하시는 분들에게","img":"/post_image/thumbnail/to-datahub-user.webp","summary":"DataHub를 도입 하려고 할 때 알아야 할 점"}}},{"node":{"id":"24718dd5-aa23-578b-aa81-0ca11fcc0f06","excerpt":"안녕하세요, 박민재입니다. 저번 시간에는 Spark Operator가 무엇인지 간단하게 알아 보았는데요, 이번 시간에는 실제로 Spark Operator Helm Chart를 설치하여, Spark Operator 관련 구동 준비를 한 후, Spark Operator 관련 Resource를 작성 하여 실제 Job을 제출 해 보는 시간을 가져 보도록 하겠습니다. Spark Operator Helm Chart Spark Operator…","frontmatter":{"date":"2025-02-02","tags":["Data-Engineering"],"path":"/data-engineering/spark-operator-2","title":"Spark Operator - 2. Practice","img":"/post_image/thumbnail/spark-operator.jpg","summary":"Spark Operator를 사용 해 보자."}}},{"node":{"id":"9ec3e659-5978-5311-b4d5-fc9d0902e008","excerpt":"안녕하세요, 박민재입니다. 아마 2년 전 즈음에 Spark on Kubernetes 관련 내용을 다뤘었는데요 (이 글 또한, 개정판을 작성 해 볼게요), 이번에는 Spark Job을 Kubernetes Cluster에 편리하게 제출할 수 있게 하는 Spark Operator에 대해 알아 보도록 하겠습니다. Spark on Kubernetes를 사용하는 이유? 그렇다고, 이 글에서 아예 설명 하지 않고 넘어가는 것은 아닌 것 같아, Spark…","frontmatter":{"date":"2025-01-19","tags":["Data-Engineering"],"path":"/data-engineering/spark-operator-1","title":"Spark Operator - 1. Spark Operator란?","img":"/post_image/thumbnail/spark-operator.jpg","summary":"Kubernetes Cluster로의 Spark Job 제출을 도와주는 Spark Operator가 무엇 인지 알아보자."}}}]}}}}},"staticQueryHashes":["3819017183","63159454"],"slicesMap":{}}