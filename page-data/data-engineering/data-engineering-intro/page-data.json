{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/data-engineering/data-engineering-intro/","result":{"data":{"markdownRemark":{"html":"<h3>Summary</h3>\n<ul>\n<li><strong>Data Engineering</strong>이 구체적으로 무엇인지 알아 봅니다.</li>\n<li>왜 <strong>Data Engineering</strong>이 필요 하게 되었는지 알아 봅니다.</li>\n<li><strong>Data Engineer</strong>는 팀 혹은 회사에서 어떤 역할을 수행 하는지 알아 봅니다.</li>\n</ul>\n<h3>머릿말</h3>\n<p>안녕하세요? <strong>JustKode, 박민재</strong>입니다. 오늘은 <strong>Data Engineering</strong>이 구체적으로 무엇 인지, <strong>Data Engineer</strong>는 왜 필요 하게 되었고, 어떤 역할을 구체적으로 수행 하는지 알아 보겠습니다.</p>\n<p><strong>Data Engineering</strong>은 무엇 일까요? 사실 양지로 나온지 얼마 되지 않은 직군이다 보니, 많은 사람들이 <strong>Data Engineer</strong>라는 직군을 <strong>Data Scientist</strong>, <strong>Data Analysist</strong>와 혼동을 하곤 합니다. (회사의 사정에 따라 병행하는 곳도 많습니다.) <strong>Data Engineering</strong>은 본론으로 넘어 가기 전, 간단하게 언급 하자면, <strong>'데이터를 가져와서 적절한 형태로 변환하여 저장하고, 데이터 과학자나 분석가 등이 사용 할 수 있도록 준비하는 것'</strong> 입니다. 이런 작업이 왜 필요하게 되었고, 어떤 식으로 서비스 운영에 기여 하는지, 이제 알아 보도록 하겠습니다.</p>\n<p align=\"center\">\n    <img src=\"/post_image/data-engineering-intro/1-0.png\" max-width=\"540px\" />\n    <div align=\"center\" color=\"#aaaaaa\">Data Scientist vs Data Engineering</div>\n</p>\n<h3>What is Data Engineering?</h3>\n<p>많은 사람들이 <strong>데이터 엔지니어링</strong>에 대해 정의한 공통된 맥락을 요약 하면, 아까 설명 드렸던 것처럼 <strong>'데이터를 가져와서 적절한 형태로 변환하여 저장하고, 데이터 과학자나 분석가 등이 사용 할 수 있도록 준비하는 것'</strong> 입니다.</p>\n<p>우리의 서비스에 AI, ML, 데이터 분석을 적용 하기 위해서는 어떤 것을 해야 할까요? 일단 제일 먼저 <strong>데이터 수집</strong>을 수행 해야 합니다. 이는 사용자의 사용 로그 및 이벤트 일 수도 있고, 사용자가 서비스에 업로드 한 게시물, 댓글, 사진, 영상 등 여러 가지가 될 수 있습니다.</p>\n<p>하지만, 로그 및 이벤트를 그대로 다른 엔지니어들이 사용 하게 하기는 어렵습니다. 만약 <strong>몇 백억 건의 이벤트</strong>가 저장 되어 있는 스토리지가 있다고 가정 할 때, 해당 스토리지에 <strong>필요 할 때마다 쿼리</strong>를 해야 한다면? 그렇게 되면, <strong>컴퓨팅 리소스가 너무 많이 사용</strong>이 될 것이기 때문에, 미리 사람들이 자주 조회하는 데이터에 대해서 미리 집계를 하여, <strong>OLAP(Online Analytical Processing) Platform</strong> 등을 이용, <strong>서비스 혹은 BI</strong> 에서 쉽게 사용 할 수 있게 하여야 할 것입니다.</p>\n<p>또한, ML 쪽이라고 다르진 않습니다. <strong>정제 되지 않은 서비스 로그</strong>를 <strong>ML Engineer</strong>들이 그대로 사용 하기에는 어려움이 존재 합니다. AI 모델의 Input <strong>데이터의 형식</strong>에도 맞춰 줘야 하고, 필요한 <strong>메타 데이터</strong>가 있다면 이에 맞춰 데이터를 조인하여 적재하는 기술 또한 필요하게 됩니다. <strong>실시간성이 중요</strong>하다면, <strong>더 빠르게 서빙</strong>이 될 수 있도록 하는 기술도 필요하겠네요!</p>\n<p>제가 생각하는 그림은 다음과 같아요.</p>\n<p align=\"center\">\n    <img src=\"/post_image/data-engineering-intro/1-1.png\" max-width=\"1080px\" />\n    <div align=\"center\" color=\"#aaaaaa\">Data Flow</div>\n</p>\n<p><strong>다양한 데이터의 원천</strong>으로 부터(Service에서 사용하는 DBMS, Event Receiver, Message Queue의 데이터 등) 정제되지 않은 많은 데이터를 <strong>수집</strong>하고, <strong>변환</strong>하고, 이를 다시 사용자들의 필요에 맞게 <strong>Data Lake, Data Warehouse, Data Mart, OLAP Platform</strong> 등에 <strong>적재</strong>합니다. 이를 줄여서 <strong>ETL (Extract, Transform, Load)</strong> 이라고 합니다. 그렇게 적재 된 데이터는 내부 사용자인 <strong>Data Anaysist, Data Scientist, ML Engineer</strong> 등과, 외부 사용자인 <strong>Application의 User와 SW Engineer</strong>가 사용하게 됩니다.</p>\n<h3>Why need Data Engineer?</h3>\n<p><strong>데이터 엔지니어</strong>들이 하던 업무들은 원래 <strong>서버 엔지니어</strong>들이 같이 수행 하던 업무였습니다. 하지만, 점점 빅데이터의 중요성이 대두 되고, 많은 회사에서 <strong>데이터 기반 의사결정</strong> 혹은 <strong>Application 내 AI 서비스, 혹은 빅데이터 기반 Analysis 서비스를 공격적으로 제공</strong> 할 필요성이 대두됨에 따라, <strong>Data Engineering</strong>에 대해서 특화된 사람들이 필요하게 되었죠. 그렇게 <strong>Data Engineer</strong>라는 직군이 생겨 나게 되었습니다. 그렇다면 <strong>데이터 엔지니어</strong>들은 어떤 역량을 가지고 있어야 할까요? 자세히 알아보기 이전에, 데이터 엔지니어들이 다루게 되는 <strong>데이터 생명 주기</strong>에 대해서 알아 보도록 하죠!</p>\n<h4>데이터 생명 주기</h4>\n<p><strong>데이터 생명 주기</strong>는 5단계로 나눌 수 있는데요, 여기서 <strong>데이터 엔지니어링 생명 주기</strong>에 해당하는 부분은 볼드체로 처리 하도록 하겠습니다.</p>\n<p align=\"center\">\n    <img src=\"/post_image/data-engineering-intro/1-2.png\" max-width=\"1080px\" />\n    <div align=\"center\" color=\"#aaaaaa\">Data Lifecycle</div>\n</p>\n<ul>\n<li>데이터 생성 (Generation)\n<ul>\n<li>DBMS, Messgae Queue 등의 <strong>원천 시스템</strong>으로부터 데이터의 원본이 생성 됩니다.</li>\n<li>데이터 엔지니어들은 <strong>원본 데이터의 생성 방식, 빈도 및 속도, 다양성</strong>을 실무적으로 이해 해야 합니다.</li>\n</ul>\n</li>\n<li><strong>데이터 저장 (Storage)</strong>\n<ul>\n<li>데이터를 저장하는 과정입니다. <strong>데이터의 접근 빈도</strong>을 기반으로 적절한 <strong>스토리지 시스템</strong>을 선택 하는 것이 관건입니다.</li>\n<li>또한, <strong>원본 저장</strong>이 목적인지, <strong>쿼리 수행</strong>이 목적인지에 따라 <strong>저장 방식</strong>이 달라 질 수 있습니다.</li>\n</ul>\n</li>\n<li><strong>데이터 수집 (Ingestion)</strong>\n<ul>\n<li>원천 시스템의 특징과, 데이터 저장 방법이 숙지 되었다면, 이에 맞춰, 원천 데이터로부터 <strong>필요한 데이터를 수집</strong>해야 합니다.</li>\n<li><strong>가장 큰 병목 현상</strong>이 일어날 가능성이 높습니다.</li>\n<li>서빙될 데이터의 <strong>SLA</strong> (Service-Level Agreement, 서비스 수준의 사용에 문제가 없을 시간)와 <strong>데이터의 용량</strong>에 따라서 <strong>Batch 기법 (데이터를 특정 시간 단위로 한꺼번에 수집)</strong> 혹은 <strong>Streaming 기법 (데이터를 실시간으로 수집)</strong> 중 하나를 선택 합니다.</li>\n<li>필요한 데이터를 <strong>원본 데이터에서 찾아 Pull</strong> 하는 방식과, 원본 데이터에서 <strong>데이터 스토리지로 Push</strong> 하는 방식 두 가지가 있습니다.</li>\n</ul>\n</li>\n<li><strong>데이터 변환 (Transformation)</strong>\n<ul>\n<li>데이터를 <strong>Down Stream</strong>에서 사용 하기 <strong>유용한 형태</strong>로 <strong>변환</strong> 합니다. 다운스트림 사용자의 <strong>데이터 소비를 위한 가치를 창출</strong>하는 단계입니다.</li>\n<li>데이터 수집 후 수행 되며, <strong>배치 단위</strong>로 수행 되거나, 스트리밍으로 데이터를 <strong>수집하면서 동시에 변환</strong>하는 방식도 사용 됩니다.</li>\n<li><strong>쿼리, 데이터 모델링, 데이터 품질 보장</strong>을 위한 작업들이 수행 됩니다.</li>\n</ul>\n</li>\n<li><strong>데이터 서빙 (Serving)</strong>\n<ul>\n<li>변환된 데이터를 바탕으로 <strong>가치를 창출</strong>하는 단계 입니다.</li>\n<li><strong>분석</strong> (BI, 운영 분석, 임베디드 분석), <strong>머신러닝</strong>, <strong>역 ETL</strong> 등에서 사용 됩니다.</li>\n</ul>\n</li>\n</ul>\n<p>이렇게 <strong>데이터 생명 주기</strong>를 나열해 보았는데요, 그럼 이제부터는 <strong>데이터 생명 주기</strong>를 바탕으로, <strong>데이터 엔지니어들이 필요로 하게 되는 역량</strong>들을 한 번 분석 해 보도록 하겠습니다.</p>\n<h3>Data Engineering에 필요한 역량들</h3>\n<h4>컴퓨터 과학 지식</h4>\n<p>먼저, 가장 기본적인 역량은 <strong>컴퓨터 과학 지식</strong>입니다. <strong>데이터를 저장할 스토리지</strong>를 고를 때도, 어떤 <strong>네트워크 프로토콜</strong>을 사용 하는지, <strong>in-memory 기반의 저장소</strong>를 사용 하는지 여부, 내부 <strong>인덱싱 알고리즘</strong>, <strong>파일 저장</strong>, <strong>캐싱</strong>은 어떤 식으로 이루어지는 지를 이해한 후, <strong>장단점을 파악하여 적절한 스토리지</strong>를 선택 할 수 있어야 합니다. 또한 ETL 과정 에서도, <strong>배치</strong>와 <strong>스트리밍</strong> 방식의 장단점을 파악하고 적절하게 활용 할 수 있어야 하며, 대용량의 데이터를 처리 하기 위한 <strong>분산 컴퓨팅 시스템 알고리즘</strong>에 대한 이해를 가지고 있어야 합니다. <strong>데이터를 정확하고 적절</strong>하게 처리 하는 것도 중요하지만, 적은 리소스로 <strong>수 많은 데이터를 처리</strong>해야 하기 때문에, <strong>운영체제, 알고리즘, 네트워크</strong>에 대한 지식은 필수적이라고 볼 수 있습니다,</p>\n<h4>도메인 지식</h4>\n<p>두 번째는 현재 운영 하는 서비스에 대한 <strong>도메인 지식</strong>입니다. 기본적인 <strong>도메인 지식</strong>이 없어 운영 되는 <strong>데이터의 의미</strong>를 모르게 된다면, 아무리 프로젝트 매니저가 존재하더라도, <strong>데이터 운영에 있어 애로 사항</strong>을 겪을 수 있습니다. 어떤 데이터가 <strong>자주 조회</strong>되고, 어떤 데이터가 <strong>어떤 비즈니스 로직</strong>으로부터 왔는지, 이 데이터는 다시 <strong>어떻게 사용 하기 위해 수집</strong> 되고 있는지 등에 대한 지식이 있어야 합니다. 그렇게 해야 데이터 파이프라인의 운영에 있어, <strong>중복되는 테이블은 병합</strong>하고, <strong>사용하지 않는 데이터는 과감하게 수집 하지 않고</strong>, <strong>사용자 입장</strong>에서 <strong>필요한 데이터</strong>만을 적극적으로 제공 해 주는 등, <strong>컴퓨팅 리소스 혹은 커뮤니케이션 리소스 최적화</strong>에 있어 큰 힘을 쓸 수 있습니다.</p>\n<h4>안정적인 운영을 위한 기술들</h4>\n<p>세 번째는 <strong>안정적인 운영을 위한 기술들</strong>입니다. 저는 크게 <strong>컨테이너/클라우드 기술</strong>과 <strong>모니터링 기술</strong>을 뽑을 수 있을 것 같습니다. 우리가 <strong>데이터 생명 주기</strong>내에서 데이터 엔지니어링 업무를 수행 하며, <strong>다양한 환경</strong>에서 실행 되는 수 많은 툴들을 사용 합니다. 하지만, 이를 단일 노드에서 <strong>환경 분리</strong> 없이 수행 하기는 너무나도 어려운 일이죠. 그렇기 때문에, 우리는 <strong>Docker</strong>를 기반으로 한 <strong>컨테이너</strong> 기술과, <strong>컨테이너</strong>의 <strong>안정적인 리소스 관리</strong>를 위한 <strong>Kubernetes</strong>와 같은 <strong>컨테이너 오케스트레이션</strong> 기술들을 배워 놓는 것이 좋습니다. 또한, <strong>Kubernetes 환경</strong>에서 효율적으로 패키지를 관리 하기 위한 <strong>Helm</strong> 같은 기술들도 필요 하게 될 것 같구요. <strong>ETL Job</strong>의 <strong>워크플로우를 관리</strong> 해 주는 <strong>Airflow</strong> 같은 프레임워크의 지식도 필요 할 것 입니다. 만약, 회사가 <strong>On-Premise</strong> 환경이 아니라면, <strong>AWS, GCP</strong>와 같은 환경에도 익숙 해 질 필요가 있을 거에요. <strong>Prometheus, Grafana</strong>와 같이 <strong>컴퓨팅 리소스 자체를 모니터링</strong> 할 수 있는 툴도 좋고, 데이터의 품질을 보장하기 위한, <strong>Data Quality</strong> 모니터링 관련 툴인 <strong>Great Expectation, Amazon Deequ</strong>도 활용 할 수 있다면 좋을 것입니다.</p>\n<h4>새로운 기술들</h4>\n<p><strong>Data Engineering</strong>을 수행 하며 직면하는 많은 문제들을 해결 하기 위해서, 우리는 <strong>끊임 없이 학습</strong> 하여야 합니다. 대충 우리가 알아야 하는 지식들은 다음과 같습니다.</p>\n<ul>\n<li>기본적인 <strong>Python, Java, Scala, Go</strong> 등의 <strong>데이터 엔지니어링</strong>에 기본적으로 사용 되는 언어</li>\n<li><strong>Streaming</strong>, <strong>Batch</strong> 데이터를 처리 하는 기술들 (<strong>Spark, Flink</strong> 등)</li>\n<li><strong>ETL Job Workflow</strong>를 관리 하는 기술들 (<strong>Airflow</strong> 등)</li>\n<li><strong>OLAP Platfrom</strong>의 운영을 위한 툴</li>\n<li><strong>HDFS Cluster</strong> 등의 여러 가지 데이터 스토리지의 지식</li>\n<li><strong>Docker Container</strong> 혹은 <strong>Kubernetes Cluster</strong>의 관리를 위한 기술들</li>\n<li><strong>Data Quality</strong>를 보장 하기 위한 툴들</li>\n<li><strong>Data Governance</strong>를 효과 적으로 관리 하기 위한 <strong>Data Discorvery</strong> 툴들</li>\n<li><strong>Apache Iceberg</strong> 등의 새로운 <strong>Table Format</strong></li>\n<li><strong>고가용성을 보장</strong>하기 위한 기술들</li>\n<li><strong>모니터링</strong>을 위한 툴들</li>\n<li><strong>Unit Test, Test Coverage</strong> 등, 코드 안정성을 보장 하기 위한 기술들</li>\n</ul>\n<p>어... 엄청 많네요, 필요한 상황에 맞게 우리가 운영 하는 컴포넌트가 <strong>주어진 요구 사항</strong>들을 문제 없이 수행 할 수 있도록, 해당 지식들에 대한 끊임 없는 학습을 하여야 합니다.</p>\n<h3>마치며</h3>\n<p>여기까지 긴 글 읽어 주신 여러분 모두 감사드립니다. 자세한 <strong>데이터 엔지니어링</strong> 로드맵은 <a href=\"https://github.com/datastacktv/data-engineer-roadmap\">해당 링크</a>의 Github Repository에서 확인 해 볼 수 있습니다. 혹시 잘못된 내용이 있거나, 피드백할 내용이 있다면 편하게 댓글 달아 주세요!</p>\n<h3>참고 문헌</h3>\n<ul>\n<li>견고한 데이터 엔지니어링 &#x3C;2023, 한빛미디어></li>\n</ul>","id":"5a1c922e-fff4-5443-87e1-15282d06c381","frontmatter":{"date":"2023-12-05","path":"/data-engineering/data-engineering-intro","title":"Data Engineering이란?","tags":["Data-Engineering"],"keyword":"Data Engineering, 데이터 엔지니어링, Big Data, 빅데이터","summary":"Data Engineering에 대하여","img":"/post_image/data-engineering-intro.png","series":null}}},"pageContext":{"postPath":"/data-engineering/data-engineering-intro","series":{"data":{"allMarkdownRemark":{"edges":[]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"5a1c922e-fff4-5443-87e1-15282d06c381","excerpt":"Summary Data Engineering이 구체적으로 무엇인지 알아 봅니다. 왜 Data Engineering이 필요 하게 되었는지 알아 봅니다. Data Engineer는 팀 혹은 회사에서 어떤 역할을 수행 하는지 알아 봅니다. 머릿말 안녕하세요? JustKode, 박민재입니다. 오늘은 Data Engineering이 구체적으로 무엇 인지, Data Engineer…","frontmatter":{"date":"2023-12-05","tags":["Data-Engineering"],"path":"/data-engineering/data-engineering-intro","title":"Data Engineering이란?","img":"/post_image/data-engineering-intro.png","summary":"Data Engineering에 대하여"}}},{"node":{"id":"f2d9685d-9388-518e-b7b2-c03a527f3c85","excerpt":"오늘은 Spark 성능 튜닝에서 가장 중요한 SQL Tuning에 대해서 알아 보도록 하겠습니다. 사실 파라미터(Shuffle Partition 갯수, Executor Instance, Core, Memory 조정) 튜닝 또한, 도움이 될 수 있겠습니다만, 그 전에 Execution Plan이 잘 짜여져 있지 않다면, 파라미터 튜닝이 큰 영향을 주지는 못할 것 입니다. What is Execution Plan? Exection Plan…","frontmatter":{"date":"2023-08-22","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-3","title":"Spark 성능 튜닝 - 3. Spark SQL Tuning","img":"/post_image/thumbnail/spark-performance-tuning-3.jpeg","summary":"Spark의 쿼리를 튜닝 해 보자"}}},{"node":{"id":"2f3abeaf-1ba1-5817-8329-0ef461b07b9c","excerpt":"오늘은 Spark 성능 튜닝에 필요한, 와  에 대해서 알아 보도록 하겠습니다. RDD는 Transformation (ex: , ,  등)을 이용 하여 새로운 RDD를 만들 수 있습니다. 하지만, Action (ex: , ,  등)이 호출 되기 전까지는, 실제 연산을 수행 하지 않죠. 다음 예제를 함께 봅시다! 하지만, 우리가 다음과 같은 상황을 가정해 보죠. 만약, 동일하게 Transformation 된 RDD에 대해, 여러 개의 Action…","frontmatter":{"date":"2023-05-24","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-2","title":"Spark 성능 튜닝 - 2. cache(), persist(), unpersist()","img":"/post_image/thumbnail/spark-performance-tuning-2.jpeg","summary":"Spark의 Performance 튜닝을 수행 해 보자."}}},{"node":{"id":"9dbd0b47-b274-5813-960b-805e0f92e2d0","excerpt":"안녕하세요? 오늘은 HDFS의 Architecture에 대해서 알아 보도록 하겠습니다. Hadoop Distributed File System(HDFS) 는 상용 하드웨어에서 동작하게 만든 오픈소스 SW입니다. 장애 발생에 강하며, 저비용 하드웨어 안에서도, 잘 작동 하게 설계 되었습니다. 또한, 많은 데이터 셋을 지닌 어플리케이션에 적합하며, 높은 throughput을 가지고 있습니다. Assumptions and Goals HDFS…","frontmatter":{"date":"2023-05-16","tags":["Data-Engineering"],"path":"/data-engineering/hdfs-architecture","title":"HDFS Architectrue","img":"/post_image/thumbnail/hdfs-architecture.png","summary":"HDFS의 구조에 대해서 알아보자."}}}]}}}}},"staticQueryHashes":["3819017183","63159454"],"slicesMap":{}}