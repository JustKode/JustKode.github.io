{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/data-engineering/iceberg-table-optimization-1/","result":{"data":{"markdownRemark":{"html":"<h3>Intro</h3>\n<p>안녕하세요, 박민재입니다. 오늘은 <strong>Apache Iceberg</strong>의 <strong>Table</strong>에 수행 되는 쿼리가 <strong>최적의 성능</strong>으로 작동 될 수 있도록, <strong>File Compaction</strong>을 통해 이를 수행하는 방법에 대해 이야기 하는 시간을 가져 보도록 하겠습니다.</p>\n<h3>File Compaction</h3>\n<p>우리가 쿼리를 수행 시, <strong>Hive Metastore</strong>의 정보를 이용하더라도, 혹은 <strong>Iceberg</strong>의 Metadata 정보를 보더라도, 우리는 <strong>결론적으로 단일 파일을 읽는 과정</strong>이 필요 하게 됩니다. 하지만, 만약 1GB 정도의 Table을 읽는 데 있어, 정보가 10000개의 파일로 쪼개져 있다면 어떤 문제가 발생 할까요? 이를 넘어서 백 만개의 파일로로 쪼개져 있다면요?</p>\n<p>그렇다면 우리는 <strong>파일 수 만큼의 File Operation</strong>을 수행 해야 될 것입니다. 10000개면 10000번의 File Open + Read를, 1000000개면 1000000번의 File Open + Read를 말이죠. 파일이 Hadoop 환경이라면 NameNode에 엄청난 부하를 가져오게 될 꺼에요.</p>\n<p>그렇기 때문에 우리는 <strong>File Compaction</strong>이 필요하게 됩니다.</p>\n<p align=\"center\">\n    <img src=\"/post_image/apache-iceberg/table-optimization/01.png\" style=\"width:75%;max-width:768px;\"/>\n</p>\n<h4>Example</h4>\n<p>우리는 Java, Python, 그리고 Spark SQL을 이용하여, <strong>SparkActions</strong> API를 호출하는 방식으로 File Compaction을 수행 할 수 있어요.</p>\n<p>FYI) Iceberg에서는 Spark Engine을 주력으로 지원 하지만, 다른 Computing Engine에서도 구현만 완료 된다면 사용 가능합니다.</p>\n<p>Java 예제입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"java\"><pre class=\"language-java\"><code class=\"language-java\"><span class=\"token class-name\">Table</span> table <span class=\"token operator\">=</span> catalog<span class=\"token punctuation\">.</span><span class=\"token function\">loadTable</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"myTable\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token class-name\">SparkActions</span>\n    <span class=\"token punctuation\">.</span><span class=\"token function\">get</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">.</span><span class=\"token function\">rewriteDataFiles</span><span class=\"token punctuation\">(</span>table<span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">.</span><span class=\"token function\">option</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"rewrite-job-order\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"files-desc\"</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">.</span><span class=\"token function\">execute</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n \n<span class=\"token class-name\">Table</span> table <span class=\"token operator\">=</span> catalog<span class=\"token punctuation\">.</span><span class=\"token function\">loadTable</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"myTable\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token class-name\">SparkActions</span>\n    <span class=\"token punctuation\">.</span><span class=\"token function\">get</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">.</span><span class=\"token function\">rewriteDataFiles</span><span class=\"token punctuation\">(</span>table<span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">.</span><span class=\"token function\">sort</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">.</span><span class=\"token function\">filter</span><span class=\"token punctuation\">(</span><span class=\"token class-name\">Expressions</span><span class=\"token punctuation\">.</span><span class=\"token function\">and</span><span class=\"token punctuation\">(</span>\n        <span class=\"token class-name\">Expressions</span><span class=\"token punctuation\">.</span><span class=\"token function\">greaterThanOrEqual</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"date\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"2023-01-01\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n        <span class=\"token class-name\">Expressions</span><span class=\"token punctuation\">.</span><span class=\"token function\">lessThanOrEqual</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"date\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"2023-01-31\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">.</span><span class=\"token function\">option</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"rewrite-job-order\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"files-desc\"</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">.</span><span class=\"token function\">execute</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>SparkSQL 예제 입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">CALL</span> catalog<span class=\"token punctuation\">.</span>system<span class=\"token punctuation\">.</span>rewrite_data_files<span class=\"token punctuation\">(</span>\n    <span class=\"token keyword\">table</span> <span class=\"token operator\">=</span><span class=\"token operator\">></span> <span class=\"token string\">'musicians'</span><span class=\"token punctuation\">,</span>\n    strategy <span class=\"token operator\">=</span><span class=\"token operator\">></span> <span class=\"token string\">'binpack'</span><span class=\"token punctuation\">,</span>\n    <span class=\"token keyword\">where</span> <span class=\"token operator\">=</span><span class=\"token operator\">></span> <span class=\"token string\">'genre = \"rock\"'</span><span class=\"token punctuation\">,</span>\n    options <span class=\"token operator\">=</span><span class=\"token operator\">></span> map<span class=\"token punctuation\">(</span>\n        <span class=\"token string\">'rewrite-job-order'</span><span class=\"token punctuation\">,</span><span class=\"token string\">'bytes-asc'</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">'target-file-size-bytes'</span><span class=\"token punctuation\">,</span><span class=\"token string\">'1073741824'</span><span class=\"token punctuation\">,</span> <span class=\"token comment\">-- 1GB</span>\n        <span class=\"token string\">'max-file-group-size-bytes'</span><span class=\"token punctuation\">,</span><span class=\"token string\">'10737418240'</span> <span class=\"token comment\">-- 10GB</span>\n    <span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span></code></pre></div>\n<h4>Options</h4>\n<p>대표적으로 Compaction 도중 다음과 같은 옵션을 삽입하여 수행 할 수 있습니다.</p>\n<ul>\n<li>Sort: 압축 과정에서 <strong>Sort</strong>를 수행 할 수 있습니다.</li>\n<li>zOrder: 압축 과정에서 <strong>zOrder Sort</strong>를 수행 할 수 있습니다.</li>\n<li>filter: 압축 과정에서 <strong>어떤 파일을 다시 쓸 지 선택</strong>할 수 있습니다. (ex: 특정 파티션에 대해서만 압축)</li>\n<li>option (단일 key-value), options (Map): 상기한 이외의 Option을 Map 형태로 추가 해 줄 수 있습니다.\n<ul>\n<li>target-file-size-bytes: <strong>원하는 압축 파일의 크기를 설정</strong> 해 줄수 있습니다. 기본 값은 Table 내의 property인 <strong>write.target.file-size-bytes</strong> (default: 512MB) 입니다.</li>\n<li>max-concurrent-file-group-rewrites: <strong>동시에 작성할 최대 File Group의 갯수</strong>를 결정합니다.</li>\n<li>max-file-group-size-bytes: <strong>File Group의 최대 파일 크기의 합</strong>을 결정 합니다. 파일 크기가 메모리 크기 보다 큰 경우, <strong>OOM을 방지</strong>하기 위해서 주로 사용합니다.</li>\n<li>partital-progress-enabled: <strong>Partial Progress로 압축 작업이 진행</strong> 되며, 파일 압축 작업이 오래 걸리게 될 때 사용하면 좋은 옵션입니다. <strong>파일 압축 중에 Commit을 허용</strong> 하며, 압축 중에 Read 쿼리가 발생 할 경우, <strong>이미 압축된 File Group을 사용</strong> 할 수 있게 됩니다.</li>\n<li>partial-progress-max-commits: 전체 <strong>Partial Progress</strong> 작업이 완료 될 때 까지, <strong>최대 Commit을 몇 회 허용</strong>할 지 설정 합니다.</li>\n<li>rewrite-job-order: 파일 압축 시, 어떤 <strong>File Group 먼저 Partial Progress를 수행</strong> 할 지 결정 합니다. (Option: bytes-asc, bytes-desc, files-asc, files-desc, none)</li>\n</ul>\n</li>\n</ul>\n<p>아래는 target-file-size-bytes: 1073741824 (1GB), max-file-group-size-bytes: 10737418240 (10GB)을 적용한 예시입니다.</p>\n<p align=\"center\">\n    <img src=\"/post_image/apache-iceberg/table-optimization/02.png\" style=\"width:75%;max-width:768px;\"/>\n</p>\n<h4>Partial Progress</h4>\n<p>Partial Progress는 특정 File Group에 대해 압축이 완료 되었을 때, <strong>부분적으로 압축된 파일에 대해 다른 Query에서 접근</strong>을 허용하는 것입니다. 즉, 전체 압축 Action이 끝날 때 까지 기다리는 것이 아닌, <strong>File Group에 대해서 압축이 끝날 때 마다 Commit을 수행</strong> 하는 거에요.</p>\n<p>이는 Trade-Off가 있는데요, 압축 과정 중간에 수행되는 Query에서도 File Compaction이 된 File에 대해 접근을 할 수 있게 하지만, 그만큼 더 많은 Snapshot이 생성되는 것이니, <strong>partial-progress-max-commits</strong> 같은 옵션으로 조정이 필요하게 됩니다.</p>\n<h3>Compaction Strategies</h3>\n<p>테이블의 파일을 압축하면서 선택 할 수 있는 많은 전략들이 존재합니다. 예를 들어, 다음과 같은 방법들이 있습니다.</p>\n<h4>Binpack</h4>\n<p>Binpack은 가장 기본적인 압축 방식입니다. <strong>정렬없이 파일 압축만을 수행</strong>하는 방법으로, Spark Structured Streaming 같은 <strong>Streaming Application</strong>에서 생성하는 Data가 저장되는 <strong>Streaming Table 같은 곳에서 사용</strong>하는 것이 유리합니다.</p>\n<blockquote>\n<p>만약, Streaming Table에 대해서 압축을 주기적으로 수행 한다면, <strong>Airflow, Serverless Function, Cronjob</strong> 등을 사용하는 것을 고려하는 것이 좋습니다.</p>\n</blockquote>\n<ul>\n<li>Pros: <strong>제일 빠른 압축 속도</strong>를 자랑합니다.</li>\n<li>Cons: 하지만 데이터의 정렬이 존재하지 않는 관계로, 해당 테이블에 대한 <strong>Query시 Full Scan을 수행</strong> 해야 한다는 문제점이 있습니다.</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">CALL</span> catalog<span class=\"token punctuation\">.</span>system<span class=\"token punctuation\">.</span>rewrite_data_files<span class=\"token punctuation\">(</span>\n    <span class=\"token keyword\">table</span> <span class=\"token operator\">=</span><span class=\"token operator\">></span> <span class=\"token string\">'streamingtable'</span><span class=\"token punctuation\">,</span>\n    strategy <span class=\"token operator\">=</span><span class=\"token operator\">></span> <span class=\"token string\">'binpack'</span><span class=\"token punctuation\">,</span>\n    <span class=\"token keyword\">where</span> <span class=\"token operator\">=</span><span class=\"token operator\">></span> <span class=\"token string\">'created_at between \"2023-01-26 09:00:00\" and \"2023-01-26 09:59:59\" '</span><span class=\"token punctuation\">,</span>\n    options <span class=\"token operator\">=</span><span class=\"token operator\">></span> map<span class=\"token punctuation\">(</span>\n        <span class=\"token string\">'rewrite-job-order'</span><span class=\"token punctuation\">,</span><span class=\"token string\">'bytes-asc'</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">'target-file-size-bytes'</span><span class=\"token punctuation\">,</span><span class=\"token string\">'1073741824'</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">'max-file-group-size-bytes'</span><span class=\"token punctuation\">,</span><span class=\"token string\">'10737418240'</span><span class=\"token punctuation\">,</span>\n        <span class=\"token string\">'partial-progress-enabled'</span><span class=\"token punctuation\">,</span> <span class=\"token string\">'true'</span>\n    <span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span></code></pre></div>\n<h4>Sorting</h4>\n<p>파일 압축 시, <strong>데이터를 단일 Column, 혹은 복수의 Column의 값을 바탕으로 정렬</strong>하여 저장하는 방식입니다. 쿼리 시 해당 Column을 사용하게 된다면, 성능이 올라가는 효과를 누릴 수 있습니다.</p>\n<p>Table을 생성 할 때, 다음과 같은 방식으로 생성 하게 되면, Data가 Write 될 때, <strong>자동으로 정렬 된 형태로 데이터가 삽입</strong> 되게 할 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">CREATE</span> <span class=\"token keyword\">TABLE</span> catalog<span class=\"token punctuation\">.</span>nfl_players <span class=\"token punctuation\">(</span>\n    id <span class=\"token keyword\">bigint</span><span class=\"token punctuation\">,</span>\n    player_name <span class=\"token keyword\">varchar</span><span class=\"token punctuation\">,</span>\n    team <span class=\"token keyword\">varchar</span><span class=\"token punctuation\">,</span>\n    num_of_touchdowns <span class=\"token keyword\">int</span><span class=\"token punctuation\">,</span>\n    num_of_yards <span class=\"token keyword\">int</span><span class=\"token punctuation\">,</span>\n    player_position <span class=\"token keyword\">varchar</span><span class=\"token punctuation\">,</span>\n    player_number <span class=\"token keyword\">int</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\">-- 해당 옵션을 통해 추후 입력되는 데이터는 정렬 된 채로 Write 됨</span>\n<span class=\"token keyword\">ALTER</span> <span class=\"token keyword\">TABLE</span> catalog<span class=\"token punctuation\">.</span>nfl_teams <span class=\"token keyword\">WRITE</span> ORDERED <span class=\"token keyword\">BY</span> team<span class=\"token punctuation\">;</span></code></pre></div>\n<p>하지만, 정렬 된 채로 Write가 된다고 하더라도, <strong>애매하게 작은 파일로 흩뿌려진 채로 Append</strong> 된다면 성능이 악화될 수 있습니다.\n따라서 <strong>Sort 옵션이 추가 된 Compaction</strong>을 통해, 더 작은 파일의 갯수로 Compaction 해 줄 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">CALL</span> catalog<span class=\"token punctuation\">.</span>system<span class=\"token punctuation\">.</span>rewrite_data_files<span class=\"token punctuation\">(</span>\n    <span class=\"token keyword\">table</span> <span class=\"token operator\">=</span><span class=\"token operator\">></span> <span class=\"token string\">'nfl_teams'</span><span class=\"token punctuation\">,</span>\n    strategy <span class=\"token operator\">=</span><span class=\"token operator\">></span> <span class=\"token string\">'sort'</span><span class=\"token punctuation\">,</span>\n    sort_order <span class=\"token operator\">=</span><span class=\"token operator\">></span> <span class=\"token string\">'team ASC NULLS LAST'</span>  <span class=\"token comment\">-- team Column에 대해 오름차순으로 정렬. null 값인 경우는 마지막에 저장 되도록.</span>\n<span class=\"token punctuation\">)</span></code></pre></div>\n<p align=\"center\">\n    <img src=\"/post_image/apache-iceberg/table-optimization/03.png\" style=\"width:75%;max-width:768px;\"/>\n</p>\n<p>다중의 Column에 대해서도 수행해 줄 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">CALL</span> catalog<span class=\"token punctuation\">.</span>system<span class=\"token punctuation\">.</span>rewrite_data_files<span class=\"token punctuation\">(</span>\n    <span class=\"token keyword\">table</span> <span class=\"token operator\">=</span><span class=\"token operator\">></span> <span class=\"token string\">'nfl_teams'</span><span class=\"token punctuation\">,</span>\n    strategy <span class=\"token operator\">=</span><span class=\"token operator\">></span> <span class=\"token string\">'sort'</span><span class=\"token punctuation\">,</span>\n    sort_order <span class=\"token operator\">=</span><span class=\"token operator\">></span> <span class=\"token string\">'team ASC NULLS LAST, name ASC NULLS FIRST'</span>\n    <span class=\"token comment\">-- team Column에 대해 오름차순으로 &amp;&amp; null 값인 경우는 마지막에 저장 되도록 정렬 후,</span>\n    <span class=\"token comment\">-- 동일한 team의 값을 가졌다면 name으로 오름 차순으로 null 값인 경우는 처음에 저장 되도록 정렬</span>\n<span class=\"token punctuation\">)</span></code></pre></div>\n<p>하지만, 위의 예제는 name column으로만 조회 할 때는 성능의 향상을 불러오기 어렵습니다. 보시다 싶이, <strong>먼저 team column으로 정렬 한 이후, 같은 team column을 가진 값에 대해서 name column으로 sorting을 수행</strong> 하고 있기 때문에, name column의 데이터를 조회 하기 위해서는 사실 상 아래와 같은 상황에서는 거의 <strong>full scan</strong>을 해야 하죠.</p>\n<p align=\"center\">\n    <img src=\"/post_image/apache-iceberg/table-optimization/04.png\" style=\"width:75%;max-width:768px;\"/>\n</p>\n<h4>Z-order</h4>\n<p>이를 위해 나온 구조는 <strong>Z-order</strong> 입니다. 복수 개의 Column으로 정렬 하였다 하더라도, <strong>Age로 조회 하든, Height으로 조회 하든 최소한의 성능</strong>을 보장하여 줍니다.</p>\n<p>Age 1-50에 대해서 찾고 싶다면, 아래의 A,C 파일만 조회 하는 형식으로 말이죠.</p>\n<p align=\"center\">\n    <img src=\"/post_image/apache-iceberg/table-optimization/05.png\" style=\"width:75%;max-width:768px;\"/>\n</p>\n<ul>\n<li>A: File with records containing Age 1–50 and Height 1–5</li>\n<li>B: File with records containing Age 51–100 and Height 1–5</li>\n<li>C: File with records containing Age 1–50 and Height 5–10</li>\n<li>D: File with records containing Age 51–100 and Height 5–10</li>\n</ul>\n<p>압축 방법은 다음과 같습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">CALL</span> catalog<span class=\"token punctuation\">.</span>system<span class=\"token punctuation\">.</span>rewrite_data_files<span class=\"token punctuation\">(</span>\n    <span class=\"token keyword\">table</span> <span class=\"token operator\">=</span><span class=\"token operator\">></span> <span class=\"token string\">'people'</span><span class=\"token punctuation\">,</span>\n    strategy <span class=\"token operator\">=</span><span class=\"token operator\">></span> <span class=\"token string\">'sort'</span><span class=\"token punctuation\">,</span>\n    sort_order <span class=\"token operator\">=</span><span class=\"token operator\">></span> <span class=\"token string\">'zorder(age,height)'</span>\n<span class=\"token punctuation\">)</span></code></pre></div>\n<p>Sort, Z-order와 같은 방식은 <strong>Read에 있어 좋은 성능</strong>을 보장하지만,\n결론적으로 단점이 있다면 <strong>압축 전에는 작은 파일로 산개되어 있는 파일들을 거의 모두 읽어 봐야 한다</strong>는 점입니다.</p>\n<p>또한, 위의 예시에서 나이가 30대인 사람만 관심이 있다면, 어쩔 수 없이 파일 앞뒤에 붙어 있는 20대, 40대의 다른 Row들도 Scan을 해야 된다는 점이 문제에요.</p>\n<h3>마치며</h3>\n<p>다음 시간에는 Partitioning, Merge-On-Read / Copy-On-Write, Metric Collection, Bloom Filter 등의 Table Optimization 기법을 알아 보도록 하겠습니다.</p>","id":"3c44d6b1-b341-5256-bb40-e6a58835b474","frontmatter":{"date":"2024-11-10","path":"/data-engineering/iceberg-table-optimization-1","title":"Iceberg Table의 성능 최적화 - 1. 압축","tags":["Data-Engineering"],"keyword":"Iceberg,Apache Iceberg,빅데이터,Big Data,Optimization,성능 최적화","summary":"File Merge를 통한 성능 최적화에 대해 알아보자.","img":"/post_image/thumbnail/iceberg-table-optimization-1.webp","series":"Iceberg Table Optimization"}}},"pageContext":{"postPath":"/data-engineering/iceberg-table-optimization-1","series":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"cbb6e851-d864-5552-86d7-08c81b4a54cc","excerpt":"Intro 안녕하세요, 박민재입니다. 저번 시간에는 Table Optimization을 위한 압축 기법에 대해 배웠습니다. 이번 시간에는 압축을 제외한 Table Optimization 기법을 알아 보도록 하겠습니다. Partitioning 역시, 기존의 방법을 꺼낼 때가 왔습니다. 바로 Partitioning입니다. 동일한 Column의 동일한 Value를 가진 친구들은 같은 File로 묶어 주는 방식이죠. 어? 왜 Directory…","frontmatter":{"date":"2024-11-24","tags":["Data-Engineering"],"path":"/data-engineering/iceberg-table-optimization-2","title":"Iceberg Table의 성능 최적화 - 2. Partitioning, MOR, Others","img":"/post_image/thumbnail/iceberg-table-optimization-1.webp","summary":"File Merge를 통한 성능 최적화에 대해 알아보자."}}},{"node":{"id":"3c44d6b1-b341-5256-bb40-e6a58835b474","excerpt":"Intro 안녕하세요, 박민재입니다. 오늘은 Apache Iceberg의 Table에 수행 되는 쿼리가 최적의 성능으로 작동 될 수 있도록, File Compaction을 통해 이를 수행하는 방법에 대해 이야기 하는 시간을 가져 보도록 하겠습니다. File Compaction 우리가 쿼리를 수행 시, Hive Metastore의 정보를 이용하더라도, 혹은 Iceberg의 Metadata…","frontmatter":{"date":"2024-11-10","tags":["Data-Engineering"],"path":"/data-engineering/iceberg-table-optimization-1","title":"Iceberg Table의 성능 최적화 - 1. 압축","img":"/post_image/thumbnail/iceberg-table-optimization-1.webp","summary":"File Merge를 통한 성능 최적화에 대해 알아보자."}}}]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"dec9e599-9444-556f-99ba-7a1dc27a4dbb","excerpt":"안녕하세요, 박민재입니다. 혹시 Data Discovery에 중요성을 느껴, DataHub를 사용하려고 하시는 분이 있나요? 아마 그렇다면 DataHub를 도입한 사례를 몇 개 읽어 보셨을꺼라 생각합니다. 대표적으로 국내 기업에서는 뱅크샐러드, 소카, 베이글코드 등에서 성공적으로 도입한 사례들을 회사 사이트에 올리는 경우를 확인 할 수 있었어요. SOCAR BankSalad BagelCode DataHub…","frontmatter":{"date":"2025-03-02","tags":["Data-Engineering"],"path":"/data-engineering/to-datahub-user","title":"DataHub 도입을 고려 하시는 분들에게","img":"/post_image/thumbnail/to-datahub-user.webp","summary":"DataHub를 도입 하려고 할 때 알아야 할 점"}}},{"node":{"id":"24718dd5-aa23-578b-aa81-0ca11fcc0f06","excerpt":"안녕하세요, 박민재입니다. 저번 시간에는 Spark Operator가 무엇인지 간단하게 알아 보았는데요, 이번 시간에는 실제로 Spark Operator Helm Chart를 설치하여, Spark Operator 관련 구동 준비를 한 후, Spark Operator 관련 Resource를 작성 하여 실제 Job을 제출 해 보는 시간을 가져 보도록 하겠습니다. Spark Operator Helm Chart Spark Operator…","frontmatter":{"date":"2025-02-02","tags":["Data-Engineering"],"path":"/data-engineering/spark-operator-2","title":"Spark Operator - 2. Practice","img":"/post_image/thumbnail/spark-operator.jpg","summary":"Spark Operator를 사용 해 보자."}}},{"node":{"id":"9ec3e659-5978-5311-b4d5-fc9d0902e008","excerpt":"안녕하세요, 박민재입니다. 아마 2년 전 즈음에 Spark on Kubernetes 관련 내용을 다뤘었는데요 (이 글 또한, 개정판을 작성 해 볼게요), 이번에는 Spark Job을 Kubernetes Cluster에 편리하게 제출할 수 있게 하는 Spark Operator에 대해 알아 보도록 하겠습니다. Spark on Kubernetes를 사용하는 이유? 그렇다고, 이 글에서 아예 설명 하지 않고 넘어가는 것은 아닌 것 같아, Spark…","frontmatter":{"date":"2025-01-19","tags":["Data-Engineering"],"path":"/data-engineering/spark-operator-1","title":"Spark Operator - 1. Spark Operator란?","img":"/post_image/thumbnail/spark-operator.jpg","summary":"Kubernetes Cluster로의 Spark Job 제출을 도와주는 Spark Operator가 무엇 인지 알아보자."}}},{"node":{"id":"2001438a-fe37-5b3c-8954-bce7d5e18a7a","excerpt":"안녕하세요? 박민재입니다. 오늘은 Iceberg Table을 관리하는 방법 중 하나인, Branching & Tagging 그리고 Rollback Action에 대해서 알아 보도록 하겠습니다. Isolation of Changes with Branches Iceberg에서는 git과 같은 방식으로 Branch를 만들어, 데이터 변경 사항을 관리 할 수 있습니다. 우리의 사례로 빗대어 보면 H/W 이슈, 혹은 Application…","frontmatter":{"date":"2025-01-03","tags":["Data-Engineering"],"path":"/data-engineering/iceberg-table-management-2","title":"Iceberg Table Management - 2. Branching, Tagging & Rollback","img":"/post_image/thumbnail/iceberg-table-management.png","summary":"Branching, Tagging & Rollback을 통해 Iceberg Table을 관리 해 보자"}}}]}}}}},"staticQueryHashes":["3819017183","63159454"],"slicesMap":{}}