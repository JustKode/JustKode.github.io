{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/data-engineering/spark-structured-streaming/","result":{"data":{"markdownRemark":{"html":"<p>안녕하세요? 오늘은 <strong>Spark Structured Streaming</strong>에 대해서 알아 보도록 하겠습니다.</p>\n<h3>Spark Structured Streaming이란?</h3>\n<p><strong>Spark Structured Streaming</strong>은, <strong>Spark SQL (Dataset/DataFrame)</strong> 엔진 기반의, 확장 가능하고, 내결함성이 있는 <strong>Stream Processing Engine</strong> 입니다. 이는 <strong>Batch</strong> 작업에서 구조화된 데이터를 처리 하는 것 처럼, <strong>Streaming</strong> 작업에서도 구조화된 데이터 형태로 데이터를 받아 처리를 가능 하게 해 줍니다. <strong>(ex: DataFrame)</strong></p>\n<p>또한, <strong>Streaming Aggregation (스트리밍 집계), event-time windows, stream과 batch join</strong> 등이 최적화된 <strong>SQL Engine</strong>에서 작동 시킬 수 있으며, <strong>Write-Ahead</strong> 체크 포인트를 통해 <strong>end-to-end exactly-once fault-tolerance</strong>를 보장 할 수 있습니다.</p>\n<p>기존 <strong>Spark Streaming</strong>과 같이, <strong>Micro-batch processing</strong>을 수행 하며, 또 다른 기능으로는, <strong>Micro-batch</strong>를 사용하지 않는, <strong>Continuous Processing</strong> 모드를 이용 하여, <strong>end-to-end</strong> latency를 1 millisecond 까지 줄일 수 있습니다. 단, 이는 실험적 기능으로 존재 하기 때문에, 큰 데이터를 처리하는 데에는 성능이 좀 보장 되기가 힘들 수도 있어 보이네요. (자원 사용량의 증가.)</p>\n<p>Scala 예제로 들어가 보죠!</p>\n<ul>\n<li><code class=\"language-text\">SparkSession</code>을 생성 해 줍니다.</li>\n<li><code class=\"language-text\">spark.implicits._</code> 를 import 해 줍니다.</li>\n<li><code class=\"language-text\">spark.readStream</code>을 통해서, Streaming Data Source를 읽습니다. <strong>socket</strong> 형식으로 읽으며, Host 이름과 포트번호를 지정 합니다. 이는 <strong>Kafka</strong>, <strong>File</strong> 등, 다양한 Data Source를 지원 합니다. return 값은 <strong>DataFrame</strong> 입니다.</li>\n<li><code class=\"language-text\">words</code> 를 통해, <code class=\"language-text\">DataFrame</code>을 <code class=\"language-text\">Dataset[String]</code>으로 변환 시키고, <code class=\"language-text\">wordCounts</code>를 통해 <code class=\"language-text\">groupBy</code>를 수행 합니다. 해당 부분이 <strong>Aggregation(집계)</strong> 을 수행 하는 역할 입니다. 추후에 <strong>사용 용도에 맞추어</strong> 해당 부분을 수정 가능 합니다.</li>\n<li><code class=\"language-text\">writeStream</code>을 통해 출력을 할 수 있습니다. 현재 <code class=\"language-text\">console</code> 모드를 이용 하여, 결과물을 출력 합니다. 혹은 <strong>External Sink</strong>를 이용하여, <strong>HDFS, S3, 로컬 파일 시스템 등</strong> 다양한 곳에 데이터를 저장 할 수 있습니다.</li>\n<li><strong>Streaming Job</strong>이 종료 되지 않게 하기 위해서, <code class=\"language-text\">query.awaitTermination()</code>을 통해, 종료 트리거가 될 때 까지 대기 합니다.</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token keyword\">import</span> <span class=\"token namespace\">org<span class=\"token punctuation\">.</span>apache<span class=\"token punctuation\">.</span>spark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">.</span>functions<span class=\"token punctuation\">.</span></span>_\n<span class=\"token keyword\">import</span> <span class=\"token namespace\">org<span class=\"token punctuation\">.</span>apache<span class=\"token punctuation\">.</span>spark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">.</span></span>SparkSession\n\n<span class=\"token keyword\">val</span> spark <span class=\"token operator\">=</span> SparkSession\n  <span class=\"token punctuation\">.</span>builder\n  <span class=\"token punctuation\">.</span>appName<span class=\"token punctuation\">(</span><span class=\"token string\">\"StructuredNetworkWordCount\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>getOrCreate<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n  \n<span class=\"token keyword\">import</span> <span class=\"token namespace\">spark<span class=\"token punctuation\">.</span>implicits<span class=\"token punctuation\">.</span></span>_\n\n<span class=\"token keyword\">val</span> lines <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>readStream\n  <span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span><span class=\"token string\">\"socket\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>option<span class=\"token punctuation\">(</span><span class=\"token string\">\"host\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"localhost\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>option<span class=\"token punctuation\">(</span><span class=\"token string\">\"port\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">9999</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\">// Split the lines into words</span>\n<span class=\"token keyword\">val</span> words <span class=\"token operator\">=</span> lines<span class=\"token punctuation\">.</span>as<span class=\"token punctuation\">[</span><span class=\"token builtin\">String</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>flatMap<span class=\"token punctuation\">(</span>_<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token string\">\" \"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\">// Generate running word count</span>\n<span class=\"token keyword\">val</span> wordCounts <span class=\"token operator\">=</span> words<span class=\"token punctuation\">.</span>groupBy<span class=\"token punctuation\">(</span><span class=\"token string\">\"value\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">val</span> query <span class=\"token operator\">=</span> wordCounts<span class=\"token punctuation\">.</span>writeStream\n  <span class=\"token punctuation\">.</span>outputMode<span class=\"token punctuation\">(</span><span class=\"token string\">\"complete\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span><span class=\"token string\">\"console\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\nquery<span class=\"token punctuation\">.</span>awaitTermination<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>일단 터미널에 <code class=\"language-text\">nc -lk 9999</code> 를 작성하여, Socket 연결을 시도 해 보죠! (NetCat 이라는 친구 입니다.) 그 다음, <strong>Spark Application</strong>을 실행 해 보면 다음과 같습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">$ <span class=\"token function\">nc</span> <span class=\"token parameter variable\">-lk</span> <span class=\"token number\">9999</span>\napache spark\napache hadoop</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">-------------------------------------------\nBatch: <span class=\"token number\">0</span>\n-------------------------------------------\n+------+-----+\n<span class=\"token operator\">|</span> value<span class=\"token operator\">|</span>count<span class=\"token operator\">|</span>\n+------+-----+\n<span class=\"token operator\">|</span>apache<span class=\"token operator\">|</span>    <span class=\"token number\">1</span><span class=\"token operator\">|</span>\n<span class=\"token operator\">|</span> spark<span class=\"token operator\">|</span>    <span class=\"token number\">1</span><span class=\"token operator\">|</span>\n+------+-----+\n\n-------------------------------------------\nBatch: <span class=\"token number\">1</span>\n-------------------------------------------\n+------+-----+\n<span class=\"token operator\">|</span> value<span class=\"token operator\">|</span>count<span class=\"token operator\">|</span>\n+------+-----+\n<span class=\"token operator\">|</span>apache<span class=\"token operator\">|</span>    <span class=\"token number\">2</span><span class=\"token operator\">|</span>\n<span class=\"token operator\">|</span> spark<span class=\"token operator\">|</span>    <span class=\"token number\">1</span><span class=\"token operator\">|</span>\n<span class=\"token operator\">|</span>hadoop<span class=\"token operator\">|</span>    <span class=\"token number\">1</span><span class=\"token operator\">|</span>\n+------+-----+</code></pre></div>\n<p>이런 식으로, <strong>word</strong>를 <strong>split</strong>하여 <strong>group by</strong> 한 결과가 실시간으로 반영 되는 것을 볼 수가 있습니다.</p>\n<p>원래라면, <strong>Trigger</strong>라는 요소를 사용하여, 들어 온 데이터에 대해 <strong>몇 초마다 쿼리를 수행</strong> 하고 저장 할 지를 정할 수도 있습니다. 다음과 같이, output option에 삽입 해 주면 됩니다. 아래 예제는 <strong>10초 마다 Trigger</strong>를 보내는 예제입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\n<span class=\"token keyword\">import</span> <span class=\"token namespace\">org<span class=\"token punctuation\">.</span>apache<span class=\"token punctuation\">.</span>spark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">.</span>streaming<span class=\"token punctuation\">.</span></span>Trigger\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\n\n<span class=\"token keyword\">val</span> query <span class=\"token operator\">=</span> wordCounts<span class=\"token punctuation\">.</span>writeStream\n  <span class=\"token punctuation\">.</span>trigger<span class=\"token punctuation\">(</span>Trigger<span class=\"token punctuation\">.</span>ProcessingTime<span class=\"token punctuation\">(</span><span class=\"token string\">\"10 seconds\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>outputMode<span class=\"token punctuation\">(</span><span class=\"token string\">\"complete\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span><span class=\"token string\">\"console\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></code></pre></div>\n<h3>기본 컨셉</h3>\n<p><strong>Spark Structured Streaming</strong>의 주 컨셉은 <strong>Input Data Stream</strong>이  테이블의 행으로 계속 <strong>추가 되는</strong> 것 입니다.</p>\n<p align=\"center\">\n    <img src=\"/post_image/spark-structured-streaming/01-01.png\" min-width=\"400px\" />\n    <div align=\"center\" color=\"#aaaaaa\">Data Stream이 들어오면, 데이터가 추가 되는 형태</div>\n</p>\n<p>그렇게 계속 들어오게 된 <strong>Input</strong>들은, 매 <strong>trigger</strong> 시간마다, 작성한 쿼리를 바탕으로 <strong>Result Table</strong>을 생성 합니다. 위의 예제로 따지면, 작성한 쿼리는 <code class=\"language-text\">words</code> 에 해당 되겠네요! 현재 우리는 <code class=\"language-text\">query</code> 결과를 디버깅을 위해서 format으로 지정 했지만, file로 지정이 된다면, result table이 <strong>업데이트 될 때</strong> 마다, <strong>external sink에 저장</strong> 되는 형식으로 진행이 되겠군요.</p>\n<p align=\"center\">\n    <img src=\"/post_image/spark-structured-streaming/01-02.png\" min-width=\"400px\" />\n    <div align=\"center\" color=\"#aaaaaa\">Data Stream이 들어오면, 데이터가 추가 되는 형태</div>\n</p>\n<p><code class=\"language-text\">outputMode</code> 의 설정을 통해서, <strong>external sink</strong>에 <strong>어떻게 저장</strong> 할 것인지 정해 줄 수 있습니다.</p>\n<ul>\n<li>Append Mode: last trigger 이후, 새롭게 추가된 row만 <strong>output sink에 기록</strong> 됩니다. 단, <strong>추가된 row가 바뀔 일이 없어야</strong> 사용 할 수 있습니다. <strong>Streaming으로 들어온 데이터를 계속 적재</strong> 해 주는 식이라면 편하게 사용 할 수 있겠군요! 아마, <strong>select, where, map, flatMap, filter, join</strong> 같은, 기존에 row에 영향을 주지 않는 쿼리들을 수행 할 수 있을 것으로 보이네요.</li>\n<li>Complete Mode: Trigger된 모든 <strong>result table</strong>을 <strong>모두 저장</strong>하는 모드입니다. <strong>주기적으로 복잡한 aggregation</strong>이 필요 한 경우에 사용 하면 될 것 같습니다.</li>\n<li>Update Mode: last trigger 이후, <strong>업데이트 된 Result Table의 행만</strong> 추가 됩니다.</li>\n</ul>\n<p>다음 시간에는, <strong>로컬(Docker)에서 Kafka를 구동</strong> 하여, 실제 데이터를 받아보는 실습을 진행 하고, <strong>Trigger, Checkpoint,</strong> 그리고 <strong>StreamingQueryListener</strong>에 대해서 알아 보도록 하겠습니다.</p>","id":"900d112f-9ad2-52a1-99f9-d2106ddec93b","frontmatter":{"date":"2023-05-13","path":"/data-engineering/spark-structured-streaming","title":"Spark Structured Streaming이란?","tags":["Data-Engineering"],"keyword":"Spark, Spark Structured Streaming","summary":"Spark로 Streaming Job을 수행 해 보자.","img":"/post_image/thumbnail/spark-structured-streaming.png","series":"Spark Structured Streaming"}}},"pageContext":{"postPath":"/data-engineering/spark-structured-streaming","series":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"3e003c7c-3b0a-5b57-adcc-4466a148d4fc","excerpt":"머릿말 안녕하세요? 새해부터 찾아온 JustKode, 박민재입니다. 오늘은 Spark Structured Streaming에 대한 Unit Test를 수행 하는 법에 대해서 공유 드려 보려고 합니다. What is Spark Structured Streaming? Spark Structured Streaming은 Spark SQL API (Dataframe, Dataset)를 이용하여, Streaming…","frontmatter":{"date":"2024-01-07","tags":["Data-Engineering"],"path":"/data-engineering/spark-structured-streaming-unit-test","title":"Unit Test of Spark Structured Streaming","img":"/post_image/thumbnail/spark-structured-streaming-unit-test.jpeg","summary":"더 정밀한 Streaming Unit Test를 위해"}}},{"node":{"id":"900d112f-9ad2-52a1-99f9-d2106ddec93b","excerpt":"안녕하세요? 오늘은 Spark Structured Streaming에 대해서 알아 보도록 하겠습니다. Spark Structured Streaming이란? Spark Structured Streaming은, Spark SQL (Dataset/DataFrame) 엔진 기반의, 확장 가능하고, 내결함성이 있는 Stream Processing Engine 입니다. 이는 Batch 작업에서 구조화된 데이터를 처리 하는 것 처럼, Streaming…","frontmatter":{"date":"2023-05-13","tags":["Data-Engineering"],"path":"/data-engineering/spark-structured-streaming","title":"Spark Structured Streaming이란?","img":"/post_image/thumbnail/spark-structured-streaming.png","summary":"Spark로 Streaming Job을 수행 해 보자."}}}]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"3c44d6b1-b341-5256-bb40-e6a58835b474","excerpt":"Intro 안녕하세요, 박민재입니다. 오늘은 Apache Iceberg의 Table에 수행 되는 쿼리가 최적의 성능으로 작동 될 수 있도록, File Compaction을 통해 이를 수행하는 방법에 대해 이야기 하는 시간을 가져 보도록 하겠습니다. File Compaction 우리가 쿼리를 수행 시, Hive Metastore의 정보를 이용하더라도, 혹은 Iceberg의 Metadata…","frontmatter":{"date":"2024-11-10","tags":["Data-Engineering"],"path":"/data-engineering/iceberg-table-optimization-1","title":"Iceberg Table의 성능 최적화 - 1. 압축","img":"/post_image/thumbnail/iceberg-table-optimization-1.webp","summary":"File Merge를 통한 성능 최적화에 대해 알아보자."}}},{"node":{"id":"6eaa7aeb-a4fe-5cd9-bbe5-309dde97514b","excerpt":"안녕하세요, 박민재입니다. 오늘은 Airflow DB를 관리하는 방법에 대해서 이야기 나눠 보도록 하겠습니다. Airflow Backend Database Airflow에서 Backend Database는 어떤 역할을 할까요? Airflow에서 DAG을 실행 하기 위해서, Airflow는 다음과 같은 정보들을 Backend Database에 저장하여 정합성을 유지 합니다. DagRun: 특정 Interval에 실행 된 DagRun…","frontmatter":{"date":"2024-10-27","tags":["Data-Engineering"],"path":"/data-engineering/airflow-db-management","title":"Airflow Backend Database Management (airflow db clean)","img":"/post_image/thumbnail/airflow-db-management.webp","summary":"Airflow의 Backend Database를 관리 하는 법"}}},{"node":{"id":"b595b168-8160-5abf-9641-39746d2f9e82","excerpt":"안녕하세요, 박민재입니다. 오늘은 Spark Application내의 각 Executor 내의 Task에 제한적으로 변수를 공유하는 두 가지 방법에 대해서 알아 보도록 하겠습니다. 시작하기에 앞서 단편적으로 생각 해 보면, Spark Application에서 연산 과정의 변수를 공유 한다는 것은 어려운 일인가? 라는 질문을 던져 볼 수 있습니다. 우리는 Task…","frontmatter":{"date":"2024-10-11","tags":["Data-Engineering"],"path":"/data-engineering/spark-sharing-variables","title":"Sharing Variables in Spark - Broadcast, Accumulator","img":"/post_image/thumbnail/spark-sharing-variables.png","summary":"Spark Application에서 변수를 공유 하는 방법"}}},{"node":{"id":"5effd6a1-bfd8-5a86-b718-5eb1e534601e","excerpt":"Intro 안녕하세요, 박민재입니다. 오늘은 Spark Memory에 관해 Deep Dive를 해 보도록 하겠습니다. Spark는 In-Memory를 이용하여, 빠른 연산을 할 수 있도록 보장합니다. 하지만, In-Memory 연산은 빠른 대신, 비싼 관계로 적은 리소스 만을 활용할 수 있습니다. 그렇기 때문에 우리는 효율적으로 Memory를 관리 하여, Spark Application이 빠르고, 안정적으로 Task…","frontmatter":{"date":"2024-04-12","tags":["Data-Engineering"],"path":"/data-engineering/spark-memory-deep-dive","title":"Deep Dive of Spark Memory","img":"/post_image/thumbnail/spark-memory-deep-dive.jpeg","summary":"Spark Memory의 깊은 이해를 위해 Deep Dive를 해 보자."}}}]}}}}},"staticQueryHashes":["3819017183","63159454"],"slicesMap":{}}