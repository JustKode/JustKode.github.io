{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/data-engineering/spark-structured-streaming/","result":{"data":{"markdownRemark":{"html":"<p>안녕하세요? 오늘은 <strong>Spark Structured Streaming</strong>에 대해서 알아 보도록 하겠습니다.</p>\n<h3>Spark Structured Streaming이란?</h3>\n<p><strong>Spark Structured Streaming</strong>은, <strong>Spark SQL (Dataset/DataFrame)</strong> 엔진 기반의, 확장 가능하고, 내결함성이 있는 <strong>Stream Processing Engine</strong> 입니다. 이는 <strong>Batch</strong> 작업에서 구조화된 데이터를 처리 하는 것 처럼, <strong>Streaming</strong> 작업에서도 구조화된 데이터 형태로 데이터를 받아 처리를 가능 하게 해 줍니다. <strong>(ex: DataFrame)</strong></p>\n<p>또한, <strong>Streaming Aggregation (스트리밍 집계), event-time windows, stream과 batch join</strong> 등이 최적화된 <strong>SQL Engine</strong>에서 작동 시킬 수 있으며, <strong>Write-Ahead</strong> 체크 포인트를 통해 <strong>end-to-end exactly-once fault-tolerance</strong>를 보장 할 수 있습니다.</p>\n<p>기존 <strong>Spark Streaming</strong>과 같이, <strong>Micro-batch processing</strong>을 수행 하며, 또 다른 기능으로는, <strong>Micro-batch</strong>를 사용하지 않는, <strong>Continuous Processing</strong> 모드를 이용 하여, <strong>end-to-end</strong> latency를 1 millisecond 까지 줄일 수 있습니다. 단, 이는 실험적 기능으로 존재 하기 때문에, 큰 데이터를 처리하는 데에는 성능이 좀 보장 되기가 힘들 수도 있어 보이네요. (자원 사용량의 증가.)</p>\n<p>Scala 예제로 들어가 보죠!</p>\n<ul>\n<li><code class=\"language-text\">SparkSession</code>을 생성 해 줍니다.</li>\n<li><code class=\"language-text\">spark.implicits._</code> 를 import 해 줍니다.</li>\n<li><code class=\"language-text\">spark.readStream</code>을 통해서, Streaming Data Source를 읽습니다. <strong>socket</strong> 형식으로 읽으며, Host 이름과 포트번호를 지정 합니다. 이는 <strong>Kafka</strong>, <strong>File</strong> 등, 다양한 Data Source를 지원 합니다. return 값은 <strong>DataFrame</strong> 입니다.</li>\n<li><code class=\"language-text\">words</code> 를 통해, <code class=\"language-text\">DataFrame</code>을 <code class=\"language-text\">Dataset[String]</code>으로 변환 시키고, <code class=\"language-text\">wordCounts</code>를 통해 <code class=\"language-text\">groupBy</code>를 수행 합니다. 해당 부분이 <strong>Aggregation(집계)</strong> 을 수행 하는 역할 입니다. 추후에 <strong>사용 용도에 맞추어</strong> 해당 부분을 수정 가능 합니다.</li>\n<li><code class=\"language-text\">writeStream</code>을 통해 출력을 할 수 있습니다. 현재 <code class=\"language-text\">console</code> 모드를 이용 하여, 결과물을 출력 합니다. 혹은 <strong>External Sink</strong>를 이용하여, <strong>HDFS, S3, 로컬 파일 시스템 등</strong> 다양한 곳에 데이터를 저장 할 수 있습니다.</li>\n<li><strong>Streaming Job</strong>이 종료 되지 않게 하기 위해서, <code class=\"language-text\">query.awaitTermination()</code>을 통해, 종료 트리거가 될 때 까지 대기 합니다.</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token keyword\">import</span> <span class=\"token namespace\">org<span class=\"token punctuation\">.</span>apache<span class=\"token punctuation\">.</span>spark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">.</span>functions<span class=\"token punctuation\">.</span></span>_\n<span class=\"token keyword\">import</span> <span class=\"token namespace\">org<span class=\"token punctuation\">.</span>apache<span class=\"token punctuation\">.</span>spark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">.</span></span>SparkSession\n\n<span class=\"token keyword\">val</span> spark <span class=\"token operator\">=</span> SparkSession\n  <span class=\"token punctuation\">.</span>builder\n  <span class=\"token punctuation\">.</span>appName<span class=\"token punctuation\">(</span><span class=\"token string\">\"StructuredNetworkWordCount\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>getOrCreate<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n  \n<span class=\"token keyword\">import</span> <span class=\"token namespace\">spark<span class=\"token punctuation\">.</span>implicits<span class=\"token punctuation\">.</span></span>_\n\n<span class=\"token keyword\">val</span> lines <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>readStream\n  <span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span><span class=\"token string\">\"socket\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>option<span class=\"token punctuation\">(</span><span class=\"token string\">\"host\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"localhost\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>option<span class=\"token punctuation\">(</span><span class=\"token string\">\"port\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">9999</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\">// Split the lines into words</span>\n<span class=\"token keyword\">val</span> words <span class=\"token operator\">=</span> lines<span class=\"token punctuation\">.</span>as<span class=\"token punctuation\">[</span><span class=\"token builtin\">String</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>flatMap<span class=\"token punctuation\">(</span>_<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token string\">\" \"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\">// Generate running word count</span>\n<span class=\"token keyword\">val</span> wordCounts <span class=\"token operator\">=</span> words<span class=\"token punctuation\">.</span>groupBy<span class=\"token punctuation\">(</span><span class=\"token string\">\"value\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">val</span> query <span class=\"token operator\">=</span> wordCounts<span class=\"token punctuation\">.</span>writeStream\n  <span class=\"token punctuation\">.</span>outputMode<span class=\"token punctuation\">(</span><span class=\"token string\">\"complete\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span><span class=\"token string\">\"console\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\nquery<span class=\"token punctuation\">.</span>awaitTermination<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>일단 터미널에 <code class=\"language-text\">nc -lk 9999</code> 를 작성하여, Socket 연결을 시도 해 보죠! (NetCat 이라는 친구 입니다.) 그 다음, <strong>Spark Application</strong>을 실행 해 보면 다음과 같습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">$ <span class=\"token function\">nc</span> <span class=\"token parameter variable\">-lk</span> <span class=\"token number\">9999</span>\napache spark\napache hadoop</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">-------------------------------------------\nBatch: <span class=\"token number\">0</span>\n-------------------------------------------\n+------+-----+\n<span class=\"token operator\">|</span> value<span class=\"token operator\">|</span>count<span class=\"token operator\">|</span>\n+------+-----+\n<span class=\"token operator\">|</span>apache<span class=\"token operator\">|</span>    <span class=\"token number\">1</span><span class=\"token operator\">|</span>\n<span class=\"token operator\">|</span> spark<span class=\"token operator\">|</span>    <span class=\"token number\">1</span><span class=\"token operator\">|</span>\n+------+-----+\n\n-------------------------------------------\nBatch: <span class=\"token number\">1</span>\n-------------------------------------------\n+------+-----+\n<span class=\"token operator\">|</span> value<span class=\"token operator\">|</span>count<span class=\"token operator\">|</span>\n+------+-----+\n<span class=\"token operator\">|</span>apache<span class=\"token operator\">|</span>    <span class=\"token number\">2</span><span class=\"token operator\">|</span>\n<span class=\"token operator\">|</span> spark<span class=\"token operator\">|</span>    <span class=\"token number\">1</span><span class=\"token operator\">|</span>\n<span class=\"token operator\">|</span>hadoop<span class=\"token operator\">|</span>    <span class=\"token number\">1</span><span class=\"token operator\">|</span>\n+------+-----+</code></pre></div>\n<p>이런 식으로, <strong>word</strong>를 <strong>split</strong>하여 <strong>group by</strong> 한 결과가 실시간으로 반영 되는 것을 볼 수가 있습니다.</p>\n<p>원래라면, <strong>Trigger</strong>라는 요소를 사용하여, 들어 온 데이터에 대해 <strong>몇 초마다 쿼리를 수행</strong> 하고 저장 할 지를 정할 수도 있습니다. 다음과 같이, output option에 삽입 해 주면 됩니다. 아래 예제는 <strong>10초 마다 Trigger</strong>를 보내는 예제입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\n<span class=\"token keyword\">import</span> <span class=\"token namespace\">org<span class=\"token punctuation\">.</span>apache<span class=\"token punctuation\">.</span>spark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">.</span>streaming<span class=\"token punctuation\">.</span></span>Trigger\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\n\n<span class=\"token keyword\">val</span> query <span class=\"token operator\">=</span> wordCounts<span class=\"token punctuation\">.</span>writeStream\n  <span class=\"token punctuation\">.</span>trigger<span class=\"token punctuation\">(</span>Trigger<span class=\"token punctuation\">.</span>ProcessingTime<span class=\"token punctuation\">(</span><span class=\"token string\">\"10 seconds\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>outputMode<span class=\"token punctuation\">(</span><span class=\"token string\">\"complete\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span><span class=\"token string\">\"console\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></code></pre></div>\n<h3>기본 컨셉</h3>\n<p><strong>Spark Structured Streaming</strong>의 주 컨셉은 <strong>Input Data Stream</strong>이  테이블의 행으로 계속 <strong>추가 되는</strong> 것 입니다.</p>\n<p align=\"center\">\n    <img src=\"/post_image/spark-structured-streaming/01-01.png\" min-width=\"400px\" />\n    <div align=\"center\" color=\"#aaaaaa\">Data Stream이 들어오면, 데이터가 추가 되는 형태</div>\n</p>\n<p>그렇게 계속 들어오게 된 <strong>Input</strong>들은, 매 <strong>trigger</strong> 시간마다, 작성한 쿼리를 바탕으로 <strong>Result Table</strong>을 생성 합니다. 위의 예제로 따지면, 작성한 쿼리는 <code class=\"language-text\">words</code> 에 해당 되겠네요! 현재 우리는 <code class=\"language-text\">query</code> 결과를 디버깅을 위해서 format으로 지정 했지만, file로 지정이 된다면, result table이 <strong>업데이트 될 때</strong> 마다, <strong>external sink에 저장</strong> 되는 형식으로 진행이 되겠군요.</p>\n<p align=\"center\">\n    <img src=\"/post_image/spark-structured-streaming/01-02.png\" min-width=\"400px\" />\n    <div align=\"center\" color=\"#aaaaaa\">Data Stream이 들어오면, 데이터가 추가 되는 형태</div>\n</p>\n<p><code class=\"language-text\">outputMode</code> 의 설정을 통해서, <strong>external sink</strong>에 <strong>어떻게 저장</strong> 할 것인지 정해 줄 수 있습니다.</p>\n<ul>\n<li>Append Mode: last trigger 이후, 새롭게 추가된 row만 <strong>output sink에 기록</strong> 됩니다. 단, <strong>추가된 row가 바뀔 일이 없어야</strong> 사용 할 수 있습니다. <strong>Streaming으로 들어온 데이터를 계속 적재</strong> 해 주는 식이라면 편하게 사용 할 수 있겠군요! 아마, <strong>select, where, map, flatMap, filter, join</strong> 같은, 기존에 row에 영향을 주지 않는 쿼리들을 수행 할 수 있을 것으로 보이네요.</li>\n<li>Complete Mode: Trigger된 모든 <strong>result table</strong>을 <strong>모두 저장</strong>하는 모드입니다. <strong>주기적으로 복잡한 aggregation</strong>이 필요 한 경우에 사용 하면 될 것 같습니다.</li>\n<li>Update Mode: last trigger 이후, <strong>업데이트 된 Result Table의 행만</strong> 추가 됩니다.</li>\n</ul>\n<p>다음 시간에는, <strong>로컬(Docker)에서 Kafka를 구동</strong> 하여, 실제 데이터를 받아보는 실습을 진행 하고, <strong>Trigger, Checkpoint,</strong> 그리고 <strong>StreamingQueryListener</strong>에 대해서 알아 보도록 하겠습니다.</p>","id":"c7182a1e-4786-5112-9048-54cca7467f6d","frontmatter":{"date":"2023-05-13","path":"/data-engineering/spark-structured-streaming","title":"Spark Structured Streaming이란?","tags":["Data-Engineering"],"keyword":"Spark, Spark Structured Streaming","summary":"Spark로 Streaming Job을 수행 해 보자.","img":"/post_image/thumbnail/spark-structured-streaming.png","series":"Spark Structured Streaming"}}},"pageContext":{"postPath":"/data-engineering/spark-structured-streaming","series":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"c7182a1e-4786-5112-9048-54cca7467f6d","excerpt":"안녕하세요? 오늘은 Spark Structured Streaming에 대해서 알아 보도록 하겠습니다. Spark Structured Streaming이란? Spark Structured Streaming은, Spark SQL (Dataset/DataFrame) 엔진 기반의, 확장 가능하고, 내결함성이 있는 Stream Processing Engine 입니다. 이는 Batch 작업에서 구조화된 데이터를 처리 하는 것 처럼, Streaming…","frontmatter":{"date":"2023-05-13","tags":["Data-Engineering"],"path":"/data-engineering/spark-structured-streaming","title":"Spark Structured Streaming이란?","img":"/post_image/thumbnail/spark-structured-streaming.png","summary":"Spark로 Streaming Job을 수행 해 보자."}}}]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"c7182a1e-4786-5112-9048-54cca7467f6d","excerpt":"안녕하세요? 오늘은 Spark Structured Streaming에 대해서 알아 보도록 하겠습니다. Spark Structured Streaming이란? Spark Structured Streaming은, Spark SQL (Dataset/DataFrame) 엔진 기반의, 확장 가능하고, 내결함성이 있는 Stream Processing Engine 입니다. 이는 Batch 작업에서 구조화된 데이터를 처리 하는 것 처럼, Streaming…","frontmatter":{"date":"2023-05-13","tags":["Data-Engineering"],"path":"/data-engineering/spark-structured-streaming","title":"Spark Structured Streaming이란?","img":"/post_image/thumbnail/spark-structured-streaming.png","summary":"Spark로 Streaming Job을 수행 해 보자."}}},{"node":{"id":"fb3c924f-4a7c-50c2-8404-09a28f1e98ec","excerpt":"오늘은 Spark의 성능 튜닝에 대해서 이야기 해 보겠습니다. Spark는 요약해서 말하면, **in-memory(RAM 위에서)**에서 작동 하는 분산 컴퓨팅을 쉽게 지원해 주는 프레임워크 입니다. in-memory 연산은 빠르지만, 불안정 합니다. 메모리 관리, CPU Core 수의 관리를 통해 Out of memory가 발생 하지 않는 선에서, Job…","frontmatter":{"date":"2023-04-07","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-1","title":"Spark 성능 튜닝 - 1. Partition, Shuffle","img":"/post_image/thumbnail/spark-performance-tuning-1.png","summary":"Spark에 성능 튜닝을 시도 해 보자."}}},{"node":{"id":"7611f6af-dc48-595b-8866-23beee4d5cda","excerpt":"저번 시간에는 Spark On Kubernetes에 대한 이론을 배웠습니다. 오늘은 Spark On Kubernetes에 대한 실습을 진행 하도록 하겠습니다. 사전 준비 Docker Minikube (Kubernetes 1.20 버전 이상) kubectl Spark 3.0 버전 이상 최신 버전일 수록 좋습니다. 얼마 전에 구형 Docker가 깔려 있는 맥북에서 진행을 해 봤는데 Pod이 생성이 안되더군요.. Pyspark Image Build…","frontmatter":{"date":"2023-03-30","tags":["Data-Engineering","Cloud-Computing"],"path":"/data-engineering/spark-on-k8s-2","title":"Spark on Kubernetes - Practice","img":"/post_image/thumbnail/spark-on-k8s-2.png","summary":"Spark를 Kubernetes Cluster에서 동작 시켜 보자."}}},{"node":{"id":"3a543178-c1c9-5cfb-9c9e-b89aed5e8ead","excerpt":"안녕하세요? 오늘은 Kubernetes 환경에 JupyterHub를 설치 하는 방법에 대해서 알아 보도록 하겠습니다. Kubernetes Kubernetes는 컨테이너를 쉽고 빠르게 배포/확장하고 관리를 자동화해주는 Container Orchestration Tool 입니다. Kubernetes 시스템을 통해, 다음을 제공 받을 수 있습니다. 서비스 디스커버리와 로드 밸런싱: DNS 이름, 혹은 자체 IP…","frontmatter":{"date":"2023-03-25","tags":["Data-Engineering","Cloud-Computing"],"path":"/data-engineering/jupyterhub-on-k8s","title":"Jupyterhub on Kubernetes","img":"/post_image/thumbnail/jupyterhub-on-k8s.jpg","summary":"Jupyterhub를 Kubernetes Cluster에서 실행 해 보자."}}}]}}}}},"staticQueryHashes":["2876327880","63159454"],"slicesMap":{}}