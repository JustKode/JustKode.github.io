{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/data-engineering/spark-structured-streaming/","result":{"data":{"markdownRemark":{"html":"<p>안녕하세요? 오늘은 <strong>Spark Structured Streaming</strong>에 대해서 알아 보도록 하겠습니다.</p>\n<h3>Spark Structured Streaming이란?</h3>\n<p><strong>Spark Structured Streaming</strong>은, <strong>Spark SQL (Dataset/DataFrame)</strong> 엔진 기반의, 확장 가능하고, 내결함성이 있는 <strong>Stream Processing Engine</strong> 입니다. 이는 <strong>Batch</strong> 작업에서 구조화된 데이터를 처리 하는 것 처럼, <strong>Streaming</strong> 작업에서도 구조화된 데이터 형태로 데이터를 받아 처리를 가능 하게 해 줍니다. <strong>(ex: DataFrame)</strong></p>\n<p>또한, <strong>Streaming Aggregation (스트리밍 집계), event-time windows, stream과 batch join</strong> 등이 최적화된 <strong>SQL Engine</strong>에서 작동 시킬 수 있으며, <strong>Write-Ahead</strong> 체크 포인트를 통해 <strong>end-to-end exactly-once fault-tolerance</strong>를 보장 할 수 있습니다.</p>\n<p>기존 <strong>Spark Streaming</strong>과 같이, <strong>Micro-batch processing</strong>을 수행 하며, 또 다른 기능으로는, <strong>Micro-batch</strong>를 사용하지 않는, <strong>Continuous Processing</strong> 모드를 이용 하여, <strong>end-to-end</strong> latency를 1 millisecond 까지 줄일 수 있습니다. 단, 이는 실험적 기능으로 존재 하기 때문에, 큰 데이터를 처리하는 데에는 성능이 좀 보장 되기가 힘들 수도 있어 보이네요. (자원 사용량의 증가.)</p>\n<p>Scala 예제로 들어가 보죠!</p>\n<ul>\n<li><code class=\"language-text\">SparkSession</code>을 생성 해 줍니다.</li>\n<li><code class=\"language-text\">spark.implicits._</code> 를 import 해 줍니다.</li>\n<li><code class=\"language-text\">spark.readStream</code>을 통해서, Streaming Data Source를 읽습니다. <strong>socket</strong> 형식으로 읽으며, Host 이름과 포트번호를 지정 합니다. 이는 <strong>Kafka</strong>, <strong>File</strong> 등, 다양한 Data Source를 지원 합니다. return 값은 <strong>DataFrame</strong> 입니다.</li>\n<li><code class=\"language-text\">words</code> 를 통해, <code class=\"language-text\">DataFrame</code>을 <code class=\"language-text\">Dataset[String]</code>으로 변환 시키고, <code class=\"language-text\">wordCounts</code>를 통해 <code class=\"language-text\">groupBy</code>를 수행 합니다. 해당 부분이 <strong>Aggregation(집계)</strong> 을 수행 하는 역할 입니다. 추후에 <strong>사용 용도에 맞추어</strong> 해당 부분을 수정 가능 합니다.</li>\n<li><code class=\"language-text\">writeStream</code>을 통해 출력을 할 수 있습니다. 현재 <code class=\"language-text\">console</code> 모드를 이용 하여, 결과물을 출력 합니다. 혹은 <strong>External Sink</strong>를 이용하여, <strong>HDFS, S3, 로컬 파일 시스템 등</strong> 다양한 곳에 데이터를 저장 할 수 있습니다.</li>\n<li><strong>Streaming Job</strong>이 종료 되지 않게 하기 위해서, <code class=\"language-text\">query.awaitTermination()</code>을 통해, 종료 트리거가 될 때 까지 대기 합니다.</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token keyword\">import</span> <span class=\"token namespace\">org<span class=\"token punctuation\">.</span>apache<span class=\"token punctuation\">.</span>spark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">.</span>functions<span class=\"token punctuation\">.</span></span>_\n<span class=\"token keyword\">import</span> <span class=\"token namespace\">org<span class=\"token punctuation\">.</span>apache<span class=\"token punctuation\">.</span>spark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">.</span></span>SparkSession\n\n<span class=\"token keyword\">val</span> spark <span class=\"token operator\">=</span> SparkSession\n  <span class=\"token punctuation\">.</span>builder\n  <span class=\"token punctuation\">.</span>appName<span class=\"token punctuation\">(</span><span class=\"token string\">\"StructuredNetworkWordCount\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>getOrCreate<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n  \n<span class=\"token keyword\">import</span> <span class=\"token namespace\">spark<span class=\"token punctuation\">.</span>implicits<span class=\"token punctuation\">.</span></span>_\n\n<span class=\"token keyword\">val</span> lines <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>readStream\n  <span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span><span class=\"token string\">\"socket\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>option<span class=\"token punctuation\">(</span><span class=\"token string\">\"host\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"localhost\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>option<span class=\"token punctuation\">(</span><span class=\"token string\">\"port\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">9999</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\">// Split the lines into words</span>\n<span class=\"token keyword\">val</span> words <span class=\"token operator\">=</span> lines<span class=\"token punctuation\">.</span>as<span class=\"token punctuation\">[</span><span class=\"token builtin\">String</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>flatMap<span class=\"token punctuation\">(</span>_<span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span><span class=\"token string\">\" \"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\">// Generate running word count</span>\n<span class=\"token keyword\">val</span> wordCounts <span class=\"token operator\">=</span> words<span class=\"token punctuation\">.</span>groupBy<span class=\"token punctuation\">(</span><span class=\"token string\">\"value\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">val</span> query <span class=\"token operator\">=</span> wordCounts<span class=\"token punctuation\">.</span>writeStream\n  <span class=\"token punctuation\">.</span>outputMode<span class=\"token punctuation\">(</span><span class=\"token string\">\"complete\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span><span class=\"token string\">\"console\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\nquery<span class=\"token punctuation\">.</span>awaitTermination<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>일단 터미널에 <code class=\"language-text\">nc -lk 9999</code> 를 작성하여, Socket 연결을 시도 해 보죠! (NetCat 이라는 친구 입니다.) 그 다음, <strong>Spark Application</strong>을 실행 해 보면 다음과 같습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">$ <span class=\"token function\">nc</span> <span class=\"token parameter variable\">-lk</span> <span class=\"token number\">9999</span>\napache spark\napache hadoop</code></pre></div>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">-------------------------------------------\nBatch: <span class=\"token number\">0</span>\n-------------------------------------------\n+------+-----+\n<span class=\"token operator\">|</span> value<span class=\"token operator\">|</span>count<span class=\"token operator\">|</span>\n+------+-----+\n<span class=\"token operator\">|</span>apache<span class=\"token operator\">|</span>    <span class=\"token number\">1</span><span class=\"token operator\">|</span>\n<span class=\"token operator\">|</span> spark<span class=\"token operator\">|</span>    <span class=\"token number\">1</span><span class=\"token operator\">|</span>\n+------+-----+\n\n-------------------------------------------\nBatch: <span class=\"token number\">1</span>\n-------------------------------------------\n+------+-----+\n<span class=\"token operator\">|</span> value<span class=\"token operator\">|</span>count<span class=\"token operator\">|</span>\n+------+-----+\n<span class=\"token operator\">|</span>apache<span class=\"token operator\">|</span>    <span class=\"token number\">2</span><span class=\"token operator\">|</span>\n<span class=\"token operator\">|</span> spark<span class=\"token operator\">|</span>    <span class=\"token number\">1</span><span class=\"token operator\">|</span>\n<span class=\"token operator\">|</span>hadoop<span class=\"token operator\">|</span>    <span class=\"token number\">1</span><span class=\"token operator\">|</span>\n+------+-----+</code></pre></div>\n<p>이런 식으로, <strong>word</strong>를 <strong>split</strong>하여 <strong>group by</strong> 한 결과가 실시간으로 반영 되는 것을 볼 수가 있습니다.</p>\n<p>원래라면, <strong>Trigger</strong>라는 요소를 사용하여, 들어 온 데이터에 대해 <strong>몇 초마다 쿼리를 수행</strong> 하고 저장 할 지를 정할 수도 있습니다. 다음과 같이, output option에 삽입 해 주면 됩니다. 아래 예제는 <strong>10초 마다 Trigger</strong>를 보내는 예제입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\n<span class=\"token keyword\">import</span> <span class=\"token namespace\">org<span class=\"token punctuation\">.</span>apache<span class=\"token punctuation\">.</span>spark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">.</span>streaming<span class=\"token punctuation\">.</span></span>Trigger\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\n\n<span class=\"token keyword\">val</span> query <span class=\"token operator\">=</span> wordCounts<span class=\"token punctuation\">.</span>writeStream\n  <span class=\"token punctuation\">.</span>trigger<span class=\"token punctuation\">(</span>Trigger<span class=\"token punctuation\">.</span>ProcessingTime<span class=\"token punctuation\">(</span><span class=\"token string\">\"10 seconds\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>outputMode<span class=\"token punctuation\">(</span><span class=\"token string\">\"complete\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>format<span class=\"token punctuation\">(</span><span class=\"token string\">\"console\"</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">.</span>start<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span></code></pre></div>\n<h3>기본 컨셉</h3>\n<p><strong>Spark Structured Streaming</strong>의 주 컨셉은 <strong>Input Data Stream</strong>이  테이블의 행으로 계속 <strong>추가 되는</strong> 것 입니다.</p>\n<p align=\"center\">\n    <img src=\"/post_image/spark-structured-streaming/01-01.png\" min-width=\"400px\" />\n    <div align=\"center\" color=\"#aaaaaa\">Data Stream이 들어오면, 데이터가 추가 되는 형태</div>\n</p>\n<p>그렇게 계속 들어오게 된 <strong>Input</strong>들은, 매 <strong>trigger</strong> 시간마다, 작성한 쿼리를 바탕으로 <strong>Result Table</strong>을 생성 합니다. 위의 예제로 따지면, 작성한 쿼리는 <code class=\"language-text\">words</code> 에 해당 되겠네요! 현재 우리는 <code class=\"language-text\">query</code> 결과를 디버깅을 위해서 format으로 지정 했지만, file로 지정이 된다면, result table이 <strong>업데이트 될 때</strong> 마다, <strong>external sink에 저장</strong> 되는 형식으로 진행이 되겠군요.</p>\n<p align=\"center\">\n    <img src=\"/post_image/spark-structured-streaming/01-02.png\" min-width=\"400px\" />\n    <div align=\"center\" color=\"#aaaaaa\">Data Stream이 들어오면, 데이터가 추가 되는 형태</div>\n</p>\n<p><code class=\"language-text\">outputMode</code> 의 설정을 통해서, <strong>external sink</strong>에 <strong>어떻게 저장</strong> 할 것인지 정해 줄 수 있습니다.</p>\n<ul>\n<li>Append Mode: last trigger 이후, 새롭게 추가된 row만 <strong>output sink에 기록</strong> 됩니다. 단, <strong>추가된 row가 바뀔 일이 없어야</strong> 사용 할 수 있습니다. <strong>Streaming으로 들어온 데이터를 계속 적재</strong> 해 주는 식이라면 편하게 사용 할 수 있겠군요! 아마, <strong>select, where, map, flatMap, filter, join</strong> 같은, 기존에 row에 영향을 주지 않는 쿼리들을 수행 할 수 있을 것으로 보이네요.</li>\n<li>Complete Mode: Trigger된 모든 <strong>result table</strong>을 <strong>모두 저장</strong>하는 모드입니다. <strong>주기적으로 복잡한 aggregation</strong>이 필요 한 경우에 사용 하면 될 것 같습니다.</li>\n<li>Update Mode: last trigger 이후, <strong>업데이트 된 Result Table의 행만</strong> 추가 됩니다.</li>\n</ul>\n<p>다음 시간에는, <strong>로컬(Docker)에서 Kafka를 구동</strong> 하여, 실제 데이터를 받아보는 실습을 진행 하고, <strong>Trigger, Checkpoint,</strong> 그리고 <strong>StreamingQueryListener</strong>에 대해서 알아 보도록 하겠습니다.</p>","id":"c7182a1e-4786-5112-9048-54cca7467f6d","frontmatter":{"date":"2023-05-13","path":"/data-engineering/spark-structured-streaming","title":"Spark Structured Streaming이란?","tags":["Data-Engineering"],"keyword":"Spark, Spark Structured Streaming","summary":"Spark로 Streaming Job을 수행 해 보자.","img":"/post_image/thumbnail/spark-structured-streaming.png","series":"Spark Structured Streaming"}}},"pageContext":{"postPath":"/data-engineering/spark-structured-streaming","series":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"861432bb-8f64-567c-93b9-306d6599cb57","excerpt":"머릿말 안녕하세요? 새해부터 찾아온 JustKode, 박민재입니다. 오늘은 Spark Structured Streaming에 대한 Unit Test를 수행 하는 법에 대해서 공유 드려 보려고 합니다. What is Spark Structured Streaming? Spark Structured Streaming은 Spark SQL API (Dataframe, Dataset)를 이용하여, Streaming…","frontmatter":{"date":"2024-01-07","tags":["Data-Engineering"],"path":"/data-engineering/spark-structured-streaming-unit-test","title":"Unit Test of Spark Structured Streaming","img":"/post_image/thumbnail/spark-structured-streaming-unit-test.jpeg","summary":"더 정밀한 Streaming Unit Test를 위해"}}},{"node":{"id":"c7182a1e-4786-5112-9048-54cca7467f6d","excerpt":"안녕하세요? 오늘은 Spark Structured Streaming에 대해서 알아 보도록 하겠습니다. Spark Structured Streaming이란? Spark Structured Streaming은, Spark SQL (Dataset/DataFrame) 엔진 기반의, 확장 가능하고, 내결함성이 있는 Stream Processing Engine 입니다. 이는 Batch 작업에서 구조화된 데이터를 처리 하는 것 처럼, Streaming…","frontmatter":{"date":"2023-05-13","tags":["Data-Engineering"],"path":"/data-engineering/spark-structured-streaming","title":"Spark Structured Streaming이란?","img":"/post_image/thumbnail/spark-structured-streaming.png","summary":"Spark로 Streaming Job을 수행 해 보자."}}}]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"861432bb-8f64-567c-93b9-306d6599cb57","excerpt":"머릿말 안녕하세요? 새해부터 찾아온 JustKode, 박민재입니다. 오늘은 Spark Structured Streaming에 대한 Unit Test를 수행 하는 법에 대해서 공유 드려 보려고 합니다. What is Spark Structured Streaming? Spark Structured Streaming은 Spark SQL API (Dataframe, Dataset)를 이용하여, Streaming…","frontmatter":{"date":"2024-01-07","tags":["Data-Engineering"],"path":"/data-engineering/spark-structured-streaming-unit-test","title":"Unit Test of Spark Structured Streaming","img":"/post_image/thumbnail/spark-structured-streaming-unit-test.jpeg","summary":"더 정밀한 Streaming Unit Test를 위해"}}},{"node":{"id":"e46743f9-8a4c-5573-9ff5-b8e1b50ff808","excerpt":"머릿말 안녕하세요? JustKode, 박민재 입니다. 이 글을 쓰는 지금, Data Engineer로 LINE Plus에 입사한지 벌써 만으로 1년이 다 되어 가네요. 올해 1월에 입사 했으니까요. 첫 사회 생활, 첫 회사에서 (첫 인턴, 첫 회사가 LINE Plus…","frontmatter":{"date":"2023-12-21","tags":["etc","Data-Engineering"],"path":"/etc/2023-retrospect","title":"1년차 Data Engineer의 회고","img":"/post_image/2023-retrospect.jpeg","summary":"2023년을 되돌아 봅니다."}}},{"node":{"id":"5a1c922e-fff4-5443-87e1-15282d06c381","excerpt":"Summary Data Engineering이 구체적으로 무엇인지 알아 봅니다. 왜 Data Engineering이 필요 하게 되었는지 알아 봅니다. Data Engineer는 팀 혹은 회사에서 어떤 역할을 수행 하는지 알아 봅니다. 머릿말 안녕하세요? JustKode, 박민재입니다. 오늘은 Data Engineering이 구체적으로 무엇 인지, Data Engineer…","frontmatter":{"date":"2023-12-05","tags":["Data-Engineering"],"path":"/data-engineering/data-engineering-intro","title":"Data Engineering이란?","img":"/post_image/data-engineering-intro.png","summary":"Data Engineering에 대하여"}}},{"node":{"id":"f2d9685d-9388-518e-b7b2-c03a527f3c85","excerpt":"오늘은 Spark 성능 튜닝에서 가장 중요한 SQL Tuning에 대해서 알아 보도록 하겠습니다. 사실 파라미터(Shuffle Partition 갯수, Executor Instance, Core, Memory 조정) 튜닝 또한, 도움이 될 수 있겠습니다만, 그 전에 Execution Plan이 잘 짜여져 있지 않다면, 파라미터 튜닝이 큰 영향을 주지는 못할 것 입니다. What is Execution Plan? Exection Plan…","frontmatter":{"date":"2023-08-22","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-3","title":"Spark 성능 튜닝 - 3. Spark SQL Tuning","img":"/post_image/thumbnail/spark-performance-tuning-3.jpeg","summary":"Spark의 쿼리를 튜닝 해 보자"}}}]}}}}},"staticQueryHashes":["3819017183","63159454"],"slicesMap":{}}