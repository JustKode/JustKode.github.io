{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/data-engineering/dbt-intro/","result":{"data":{"markdownRemark":{"html":"<p>안녕하세요? 박민재입니다. 오늘날의 <strong>Data Engineer들</strong>은 로그, 혹은 이벤트 데이터를 <strong>적재</strong> 하고, 적절한 <strong>데이터 변환</strong>을 수행 하여 데이터 분석가, 백엔드 개발자, 혹은 소비자에게 직접 <strong>전달</strong> 하는 등의 <strong>다양한 업무</strong>들을 수행 합니다.</p>\n<p>하지만, 점점 <strong>데이터의 요구사항이 복잡</strong>해지면서 데이터 엔지니어가 제공해야 하는 데이터와, 데이터 분석가가 제공해야 하는 데이터의 싱크를 맞추기가 상당히 어려워 졌습니다. 데이터 엔지니어가 데이터 분석가의 도메인 영역에 대해 완벽히 이해 하는 것도 어렵고, 데이터 분석가가 데이터 엔지니어들의 데이터 엔지니어링 영역에 대해 완벽히 이해 하는 것도 어렵습니다.</p>\n<p>또한, <strong>Data Analysis Logic의 파편화</strong>로 인해 <strong>낮은 재사용성과 유지보수의 어려움</strong>을 겪게 되는 경우도 있고, 문서화의 부재로 Data Mart의 History 파악이 어렵게 되며, 당연스럽게도 이에 따라 Table 관리에 있어, Side Effect 파악이 어려워 지는 문제도 동반하게 됩니다.</p>\n<p>그렇기 때문에, 데이터 분석가와 데이터 엔지니어들이 서로의 공통된 언어로 이야기를 나눠야 하는데요, 바로 <strong>SQL</strong> 입니다. dbt는 <strong>SQL을 이용하여 Data Pipeline을 구성</strong>하게 해주는 툴입니다. ETL 과정 중의 <strong>Transform 파트를 전문적</strong>으로 다루며, 사용자가 SQL과 DB Connection, Schema 등이 포함된 yaml 파일을 전달하면, Data Lineage에 맞게 <strong>데이터를 Build 해주며, Versioning, Test, Documentation 모두를 일괄적으로 수행</strong> 할 수 있습니다. 추가적으로 dbt에서는 <strong>SQL의 모듈화</strong>를 수행 하여, 데이터의 <strong>중복 연산 방지</strong>, <strong>통일된 지표</strong>를 통해 데이터 분석 결과에 오류를 줄이는 등의 효과를 누릴 수 있습니다.</p>\n<p>dbt는 BigQuery, Snowflake, Databricks 등의 Cloud Platform 부터, Spark, Hive, Trino, Flink 등을 지원합니다.</p>\n<p align=\"center\">\n\t<img src=\"/post_image/dbt/01-01.png\" width=\"786px\"/>\n</p>\n<h2>Features</h2>\n<p>주요 기능은 다음과 같습니다.</p>\n<ul>\n<li>SQL Script를 <strong>모듈화</strong> 하여, <strong>모델 간 의존 관계를 추적</strong> 하여 작업 수행 가능\n<ul>\n<li>event Table에서 매출이 발생한 경우의 데이터만 추출 한 spending_event 라는 Staging Model을 생성</li>\n<li>spending_event에서 이벤트 유형, 광고 ID 별로 group by 한 spending_event_per_type 이라는 Intermediate Model 생성</li>\n<li>spending_event_per_type에서 Click이 발생 한 경우만 추출한 spending_event_click 이라는 최종 Mart Table Model 생성</li>\n</ul>\n</li>\n<li>집계 된 데이터에서, 어떤 데이터를 참고하는지 Data Lineage를 제공해 줄 수 있음.</li>\n<li>SQL Script에 대해 Git과 같은 버전 관리 시스템으로 History를 추적 할 수 있음</li>\n<li>Data Quality Check에 있어서도, dbt Model에 대해서 null 값, 중복 키등에 대한 Test를 손쉽게 수행 할 수 있음</li>\n</ul>\n<p align=\"center\">\n\t<img src=\"/post_image/dbt/01-02.png\" width=\"786px\"/>\n</p>\n<h2>Model</h2>\n<p>모델은 하나의 SQL 쿼리로 대응됩니다.\nSource 데이터가 준비 되었다면, 이를 통해, 여러 가지 모델을 구성 할 수 있습니다.</p>\n<p>또한, materialized Option을 통해, 실물 테이블을 생성할지 말지를 결정 할 수 있습니다.</p>\n<ul>\n<li>table: 실물 테이블 생성</li>\n<li>view: View만 생성</li>\n<li>incremental: 증분된 데이터에 대해서만 생성, 증분 기준은 템플릿을 통해 지정 가능.</li>\n</ul>\n<p>dbt 공식 문서에서는 Staging, Intermediate, Mart 패턴을 사용 하는 것을 권장 합니다.</p>\n<h3>Staging</h3>\n<p>초기 정제가 진행 되는 모델입니다.\n작성 예제는 다음과 같습니다.</p>\n<blockquote>\n<p>stg_spending_event.sql</p>\n</blockquote>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\">{{ config<span class=\"token punctuation\">(</span>materialized<span class=\"token operator\">=</span><span class=\"token string\">'table'</span><span class=\"token punctuation\">)</span> }} <span class=\"token comment\">-- materialized table: 실제 Table로 파일을 생성하겠다는 의미입니다.</span>\n\n<span class=\"token keyword\">SELECT</span>\n    event_type<span class=\"token punctuation\">,</span>\n    country<span class=\"token punctuation\">,</span> \n    cost<span class=\"token punctuation\">,</span>\n    ad_id\n<span class=\"token keyword\">FROM</span> <span class=\"token keyword\">local</span><span class=\"token punctuation\">.</span>event\n<span class=\"token keyword\">WHERE</span> cost <span class=\"token operator\">></span> <span class=\"token number\">0</span></code></pre></div>\n<h3>Intermediate</h3>\n<p>중간 집계나 조인 테이블이 들어가는 모델입니다.</p>\n<blockquote>\n<p>int_spending_event_per_type.sql</p>\n</blockquote>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\">{{ config<span class=\"token punctuation\">(</span>materialized<span class=\"token operator\">=</span><span class=\"token string\">'table'</span><span class=\"token punctuation\">)</span> }}\n\n<span class=\"token keyword\">SELECT</span> event_type<span class=\"token punctuation\">,</span> country<span class=\"token punctuation\">,</span> ad_id<span class=\"token punctuation\">,</span> <span class=\"token function\">sum</span><span class=\"token punctuation\">(</span>cost<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">FROM</span> {{ ref<span class=\"token punctuation\">(</span><span class=\"token string\">'stg_spending_event'</span><span class=\"token punctuation\">)</span> }}\n<span class=\"token keyword\">GROUP</span> <span class=\"token keyword\">BY</span> event_type<span class=\"token punctuation\">,</span> country<span class=\"token punctuation\">,</span> ad_id</code></pre></div>\n<h3>Mart</h3>\n<p>최종 분석용 테이블입니다.</p>\n<blockquote>\n<p>spending_event_click.sql</p>\n</blockquote>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\">{{ config<span class=\"token punctuation\">(</span>materialized<span class=\"token operator\">=</span><span class=\"token string\">'table'</span><span class=\"token punctuation\">)</span> }}\n\n<span class=\"token keyword\">SELECT</span> event_type<span class=\"token punctuation\">,</span> country<span class=\"token punctuation\">,</span> ad_id<span class=\"token punctuation\">,</span> <span class=\"token function\">sum</span><span class=\"token punctuation\">(</span>cost<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">FROM</span> {{ ref<span class=\"token punctuation\">(</span><span class=\"token string\">'int_spending_event_per_type'</span><span class=\"token punctuation\">)</span> }}\n<span class=\"token keyword\">GROUP</span> <span class=\"token keyword\">BY</span> event_type<span class=\"token punctuation\">,</span> country<span class=\"token punctuation\">,</span> ad_id</code></pre></div>\n<h3>Project Structure</h3>\n<p>Spark를 이용하여 Build 하는 것을 기준으로 한다면,\n<code class=\"language-text\">pip install dbt-core \"dbt-spark[session]\"</code> 을 통해 Package 설치 이후 (PySpark 포함)\n<code class=\"language-text\">dbt init &lt;project_name></code> 을 통해서 프로젝트를 초기화 한 이후에, dbt 설치 경로에 있는 <code class=\"language-text\">profiles.yml</code> 을 수정하여 실행 옵션을 설정 할 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">your_profile_name</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">target</span><span class=\"token punctuation\">:</span> dev\n  <span class=\"token key atrule\">outputs</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">dev</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">type</span><span class=\"token punctuation\">:</span> spark\n      <span class=\"token key atrule\">method</span><span class=\"token punctuation\">:</span> session\n      <span class=\"token key atrule\">schema</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span>database/schema name<span class=\"token punctuation\">]</span>\n      <span class=\"token key atrule\">host</span><span class=\"token punctuation\">:</span> NA  <span class=\"token comment\"># not used, but required by `dbt-core`</span>\n      <span class=\"token key atrule\">server_side_parameters</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">\"spark.driver.memory\"</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"4g\"</span></code></pre></div>\n<p>실제 Model이 추가 된 Project는 다음과 같이 구성할 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">models/\n├── staging/\n│   ├── stg_spending_event.sql\n├── intermediate/\n│   ├── int_spending_event_per_type.sql\n└── mart/\n    └── country/\n        ├── spending_event_click.sql\n        ├── spending_event_watch.sql \n        └── spending_event_buy.sql</code></pre></div>\n<p><code class=\"language-text\">dbt run</code>을 통해서 이를 실행 할 수 있습니다. 자세한 내용은 공식 문서를 참조 해 주세요.</p>\n<ul>\n<li>dbt core Quickstart: <a href=\"https://docs.getdbt.com/guides/manual-install?step=1\">https://docs.getdbt.com/guides/manual-install?step=1</a></li>\n<li>Spark Session 설정: <a href=\"https://docs.getdbt.com/docs/core/connect-data-platform/spark-setup#session\">https://docs.getdbt.com/docs/core/connect-data-platform/spark-setup#session</a></li>\n</ul>\n<h2>Incremental Materilization</h2>\n<p>dbt는 데이터를 증분해서 가져올 수 있는 기능을 가지고 있습니다.\n위의 예제에서 만약 현재 데이터 연산이 완료된 이후의 데이터만 가져오고 싶다면, 다음과 같이 필터링을 수행 할 수 있습니다.</p>\n<p>(is_incremental()은, 최초 Build를 제외한 Build에 bool True를 반환 합니다.)</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\">{{ config<span class=\"token punctuation\">(</span>\n    materialized<span class=\"token operator\">=</span><span class=\"token string\">'incremental'</span><span class=\"token punctuation\">,</span>\n    unique_key<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token string\">'hour'</span><span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">)</span> }}\n\n<span class=\"token keyword\">SELECT</span>\n    event_type<span class=\"token punctuation\">,</span>\n    country<span class=\"token punctuation\">,</span> \n    cost<span class=\"token punctuation\">,</span>\n    ad_id<span class=\"token punctuation\">,</span>\n    <span class=\"token keyword\">hour</span>\n<span class=\"token keyword\">FROM</span> <span class=\"token keyword\">local</span><span class=\"token punctuation\">.</span>event\n<span class=\"token keyword\">WHERE</span> cost <span class=\"token operator\">></span> <span class=\"token number\">0</span>\n\n{<span class=\"token operator\">%</span> <span class=\"token keyword\">if</span> is_incremental<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">%</span>} \n    <span class=\"token comment\">-- 이미 처리된 테이블이 있는 경우에만 실행</span>\n    <span class=\"token operator\">AND</span> <span class=\"token keyword\">hour</span> <span class=\"token operator\">></span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">SELECT</span> <span class=\"token keyword\">coalesce</span><span class=\"token punctuation\">(</span><span class=\"token function\">max</span><span class=\"token punctuation\">(</span><span class=\"token keyword\">hour</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1990010100</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">FROM</span> {{ this }}<span class=\"token punctuation\">)</span>\n    <span class=\"token comment\">-- {{ this }}는 현재 생성되는 테이블을 참조</span>\n{<span class=\"token operator\">%</span> endif <span class=\"token operator\">%</span>}</code></pre></div>\n<h3>FYI</h3>\n<p><code class=\"language-text\">if is_incremental()</code> 내에 있는 코드는 템플릿 입니다.\n만약 모델을 선언하는 SQL에 WHERE 절이 없다면, is_incremental에 WHERE 절을 추가하여\n증분된 데이터에 대해서만 가져올 수 있도록 합니다. 예제는 아래와 같습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\">{{ config<span class=\"token punctuation\">(</span>materialized<span class=\"token operator\">=</span><span class=\"token string\">'incremental'</span><span class=\"token punctuation\">)</span> }}\n\n<span class=\"token keyword\">select</span>\n    <span class=\"token operator\">*</span><span class=\"token punctuation\">,</span>\n    my_slow_function<span class=\"token punctuation\">(</span>my_column<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">from</span> {{ ref<span class=\"token punctuation\">(</span><span class=\"token string\">'app_data_events'</span><span class=\"token punctuation\">)</span> }}\n\n{<span class=\"token operator\">%</span> <span class=\"token keyword\">if</span> is_incremental<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">%</span>}\n<span class=\"token keyword\">where</span> event_time <span class=\"token operator\">>=</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">select</span> <span class=\"token keyword\">coalesce</span><span class=\"token punctuation\">(</span><span class=\"token function\">max</span><span class=\"token punctuation\">(</span>event_time<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span><span class=\"token string\">'1900-01-01'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">from</span> {{ this }} <span class=\"token punctuation\">)</span>\n{<span class=\"token operator\">%</span> endif <span class=\"token operator\">%</span>}</code></pre></div>\n<p>이를 통해서 증분된 데이터에 대해서만 연산을 수행 하여, UPDATE 혹은 APPEND를 수행 할 수 있습니다.</p>\n<h3>Airflow Integration</h3>\n<p>아무리 생각해도 dbt Job을 Scheduling 하기 위해서는 Airflow가 적합 할 것으로 보입니다. 어떻게 하면 좋을까요? 바로 Astronomer 사에서 준비한 <a href=\"https://github.com/astronomer/astronomer-cosmos\">Astronomer Cosmos</a>를 이용하여, DBT Job을 Airflow DAG 으로 제작 하면 됩니다.</p>\n<p>당근 마켓에서 실제로 이를 활용한 사례가 있으니, 이를 참고해 주세요.\n<a href=\"https://medium.com/daangn/dbt%EC%99%80-airflow-%EB%8F%84%EC%9E%85%ED%95%98%EB%A9%B0-%EB%A7%88%EC%A3%BC%ED%95%9C-7%EA%B0%80%EC%A7%80-%EB%AC%B8%EC%A0%9C%EB%93%A4-61250a9904ab\">https://medium.com/daangn/dbt%EC%99%80-airflow-%EB%8F%84%EC%9E%85%ED%95%98%EB%A9%B0-%EB%A7%88%EC%A3%BC%ED%95%9C-7%EA%B0%80%EC%A7%80-%EB%AC%B8%EC%A0%9C%EB%93%A4-61250a9904ab</a></p>\n<p>지금 까지 간단하게 dbt와 dbt의 모델 구성법에 대해서 러프하게 살펴보았습니다. 긴글 읽어 주셔서 감사합니다.</p>","id":"ac2612fc-bbdf-504f-94d9-d2bab40e68e6","frontmatter":{"date":"2025-03-26","path":"/data-engineering/dbt-intro","title":"dbt란 무엇인가?","tags":["Data-Engineering"],"keyword":"dbt,Data Engineering,dbt tutorial","summary":"SQL을 바탕으로 Data Transformation Pipeline을 구성해 주는 dbt를 알아보자.","img":"/post_image/thumbnail/dbt-intro.webp","series":null}}},"pageContext":{"postPath":"/data-engineering/dbt-intro","series":{"data":{"allMarkdownRemark":{"edges":[]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"ac2612fc-bbdf-504f-94d9-d2bab40e68e6","excerpt":"안녕하세요? 박민재입니다. 오늘날의 Data Engineer…","frontmatter":{"date":"2025-03-26","tags":["Data-Engineering"],"path":"/data-engineering/dbt-intro","title":"dbt란 무엇인가?","img":"/post_image/thumbnail/dbt-intro.webp","summary":"SQL을 바탕으로 Data Transformation Pipeline을 구성해 주는 dbt를 알아보자."}}},{"node":{"id":"dec9e599-9444-556f-99ba-7a1dc27a4dbb","excerpt":"안녕하세요, 박민재입니다. 혹시 Data Discovery에 중요성을 느껴, DataHub를 사용하려고 하시는 분이 있나요? 아마 그렇다면 DataHub를 도입한 사례를 몇 개 읽어 보셨을꺼라 생각합니다. 대표적으로 국내 기업에서는 뱅크샐러드, 소카, 베이글코드 등에서 성공적으로 도입한 사례들을 회사 사이트에 올리는 경우를 확인 할 수 있었어요. SOCAR BankSalad BagelCode DataHub…","frontmatter":{"date":"2025-03-02","tags":["Data-Engineering"],"path":"/data-engineering/to-datahub-user","title":"DataHub 도입을 고려 하시는 분들에게","img":"/post_image/thumbnail/to-datahub-user.webp","summary":"DataHub를 도입 하려고 할 때 알아야 할 점"}}},{"node":{"id":"24718dd5-aa23-578b-aa81-0ca11fcc0f06","excerpt":"안녕하세요, 박민재입니다. 저번 시간에는 Spark Operator가 무엇인지 간단하게 알아 보았는데요, 이번 시간에는 실제로 Spark Operator Helm Chart를 설치하여, Spark Operator 관련 구동 준비를 한 후, Spark Operator 관련 Resource를 작성 하여 실제 Job을 제출 해 보는 시간을 가져 보도록 하겠습니다. Spark Operator Helm Chart Spark Operator…","frontmatter":{"date":"2025-02-02","tags":["Data-Engineering"],"path":"/data-engineering/spark-operator-2","title":"Spark Operator - 2. Practice","img":"/post_image/thumbnail/spark-operator.jpg","summary":"Spark Operator를 사용 해 보자."}}},{"node":{"id":"9ec3e659-5978-5311-b4d5-fc9d0902e008","excerpt":"안녕하세요, 박민재입니다. 아마 2년 전 즈음에 Spark on Kubernetes 관련 내용을 다뤘었는데요 (이 글 또한, 개정판을 작성 해 볼게요), 이번에는 Spark Job을 Kubernetes Cluster에 편리하게 제출할 수 있게 하는 Spark Operator에 대해 알아 보도록 하겠습니다. Spark on Kubernetes를 사용하는 이유? 그렇다고, 이 글에서 아예 설명 하지 않고 넘어가는 것은 아닌 것 같아, Spark…","frontmatter":{"date":"2025-01-19","tags":["Data-Engineering"],"path":"/data-engineering/spark-operator-1","title":"Spark Operator - 1. Spark Operator란?","img":"/post_image/thumbnail/spark-operator.jpg","summary":"Kubernetes Cluster로의 Spark Job 제출을 도와주는 Spark Operator가 무엇 인지 알아보자."}}}]}}}}},"staticQueryHashes":["3819017183","63159454"],"slicesMap":{}}