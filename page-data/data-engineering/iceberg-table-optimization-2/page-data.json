{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/data-engineering/iceberg-table-optimization-2/","result":{"data":{"markdownRemark":{"html":"<h3>Intro</h3>\n<p>안녕하세요, 박민재입니다. <a href=\"https://justkode.kr/data-engineering/iceberg-table-optimization-1/\">저번 시간</a>에는 <strong>Table Optimization을 위한 압축 기법</strong>에 대해 배웠습니다. 이번 시간에는 <strong>압축을 제외한 Table Optimization 기법</strong>을 알아 보도록 하겠습니다.</p>\n<h3>Partitioning</h3>\n<p>역시, 기존의 방법을 꺼낼 때가 왔습니다. 바로 <strong>Partitioning</strong>입니다. 동일한 Column의 동일한 Value를 가진 친구들은 같은 File로 묶어 주는 방식이죠. 어? 왜 Directory가 아니라 File 이냐고요?</p>\n<p align=\"center\">\n    <img src=\"/post_image/apache-iceberg/table-optimization/06.png\" style=\"width:75%;max-width:768px;\"/>\n</p>\n<p>참고로, File이 저장 될 때 (Snapshot이 생성 될 때), Metadata Table인 <code class=\"language-text\">files</code> Table에서 각 file에 대한 partition 정보가 함께 삽입 됩니다. ref) <a href=\"https://iceberg.apache.org/docs/1.6.0/spark-queries/#querying-with-sql\">https://iceberg.apache.org/docs/1.6.0/spark-queries/#querying-with-sql</a></p>\n<p>추후 Query 실행 시, <strong>Physical Plan 측에서 이를 참고</strong> 합니다.</p>\n<h4>Hidden Partitioning</h4>\n<p>Iceberg는 <strong>Hidden Partitioning을 지원</strong> 합니다. 여기서 말하는 Hidden은 End User가 데이터를 삽입 함에 있어,\nPartitioning을 고려 하지 않고 데이터를 삽입 하여도 알아서 <strong>Query Planning에서 Partitioning을 수행 함</strong>을 의미합니다.</p>\n<p>아래와 같이 timestamp를 가지고 있는 column에 대해서 <code class=\"language-text\">PARTITIONED BY months(time)</code>을 추가하여 <code class=\"language-text\">CREATE TABLE</code>을 수행 하면,\n파일이 삽입 될 때, <strong>timestamp 내에서 가지고 있는 월별로 데이터를 삽입</strong> 합니다.</p>\n<p>month 제외 하고도, 다양한 <strong>transforms function</strong>을 제공합니다.</p>\n<ul>\n<li>year: <strong>연 단위 조회</strong>시 이점</li>\n<li>month: <strong>월 단위 이상 조회</strong>에서 이점</li>\n<li>day: <strong>일 단위 이상 조회</strong>에서 이점</li>\n<li>hour: <strong>시간 단위 이상 조회</strong>에서 이점</li>\n<li>truncate(column, num): column내 문자열 중, <strong>num 번째 문자 까지 절삭</strong>하여 파티셔닝\n<ul>\n<li>ex) a라는 column에서 'abc'라는 값을 가지고 있고, num=2 라면 a='ab'로 파티셔닝</li>\n</ul>\n</li>\n<li>bucket(num, column): num 갯수만큼 <strong>hash 값을 바탕</strong>으로 파티셔닝 하여 저장. 단일 값을 추출 할 때 유용.</li>\n</ul>\n<p>아래는 예시입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">CREATE</span> <span class=\"token keyword\">TABLE</span> catalog<span class=\"token punctuation\">.</span>MyTable <span class=\"token punctuation\">(</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">)</span> PARTITIONED <span class=\"token keyword\">BY</span> months<span class=\"token punctuation\">(</span><span class=\"token keyword\">time</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">USING</span> iceberg<span class=\"token punctuation\">;</span></code></pre></div>\n<p>만약 상기한 테이블에 SELECT QUERY로 time column에 대해 조건을 넣어 쿼리를 수행 한다면,\n조회 하려는 Partition을 명시 하지 않아도 <strong>Query Plan 수립 과정에서 최적화</strong>하여 조회 합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">SELECT</span> <span class=\"token operator\">*</span> <span class=\"token keyword\">FROM</span> MYTABLE <span class=\"token keyword\">WHERE</span> <span class=\"token keyword\">time</span> <span class=\"token operator\">BETWEEN</span> <span class=\"token string\">'2022-07-01 00:00:00'</span> <span class=\"token operator\">AND</span> <span class=\"token string\">'2022-07-31 00:00:00'</span><span class=\"token punctuation\">;</span></code></pre></div>\n<h4>Partition Evolution</h4>\n<p>기존에는 Directory 별로 저장 되기 때문에, 만약 month로 파티셔닝 하다가, day로도 파티셔닝을 하려는 요구 사항이 있을 때, directory의 물리적인 한계 때문에 실시간으로 Partition을 Evolution 하는 것이 어려웠습니다.</p>\n<p>하지만, Iceberg 에서는 이를 가능하게 할 수 있습니다. 기존 아래와 같은 테이블이 있다고 가정 하겠습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">CREATE</span> <span class=\"token keyword\">TABLE</span> catalog<span class=\"token punctuation\">.</span>members <span class=\"token punctuation\">(</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">)</span> PARTITIONED <span class=\"token keyword\">BY</span> years<span class=\"token punctuation\">(</span>registration_ts<span class=\"token punctuation\">)</span> <span class=\"token keyword\">USING</span> iceberg<span class=\"token punctuation\">;</span></code></pre></div>\n<p>만약 데이터의 절대적인 크기가 커져, 이를 <strong>월 단위로 파티셔닝</strong> 하여야 할 때, 다음 명령어로 <strong>Partition Evolution</strong>을 할 수 있습니다. 단, 해당 명령 수행 직후, 파일을 다시 Write하는 것은 아니고, <code class=\"language-text\">rewriteDataFiles</code> 수행 시에 다시 Write 하는 방식입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">ALTER</span> <span class=\"token keyword\">TABLE</span> catalog<span class=\"token punctuation\">.</span>members <span class=\"token keyword\">ADD</span> <span class=\"token keyword\">PARTITION</span> FIELD months<span class=\"token punctuation\">(</span>registration_ts<span class=\"token punctuation\">)</span></code></pre></div>\n<p>당연히 Partition 삭제 또한 가능합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">ALTER</span> <span class=\"token keyword\">TABLE</span> catalog<span class=\"token punctuation\">.</span>members <span class=\"token keyword\">DROP</span> <span class=\"token keyword\">PARTITION</span> FIELD bucket<span class=\"token punctuation\">(</span><span class=\"token number\">24</span><span class=\"token punctuation\">,</span> id<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<h3>Copy-on-Write VS Merge-on-Read</h3>\n<p>우리는 File Update, Delete가 발생 했을 때 기존에는 <strong>Copy-on-Write</strong>, 즉, <strong>File을 지우고 다시 쓰는 방법</strong>으로 대응 하였습니다. 하지만, Iceberg에서는 <strong>Merge-on-Read</strong> 라는 새로운 기법을 제공 합니다.</p>\n<p>요약하면 다음과 같습니다.</p>\n<table>\n<thead>\n<tr>\n<th>Update Style</th>\n<th>Summary</th>\n<th>Read Speed</th>\n<th>Write Speed</th>\n<th>Best Practice</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Copy-on-Write</td>\n<td>파일 내의 <strong>특정 Row에 Update가 필요</strong>하게 될 경우, <strong>해당 파일을 다시 Write</strong> 합니다.</td>\n<td>가장 빠름</td>\n<td>가장 느림</td>\n<td></td>\n</tr>\n<tr>\n<td>Merge-on-Read (position delete)</td>\n<td>파일 내의 <strong>특정 Row에 Update가 필요</strong>하게 될 경우, 특정 Row의 <strong>Position을 기억</strong>하고, <strong>해당 Row가 어떻게 Update 되어야 하는지 내역을 저</strong>장 합니다. 읽을 때는, <strong>해당 Postition의 Row인 경우, Data를 수정 하여 Read</strong> 합니다.</td>\n<td>빠름</td>\n<td>빠름</td>\n<td>Read Cost를 감소 시키기 위해서 <strong>File Compaction</strong>을 수행</td>\n</tr>\n<tr>\n<td>Merge-on-Read (equality delete)</td>\n<td>파일 내의 <strong>특정 Row에 Update가 필요</strong>하게 될 경우, 해당 파일이 <strong>어떻게 Update 되어야 하는지 내역을 저장</strong> 하고, 읽을 때 <strong>전체 Update</strong>를 수행 합니다.</td>\n<td>느림</td>\n<td>매우 빠름</td>\n<td>Read Cost를 감소 시키기 위해서 <strong>File Compaction을 자주 수행</strong></td>\n</tr>\n</tbody>\n</table>\n<h4>Copy on Write</h4>\n<p>기존의 방식과 동일하게, File Update가 필요 할 경우, Rewrite를 수행합니다.</p>\n<p align=\"center\">\n    <img src=\"/post_image/apache-iceberg/table-optimization/07.png\" style=\"width:75%;max-width:768px;\"/>\n</p>\n<h4>Merge On Read</h4>\n<p>Copy-on-Write의 대체 방식입니다.\nUpdate/Delete 시에 File을 Rewrite 하는 것이 아닌, <strong>Datafile/Delete File을 생성 하고, 읽을 때 이를 대응</strong> 합니다.</p>\n<p>File Update/Delete의 <strong>성능을 올리기 위해 사용</strong>합니다. 단, <strong>조회 시 이를 Merge 하는 과정</strong>이 있어,\n<strong>조회가 많은 Table에 사용하기 적합하지 않습니다.</strong></p>\n<p>아래의 예시는 Position delete의 예시입니다.</p>\n<p align=\"center\">\n    <img src=\"/post_image/apache-iceberg/table-optimization/08.png\" style=\"width:75%;max-width:768px;\"/>\n</p>\n<p>아래는 Equality delete의 delete file의 예시입니다.</p>\n<p align=\"center\">\n    <img src=\"/post_image/apache-iceberg/table-optimization/09.png\" style=\"width:30%;max-width:300px;\"/>\n</p>\n<p>Read 성능을 높이기 위해서는 <strong>압축을 자주 수행</strong> 해 줘야 하는데요, <strong>전체 파일에 대해서 재압축 하는 것은 효율적이지 않습니다.</strong> timestamp에 대한 Partition을 이용하여, <strong>다시는 Update / Delete가 없을 것으로 예상 되는 최신의 Record에 압축을 수행 하는 것을 권장</strong>하고 있습니다. 또한, <strong>partial progress</strong>로, <strong>현재 파일을 읽고 있는 유저</strong>에게도 업데이트 된 파일이 최대한 보이게 하는 것도 좋습니다.</p>\n<h4>Configuring COW and MOR</h4>\n<p>DELETE, UPDATE, MERGE 각각의 Update 시에 <strong>다른 Operation이 수행 되도록 설정</strong> 해 줄 수 있습니다.</p>\n<ul>\n<li>write.delete.mode</li>\n<li>write.update.mode</li>\n<li>write.merge.mode</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">CREATE</span> <span class=\"token keyword\">TABLE</span> catalog<span class=\"token punctuation\">.</span>people <span class=\"token punctuation\">(</span>\n    id <span class=\"token keyword\">int</span><span class=\"token punctuation\">,</span>\n    first_name string<span class=\"token punctuation\">,</span>\n    last_name string\n<span class=\"token punctuation\">)</span> TBLPROPERTIES <span class=\"token punctuation\">(</span>\n    <span class=\"token string\">'write.delete.mode'</span><span class=\"token operator\">=</span><span class=\"token string\">'copy-on-write'</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">'write.update.mode'</span><span class=\"token operator\">=</span><span class=\"token string\">'merge-on-read'</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">'write.merge.mode'</span><span class=\"token operator\">=</span><span class=\"token string\">'merge-on-read'</span>\n<span class=\"token punctuation\">)</span> <span class=\"token keyword\">USING</span> iceberg<span class=\"token punctuation\">;</span>\n \n<span class=\"token keyword\">ALTER</span> <span class=\"token keyword\">TABLE</span> catalog<span class=\"token punctuation\">.</span>people <span class=\"token keyword\">SET</span> TBLPROPERTIES <span class=\"token punctuation\">(</span>\n    <span class=\"token string\">'write.delete.mode'</span><span class=\"token operator\">=</span><span class=\"token string\">'merge-on-read'</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">'write.update.mode'</span><span class=\"token operator\">=</span><span class=\"token string\">'copy-on-write'</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">'write.merge.mode'</span><span class=\"token operator\">=</span><span class=\"token string\">'copy-on-write'</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>하지만, 해당 옵션들은 Spark 외의 다른 Engine을 사용 할 때 무시 될 수 있으니, <strong>각 Engine을 사용 할 때 마다 이를 지원 하는지 확인</strong>이 필요 합니다.</p>\n<h3>Other Considerations</h3>\n<h4>Metric Collection</h4>\n<p>여러 개의 Data File을 가르키고 있는 Manifest 파일은, <strong>쿼리의 최적화를 위해서 각 field의 metric을 Tracking</strong> 하고 있습니다.</p>\n<ul>\n<li>Value의 갯수, null 값의 갯수, distinct 한 값의 갯수</li>\n<li>Upper Bound + Lower Bound</li>\n</ul>\n<p>만약, 테이블의 column이 100개를 넘어 간다면, 일부 metric을 수집하지 않는 것을 고려 해 볼 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">ALTER</span> <span class=\"token keyword\">TABLE</span> catalog<span class=\"token punctuation\">.</span>db<span class=\"token punctuation\">.</span>students <span class=\"token keyword\">SET</span> TBLPROPERTIES <span class=\"token punctuation\">(</span>\n    <span class=\"token string\">'write.metadata.metrics.default'</span><span class=\"token operator\">=</span><span class=\"token string\">'none'</span>\n    <span class=\"token string\">'write.metadata.metrics.column.col1'</span><span class=\"token operator\">=</span><span class=\"token string\">'none'</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">'write.metadata.metrics.column.col2'</span><span class=\"token operator\">=</span><span class=\"token string\">'full'</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">'write.metadata.metrics.column.col3'</span><span class=\"token operator\">=</span><span class=\"token string\">'counts'</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">'write.metadata.metrics.column.col4'</span><span class=\"token operator\">=</span><span class=\"token string\">'truncate(16)'</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<ul>\n<li>write.metadata.metrics.default: 전체 Column의 <strong>Metric 수집 기본값</strong>을 설정 해 줄 수 있으며, default value는 truncate(16) 입니다.</li>\n<li>write.metadata.metrics.column.col1: <strong>특정 Column의 Metric 수집 값</strong>을 설정 해 줄 수 있습니다.</li>\n</ul>\n<p>다음은 Option입니다.</p>\n<ul>\n<li>none: 아무 metric도 수집하지 않습니다.</li>\n<li>counts: <strong>values, distinct values, null values 의 count 만을 수집</strong> 합니다.</li>\n<li>truncate(num): counts + upper/lower bound를 수집하는데, 문자열인 경우 <strong>num 번째 문자열 까지 절삭한 데이터</strong>로 메타 데이터를 수집 합니다.</li>\n<li>full: count + upper/lower bound를 수집합니다. 문자열인 경우, <strong>전체 문자열에 대해 메타 데이터를 수집</strong> 합니다.</li>\n</ul>\n<h4>Rewriting Manifests</h4>\n<p>Streaming Table 같이, 많은 Commit이 발생하면 <strong>수많은 Metadata가 쌓이게 되어 문제가 발생</strong> 할 수 있습니다.\nMetadata가 산개하여 있다면, 그 만큼 많은 Metadata를 읽어야 한다는 것이 문제에요.</p>\n<p>우리는 이를 <code class=\"language-text\">rewrite_manifest</code>를 통해 해결 할 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">CALL</span> catalog<span class=\"token punctuation\">.</span>system<span class=\"token punctuation\">.</span>rewrite_manifests<span class=\"token punctuation\">(</span><span class=\"token string\">'MyTable'</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">CALL</span> catalog<span class=\"token punctuation\">.</span>system<span class=\"token punctuation\">.</span>rewrite_manifests<span class=\"token punctuation\">(</span><span class=\"token string\">'MyTable'</span><span class=\"token punctuation\">,</span> <span class=\"token boolean\">false</span><span class=\"token punctuation\">)</span> <span class=\"token comment\">-- Memory에 이슈가 있을 시, Spark에서 Cache하는 것을 중단</span></code></pre></div>\n<h4>Optimizing Storage</h4>\n<p>우리는 Table에 대해 update를 수행 하고, 압축을 수행하지만,\n과거의 Snapshot이 과거의 데이터를 바라 본다면, <strong>이는 계속 실제로 데이터가 Storage에 남게 되는 문제</strong>가 발생 합니다.</p>\n<p>이를 해결하기 위해 우리는 <strong>snapshot을 expire</strong> 하고, 더 이상 <strong>snapshot이 가르키지 않는 파일들을 제</strong>거 해 줄 수 있습니다.</p>\n<p>아래는 snapshot expire 예시입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token comment\">-- 2023-02-01 전 Snapshot 모두 삭제, 단 100개만 남기고</span>\n<span class=\"token keyword\">CALL</span> catalog<span class=\"token punctuation\">.</span>system<span class=\"token punctuation\">.</span>expire_snapshots<span class=\"token punctuation\">(</span><span class=\"token string\">'MyTable'</span><span class=\"token punctuation\">,</span> <span class=\"token keyword\">TIMESTAMP</span> <span class=\"token string\">'2023-02-01 00:00:00.000'</span><span class=\"token punctuation\">,</span> <span class=\"token number\">100</span><span class=\"token punctuation\">)</span>\n \n<span class=\"token comment\">-- snapshot_ids=53 Snapshot만을 삭제</span>\n<span class=\"token keyword\">CALL</span> catalog<span class=\"token punctuation\">.</span>system<span class=\"token punctuation\">.</span>expire_snapshots<span class=\"token punctuation\">(</span><span class=\"token keyword\">table</span> <span class=\"token operator\">=</span><span class=\"token operator\">></span> <span class=\"token string\">'MyTable'</span><span class=\"token punctuation\">,</span> snapshot_ids <span class=\"token operator\">=</span><span class=\"token operator\">></span> ARRAY<span class=\"token punctuation\">(</span><span class=\"token number\">53</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li>table: Table 명</li>\n<li>older_then: 해당 timestamp 보다 오래된 snapshot 삭제</li>\n<li>retain_last: 삭제 이후 남길 최소한의 snapshot 갯수</li>\n<li>snapshot_ids: 삭제 할 snapshot id</li>\n<li>max_concurrent_deletes: 삭제 할 때 사용한 thread 갯수</li>\n<li>stream_results: File 삭제시, <strong>삭제할 파일을 Spark Driver에 RDD partition으로 쪼개서 전송</strong>. 큰 데이터를 삭제할 때 유용</li>\n</ul>\n<p>아래는 remove_orphan_file 예시입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token comment\">-- MyTable 내, Metadata가 가르키고 있지 않은 file 삭제</span>\n<span class=\"token keyword\">CALL</span> catalog<span class=\"token punctuation\">.</span>system<span class=\"token punctuation\">.</span>remove_orphan_files<span class=\"token punctuation\">(</span><span class=\"token keyword\">table</span> <span class=\"token operator\">=</span><span class=\"token operator\">></span> <span class=\"token string\">'MyTable'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li>table: Table 명</li>\n<li>older_then: 해당 timestamp 보다 오래된 orphan file 삭제</li>\n<li>location: 삭제할 orphan file의 위치. 기본 값은 table의 location입니다.</li>\n<li>dry_run: 삭제하는 대신, 삭제할 파일의 list 출력</li>\n<li>max_concurrent_deletes: 삭제 할 때 사용한 thread 갯수</li>\n</ul>\n<h4>Write Distribution Mode</h4>\n<p>요약 하자면 <strong>비슷한 파일</strong>들은 <strong>비슷한 Task</strong>에 들어가서, <strong>동일 File로 Write</strong> 되어야 합니다.\n이를 위해서 <strong>pre-sort</strong>를 진행 하는 여러 가지 <strong>distribution mode</strong>를 지원 합니다.</p>\n<p>아래는 테이블에 distribution mode를 지정 하는 예시입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">ALTER</span> <span class=\"token keyword\">TABLE</span> catalog<span class=\"token punctuation\">.</span>MyTable <span class=\"token keyword\">SET</span> TBLPROPERTIES <span class=\"token punctuation\">(</span>\n<span class=\"token string\">'write.distribution-mode'</span><span class=\"token operator\">=</span><span class=\"token string\">'hash'</span><span class=\"token punctuation\">,</span>\n<span class=\"token string\">'write.delete.distribution-mode'</span><span class=\"token operator\">=</span><span class=\"token string\">'none'</span><span class=\"token punctuation\">,</span>\n<span class=\"token string\">'write.update.distribution-mode'</span><span class=\"token operator\">=</span><span class=\"token string\">'range'</span><span class=\"token punctuation\">,</span>\n<span class=\"token string\">'write.merge.distribution-mode'</span><span class=\"token operator\">=</span><span class=\"token string\">'hash'</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<ul>\n<li>none: 1.2.0 버전 이전의 기본 값으로, File Write를 수행 할 때, pre-sort를 진행 하지 않고, <strong>빠르게 값을 Write</strong> 합니다.</li>\n<li>hash: 1.2.0 버전 이후의 기본 값으로, File Write를 수행 할 때, Partition Key에 맞춰서 <strong>Hash 값 바탕</strong>으로 <strong>pre-sort 하여 Write</strong> 합니다.</li>\n<li>range: File Write를 수행 할 때, Partition Key에 맞춰서 <strong>range 바탕으로 pre-sort 하여 Write</strong> 합니다.</li>\n</ul>\n<h4>Datafile Bloom Filters</h4>\n<p>bloom filter는 <strong>Hash 함수</strong>를 이용하여, <strong>데이터가 있을 수도 있는 파일에 마킹</strong>을 하는 방식입니다.</p>\n<p>다음과 같이 hash function으로 생성 된 Bloom Filter가 있다고 가정 하겠습니다 → <code class=\"language-text\">[0,1,1,0,0,1,1,1,1,0]</code>\n0번째는 0값을 가지고 있으므로, 해당 File을 조회 할 필요가 없고, 1번째는 1값을 가지고 있으므로, <strong>해당 파일을 조회하면 데이터가 있을 가능성</strong>이 있습니다.</p>\n<p>Table에 설정은 다음과 같이 가능합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">ALTER</span> <span class=\"token keyword\">TABLE</span> catalog<span class=\"token punctuation\">.</span>MyTable <span class=\"token keyword\">SET</span> TBLPROPERTIES <span class=\"token punctuation\">(</span>\n<span class=\"token string\">'write.parquet.bloom-filter-enabled.column.col1'</span><span class=\"token operator\">=</span> <span class=\"token boolean\">true</span><span class=\"token punctuation\">,</span>\n<span class=\"token string\">'write.parquet.bloom-filter-max-bytes'</span><span class=\"token operator\">=</span> <span class=\"token number\">1048576</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>Bloom Filter 크기를 키우면, 더 세밀한 bloom filter를 만들 수 있지만,\n연산 시 Memory와 저장 시 Disk에 영향을 줄 수 있으니, <strong>Trade-off</strong>를 고려하여야 합니다.</p>","id":"cbb6e851-d864-5552-86d7-08c81b4a54cc","frontmatter":{"date":"2024-11-24","path":"/data-engineering/iceberg-table-optimization-2","title":"Iceberg Table의 성능 최적화 - 2. Partitioning, MOR, Others","tags":["Data-Engineering"],"keyword":"Iceberg,Apache Iceberg,빅데이터,Big Data,Optimization,성능 최적화","summary":"File Merge를 통한 성능 최적화에 대해 알아보자.","img":"/post_image/thumbnail/iceberg-table-optimization-1.webp","series":"Iceberg Table Optimization"}}},"pageContext":{"postPath":"/data-engineering/iceberg-table-optimization-2","series":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"cbb6e851-d864-5552-86d7-08c81b4a54cc","excerpt":"Intro 안녕하세요, 박민재입니다. 저번 시간에는 Table Optimization을 위한 압축 기법에 대해 배웠습니다. 이번 시간에는 압축을 제외한 Table Optimization 기법을 알아 보도록 하겠습니다. Partitioning 역시, 기존의 방법을 꺼낼 때가 왔습니다. 바로 Partitioning입니다. 동일한 Column의 동일한 Value를 가진 친구들은 같은 File로 묶어 주는 방식이죠. 어? 왜 Directory…","frontmatter":{"date":"2024-11-24","tags":["Data-Engineering"],"path":"/data-engineering/iceberg-table-optimization-2","title":"Iceberg Table의 성능 최적화 - 2. Partitioning, MOR, Others","img":"/post_image/thumbnail/iceberg-table-optimization-1.webp","summary":"File Merge를 통한 성능 최적화에 대해 알아보자."}}},{"node":{"id":"3c44d6b1-b341-5256-bb40-e6a58835b474","excerpt":"Intro 안녕하세요, 박민재입니다. 오늘은 Apache Iceberg의 Table에 수행 되는 쿼리가 최적의 성능으로 작동 될 수 있도록, File Compaction을 통해 이를 수행하는 방법에 대해 이야기 하는 시간을 가져 보도록 하겠습니다. File Compaction 우리가 쿼리를 수행 시, Hive Metastore의 정보를 이용하더라도, 혹은 Iceberg의 Metadata…","frontmatter":{"date":"2024-11-10","tags":["Data-Engineering"],"path":"/data-engineering/iceberg-table-optimization-1","title":"Iceberg Table의 성능 최적화 - 1. 압축","img":"/post_image/thumbnail/iceberg-table-optimization-1.webp","summary":"File Merge를 통한 성능 최적화에 대해 알아보자."}}}]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"cbb6e851-d864-5552-86d7-08c81b4a54cc","excerpt":"Intro 안녕하세요, 박민재입니다. 저번 시간에는 Table Optimization을 위한 압축 기법에 대해 배웠습니다. 이번 시간에는 압축을 제외한 Table Optimization 기법을 알아 보도록 하겠습니다. Partitioning 역시, 기존의 방법을 꺼낼 때가 왔습니다. 바로 Partitioning입니다. 동일한 Column의 동일한 Value를 가진 친구들은 같은 File로 묶어 주는 방식이죠. 어? 왜 Directory…","frontmatter":{"date":"2024-11-24","tags":["Data-Engineering"],"path":"/data-engineering/iceberg-table-optimization-2","title":"Iceberg Table의 성능 최적화 - 2. Partitioning, MOR, Others","img":"/post_image/thumbnail/iceberg-table-optimization-1.webp","summary":"File Merge를 통한 성능 최적화에 대해 알아보자."}}},{"node":{"id":"3c44d6b1-b341-5256-bb40-e6a58835b474","excerpt":"Intro 안녕하세요, 박민재입니다. 오늘은 Apache Iceberg의 Table에 수행 되는 쿼리가 최적의 성능으로 작동 될 수 있도록, File Compaction을 통해 이를 수행하는 방법에 대해 이야기 하는 시간을 가져 보도록 하겠습니다. File Compaction 우리가 쿼리를 수행 시, Hive Metastore의 정보를 이용하더라도, 혹은 Iceberg의 Metadata…","frontmatter":{"date":"2024-11-10","tags":["Data-Engineering"],"path":"/data-engineering/iceberg-table-optimization-1","title":"Iceberg Table의 성능 최적화 - 1. 압축","img":"/post_image/thumbnail/iceberg-table-optimization-1.webp","summary":"File Merge를 통한 성능 최적화에 대해 알아보자."}}},{"node":{"id":"6eaa7aeb-a4fe-5cd9-bbe5-309dde97514b","excerpt":"안녕하세요, 박민재입니다. 오늘은 Airflow DB를 관리하는 방법에 대해서 이야기 나눠 보도록 하겠습니다. Airflow Backend Database Airflow에서 Backend Database는 어떤 역할을 할까요? Airflow에서 DAG을 실행 하기 위해서, Airflow는 다음과 같은 정보들을 Backend Database에 저장하여 정합성을 유지 합니다. DagRun: 특정 Interval에 실행 된 DagRun…","frontmatter":{"date":"2024-10-27","tags":["Data-Engineering"],"path":"/data-engineering/airflow-db-management","title":"Airflow Backend Database Management (airflow db clean)","img":"/post_image/thumbnail/airflow-db-management.webp","summary":"Airflow의 Backend Database를 관리 하는 법"}}},{"node":{"id":"b595b168-8160-5abf-9641-39746d2f9e82","excerpt":"안녕하세요, 박민재입니다. 오늘은 Spark Application내의 각 Executor 내의 Task에 제한적으로 변수를 공유하는 두 가지 방법에 대해서 알아 보도록 하겠습니다. 시작하기에 앞서 단편적으로 생각 해 보면, Spark Application에서 연산 과정의 변수를 공유 한다는 것은 어려운 일인가? 라는 질문을 던져 볼 수 있습니다. 우리는 Task…","frontmatter":{"date":"2024-10-11","tags":["Data-Engineering"],"path":"/data-engineering/spark-sharing-variables","title":"Sharing Variables in Spark - Broadcast, Accumulator","img":"/post_image/thumbnail/spark-sharing-variables.png","summary":"Spark Application에서 변수를 공유 하는 방법"}}}]}}}}},"staticQueryHashes":["3819017183","63159454"],"slicesMap":{}}