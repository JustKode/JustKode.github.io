{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/data-engineering/spark-sharing-variables/","result":{"data":{"markdownRemark":{"html":"<p>안녕하세요, 박민재입니다. 오늘은 <strong>Spark Application</strong>내의 각 Executor 내의 Task에 <strong>제한적으로 변수를 공유</strong>하는 두 가지 방법에 대해서 알아 보도록 하겠습니다.</p>\n<h3>시작하기에 앞서</h3>\n<p>단편적으로 생각 해 보면, <strong>Spark Application</strong>에서 <strong>연산 과정의 변수를 공유</strong> 한다는 것은 어려운 일인가? 라는 질문을 던져 볼 수 있습니다. 우리는 Task 내에서 변수를 공유 하기 위해서, 다음과 같은 코드를 작성 할 수 있겠죠.</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token keyword\">import</span> <span class=\"token namespace\">org<span class=\"token punctuation\">.</span>apache<span class=\"token punctuation\">.</span>spark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">.</span>functions<span class=\"token punctuation\">.</span></span>_\n<span class=\"token keyword\">import</span> <span class=\"token namespace\">org<span class=\"token punctuation\">.</span>apache<span class=\"token punctuation\">.</span>spark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">.</span></span>Row\n\n<span class=\"token keyword\">val</span> df <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">)</span> <span class=\"token comment\">// DataFrame 선언부</span>\n<span class=\"token keyword\">var</span> i <span class=\"token operator\">=</span> <span class=\"token number\">0</span>\n\n<span class=\"token comment\">// repartition으로 Partitioning을 강제 함. 12개의 task가 생성 됨.</span>\n<span class=\"token comment\">// Parameter로 들어간 lambda 함수를 실행.</span>\ndf<span class=\"token punctuation\">.</span>repartition<span class=\"token punctuation\">(</span><span class=\"token number\">12</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>foreachPartition<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>rows<span class=\"token operator\">:</span> Iterator<span class=\"token punctuation\">[</span>Row<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">=></span> <span class=\"token punctuation\">{</span>  \n    i <span class=\"token operator\">+=</span> <span class=\"token number\">1</span>\n    println<span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">)</span>  <span class=\"token comment\">// 스포: 1 출력됨</span>\n<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n\nprintln<span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">)</span>  <span class=\"token comment\">// 스포: 0 출력됨</span></code></pre></div>\n<p>이는 충분히 납득이 가능한 코드로 보입니다. DataFrame의 <code class=\"language-text\">repartition</code> Method를 생성 하여 <strong>12개의 Partition이 생성</strong> 되도록 repartition을 수행하고, 각 파티션 별로 i에 1씩 더해주는 코드로, <strong>우리는 12라는 값이 출력</strong> 될 것이라고 예상할 수 있겠죠. 하지만, 우리는 Spark Application을 작성 할 때, 유의 할 점이 있습니다.</p>\n<h4>Spark Application Architecture</h4>\n<p align=\"center\">\n    <img src=\"/post_image/spark-sharing-variables/01.png\" max-width=\"400px\" />\n    <div align=\"center\" color=\"#aaaaaa\">Spark Application Architecture</div>\n</p>\n<p><strong>Spark Application Architecture</strong>를 다시 살펴 봅니다. Spark의 <strong>각 Partition</strong> 처리는 <strong>Task 단위로 수행</strong>되며, 이 Task는 <strong>Thread를 통해 1개의 CPU Core</strong>에서 수행 됩니다. (1개의 partition을 처리하는 1개의 task는 1개의 Core에서 실행) Spark Application이 실행 될 때, 아래와 같은 과정이 수행 됩니다.</p>\n<ul>\n<li><strong>SparkContext</strong>가 생성 되면, 각 <strong>Worker Node</strong> 별로 Executor를 위한 <strong>JVM Context 생성</strong></li>\n<li>Driver에서 Spark RDD 연산, 혹은 Spark SQL 연산에 대한 <strong>Action 코드 호출</strong> 시 <strong>Logical Plan</strong>을 <strong>SparkContext로 전달</strong>,</li>\n<li>SparkContext단에서 실제 <strong>Physical Plan을 수립</strong>한 후, Executor Process에 <strong>연산 해야 할 항목 전달</strong></li>\n<li>각 Task는 <strong>Executor Process 내</strong>에서 <strong>Thread를 생성</strong>하여 수행함.</li>\n</ul>\n<p>추가적으로 <strong>Spark Application</strong>에서 사용되는 <strong>DataFrame API</strong>은 <strong>Logical Plan을 Driver Process에 제공하는 API에 불과</strong> 하다는 점도 유의 해야 합니다. 상기한 코드에서 사용된 <code class=\"language-text\">Dataframe.foreachPartition</code>은 각 Partition을 이용하여 Task를 처리 할 때 사용할 함수를 보내는 역할만을 수행 합니다.</p>\n<p>결론적으로는 위에 있는 코드는 어떤 값을 반환 하냐고요? <strong>각자 다른 Process에서 수행 되는 변수는 특별한 로직이 없다면 공유 될 수 없습니다.</strong> 그렇기 때문에 foreachPartition 밖에서 호출 된 println(i)는 0을 반환 하게 됩니다. foreachPartition으로 들어간 익명 함수 안에서는 1을 출력하고요.</p>\n<h4>그럼 어떻게 값을 공유할 수가 있나요?</h4>\n<p>사실 Spark에서 <strong>여러 Executor가 하나의 Driver와 값을 공유</strong>하는 패턴을 <strong>지양</strong>하긴 합니다. 이는 대부분의 상황에서 비효율적인 작업일 가능성이 높기 때문이에요. 하지만, <strong>제한적인 기능만을 수행 하는 두 가지 API를 제공</strong> 합니다. 이는 Driver에서 Executor로의 일관 된 단방향 통신을 지원 하는 <strong>Broadcast</strong>와 Executor에서 Driver로의 단방향 통신을 지원 하는 <strong>Accumulator</strong> 입니다.</p>\n<blockquote>\n<p>Supporting general, read-write shared variables across tasks would be inefficient. However, Spark does provide two limited types of shared variables for two common usage patterns: broadcast variables and accumulators.</p>\n</blockquote>\n<h3>Broadcast</h3>\n<p>Broadcast는 <strong>Driver에서 Executor로의 일관 된 단방향 통신</strong>을 지원 합니다. 이는 Spark의 low-level API를 개발 할 때도 사용하지만, SparkSQL 단에서 Join을 수행 할 때, 작은 Table을 Join하게 될 때 성능을 높이기 위해서, 실제 Logical Plan 단에서 크기가 작은 Table을 Broadcast 하여 각 Task에서 Join하는 경우도 있습니다.(<a href=\"https://spark.apache.org/docs/3.5.3/sql-ref-syntax-qry-select-hints.html#join-hints\">ref</a>)</p>\n<p>사용 방법은 SparkContext의 <code class=\"language-text\">broadcast(value)</code> Method를 호출 하여, <strong>Broadcast 객체</strong>를 얻어 내고, <strong>이를 실제 연산하는 함수에서 참조</strong>하여 사용하는 방식입니다.</p>\n<p><code class=\"language-text\">Broadcast.unpersist()</code> 를 이용하여 Executor에 <strong>Broadcast 된 데이터만을 파기</strong> 하거나 (이를 이용한 경우, Broadcast 변수가 다시 사용되게 되면, 다시 Broadcast를 수행 하여, 데이터를 받을 수 있습니다.) <code class=\"language-text\">Broadcast.destroy()</code> 는 Broadcast로 전파된 모든 데이터와 메타데이터를 파기하여, 다시 사용할 수 없게 합니다. <code class=\"language-text\">destroy(true)</code> 로 동작 시키게 되면, Broadcast 된 데이터가 실제로 파기 될 때 까지 데이터 접근을 blocking 합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token keyword\">import</span> <span class=\"token namespace\">org<span class=\"token punctuation\">.</span>apache<span class=\"token punctuation\">.</span>spark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">.</span>functions<span class=\"token punctuation\">.</span></span>_\n<span class=\"token keyword\">import</span> <span class=\"token namespace\">org<span class=\"token punctuation\">.</span>apache<span class=\"token punctuation\">.</span>spark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">.</span></span>Row\n \n<span class=\"token keyword\">val</span> df <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\">// SparkSession.sparkContext.broadcast(value)로 Broadcast 객체 생성</span>\n<span class=\"token comment\">// type: org.apache.spark.broadcast.Broadcast[Array[Int]]</span>\n<span class=\"token keyword\">val</span> broadcast <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>sparkContext<span class=\"token punctuation\">.</span>broadcast<span class=\"token punctuation\">(</span>Array<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  \n \ndf<span class=\"token punctuation\">.</span>repartition<span class=\"token punctuation\">(</span><span class=\"token number\">12</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>foreachPartition<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>rows<span class=\"token operator\">:</span> Iterator<span class=\"token punctuation\">[</span>Row<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">=></span> <span class=\"token punctuation\">{</span>\n    println<span class=\"token punctuation\">(</span>broadcast<span class=\"token punctuation\">.</span>value<span class=\"token punctuation\">.</span>mkString<span class=\"token punctuation\">(</span><span class=\"token string\">\"Array(\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\", \"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\")\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// Array(1, 2, 3)</span>\n<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n\nbroadcast<span class=\"token punctuation\">.</span>unpersist<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\ndf<span class=\"token punctuation\">.</span>repartition<span class=\"token punctuation\">(</span><span class=\"token number\">12</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>foreachPartition<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>rows<span class=\"token operator\">:</span> Iterator<span class=\"token punctuation\">[</span>Row<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">=></span> <span class=\"token punctuation\">{</span>\n    println<span class=\"token punctuation\">(</span>broadcast<span class=\"token punctuation\">.</span>value<span class=\"token punctuation\">.</span>mkString<span class=\"token punctuation\">(</span><span class=\"token string\">\"Array(\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\", \"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\")\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// Array(1, 2, 3)</span>\n<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n\nbroadcast<span class=\"token punctuation\">.</span>destory<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\ndf<span class=\"token punctuation\">.</span>repartition<span class=\"token punctuation\">(</span><span class=\"token number\">12</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>foreachPartition<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>rows<span class=\"token operator\">:</span> Iterator<span class=\"token punctuation\">[</span>Row<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">=></span> <span class=\"token punctuation\">{</span>\n    println<span class=\"token punctuation\">(</span>broadcast<span class=\"token punctuation\">.</span>value<span class=\"token punctuation\">.</span>mkString<span class=\"token punctuation\">(</span><span class=\"token string\">\"Array(\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\", \"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\")\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// ERROR OCCUR!</span>\n<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h3>Accumulator</h3>\n<p><strong>Accumulator</strong>는 <strong>Executor에서 Driver로의 일관 된 단방향 통신</strong>을 지원 합니다. 단, 여러 개의 Executor에서 연산 된 값들을 하나의 Driver로 모아주는 역할을 수행 하다보니, <strong>결합법칙과 교환법칙</strong>이 가능 한 연산을 지원 합니다. 대표적으로 덧셈 연산이 있죠.</p>\n<blockquote>\n<p>저희 학생 때 한 내용이라 기억이 잘 안나실까봐...</p>\n<p>세 정수 a, b, c에 대하여</p>\n<ul>\n<li>교환법칙: a + b = b + a, a × b = b × a</li>\n<li>결합법칙: (a + b) + c = a + (b + c), (a × b) × c = a × (b × c)</li>\n</ul>\n</blockquote>\n<p>기본적으로 <code class=\"language-text\">SparkContext</code>에서 <code class=\"language-text\">longAccumulator</code>, <code class=\"language-text\">doubleAccumulator</code>를 사용 하실 수 있습니다.</p>\n<p>여기서 동작을 유심히 보면 좋은데요, Driver에서 <code class=\"language-text\">accumulator.value</code> 를 참조 하였을 때와, Executor 내에서 수행되는 함수에서 <code class=\"language-text\">accumulator.value</code> 를 참조 하였을 때, 각 <strong>Executor에서 연산 된 값</strong>을 지속적으로 <strong>Drvier의 Accumulator에 더해 주는 것</strong>을 확인 할 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token keyword\">import</span> <span class=\"token namespace\">org<span class=\"token punctuation\">.</span>apache<span class=\"token punctuation\">.</span>spark<span class=\"token punctuation\">.</span>sql<span class=\"token punctuation\">.</span>functions<span class=\"token punctuation\">.</span></span>_\n \n<span class=\"token keyword\">val</span> df <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">var</span> long <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>sparkContext<span class=\"token punctuation\">.</span>longAccumulator<span class=\"token punctuation\">(</span><span class=\"token string\">\"My Accumulator\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token comment\">// SparkSession.sparkContext.longAccumulator(name)으로 name이라는 이름을 가진 LongAccumulator 생성</span>\n \ndf<span class=\"token punctuation\">.</span>repartition<span class=\"token punctuation\">(</span><span class=\"token number\">12</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>foreachPartition<span class=\"token punctuation\">(</span>rows <span class=\"token keyword\">=></span> <span class=\"token punctuation\">{</span>\n    long<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n \nprintln<span class=\"token punctuation\">(</span>long<span class=\"token punctuation\">.</span>value<span class=\"token punctuation\">)</span>  <span class=\"token comment\">// 12를 출력합니다. 해당 println은 Driver 단에서 호출 됩니다. Partition 갯수 만큼 add가 된 모습.</span>\n \ndf<span class=\"token punctuation\">.</span>repartition<span class=\"token punctuation\">(</span><span class=\"token number\">12</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>foreachPartition<span class=\"token punctuation\">(</span>rows <span class=\"token keyword\">=></span> <span class=\"token punctuation\">{</span>\n    long<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n \nprintln<span class=\"token punctuation\">(</span>long<span class=\"token punctuation\">.</span>value<span class=\"token punctuation\">)</span>  <span class=\"token comment\">// 24를 출력합니다. 이는 계속 누적됩니다.</span>\n \ndf<span class=\"token punctuation\">.</span>repartition<span class=\"token punctuation\">(</span><span class=\"token number\">12</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>foreachPartition<span class=\"token punctuation\">(</span>rows <span class=\"token keyword\">=></span> <span class=\"token punctuation\">{</span>\n    long<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n    println<span class=\"token punctuation\">(</span>long<span class=\"token punctuation\">.</span>value<span class=\"token punctuation\">)</span>  <span class=\"token comment\">// 각 Executor에서는 1을 출력 합니다.</span>\n<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n \nprintln<span class=\"token punctuation\">(</span>long<span class=\"token punctuation\">.</span>value<span class=\"token punctuation\">)</span>  <span class=\"token comment\">// 36을 출력 합니다.</span></code></pre></div>\n<p>직접 <code class=\"language-text\">AccumulatorV2</code> 라는 친구를 상속 받아, 결합법칙과 교환법칙이 가능한 연산에 대해서 <code class=\"language-text\">Accumulator</code>를 구현 할 수 있습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token keyword\">class</span> VectorAccumulatorV2 <span class=\"token keyword\">extends</span> AccumulatorV2<span class=\"token punctuation\">[</span>MyVector<span class=\"token punctuation\">,</span> MyVector<span class=\"token punctuation\">]</span> <span class=\"token punctuation\">{</span>\n\n  <span class=\"token keyword\">private</span> <span class=\"token keyword\">val</span> myVector<span class=\"token operator\">:</span> MyVector <span class=\"token operator\">=</span> MyVector<span class=\"token punctuation\">.</span>createZeroVector\n\n  <span class=\"token keyword\">def</span> reset<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token operator\">:</span> <span class=\"token builtin\">Unit</span> <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span>\n    myVector<span class=\"token punctuation\">.</span>reset<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">}</span>\n\n  <span class=\"token keyword\">def</span> add<span class=\"token punctuation\">(</span>v<span class=\"token operator\">:</span> MyVector<span class=\"token punctuation\">)</span><span class=\"token operator\">:</span> <span class=\"token builtin\">Unit</span> <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span>\n    myVector<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>v<span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">}</span>\n  <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token comment\">// Then, create an Accumulator of this type:</span>\n<span class=\"token keyword\">val</span> myVectorAcc <span class=\"token operator\">=</span> <span class=\"token keyword\">new</span> VectorAccumulatorV2\n<span class=\"token comment\">// Then, register it into spark context:</span>\nsc<span class=\"token punctuation\">.</span>register<span class=\"token punctuation\">(</span>myVectorAcc<span class=\"token punctuation\">,</span> <span class=\"token string\">\"MyVectorAcc1\"</span><span class=\"token punctuation\">)</span></code></pre></div>","id":"b595b168-8160-5abf-9641-39746d2f9e82","frontmatter":{"date":"2024-10-11","path":"/data-engineering/spark-sharing-variables","title":"Sharing Variables in Spark - Broadcast, Accumulator","tags":["Data-Engineering"],"keyword":"Spark, Spark Optimization, Spark Tips","summary":"Spark Application에서 변수를 공유 하는 방법","img":"/post_image/thumbnail/spark-sharing-variables.png","series":null}}},"pageContext":{"postPath":"/data-engineering/spark-sharing-variables","series":{"data":{"allMarkdownRemark":{"edges":[]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"dcd44de2-0eff-56f8-ac2d-2a99250ab9cf","excerpt":"안녕하세요? 박민재입니다. 오늘은 Iceberg Table을 관리하는 방법을 Metadata Table의 사용을 중심으로 깊게 알아 보도록 하겠습니다. Apache Iceberg의 경우에는 Metadata Table 기능을 매우 강력하게 지원합니다. 이를 통해 Iceberg Table을 운영을 쉽게 수행 할 수 있죠. 예를 들어, Table의 Evolution이 어떻게 진행 되었는지, 파일들이 어떻게 Partitioning…","frontmatter":{"date":"2024-12-05","tags":["Data-Engineering"],"path":"/data-engineering/iceberg-table-management-1","title":"Iceberg Table Management - 1. Metadata Table ","img":"/post_image/thumbnail/iceberg-table-management.png","summary":"Metadata Table을 통해 Iceberg Table을 관리 해 보자"}}},{"node":{"id":"cbb6e851-d864-5552-86d7-08c81b4a54cc","excerpt":"Intro 안녕하세요, 박민재입니다. 저번 시간에는 Table Optimization을 위한 압축 기법에 대해 배웠습니다. 이번 시간에는 압축을 제외한 Table Optimization 기법을 알아 보도록 하겠습니다. Partitioning 역시, 기존의 방법을 꺼낼 때가 왔습니다. 바로 Partitioning입니다. 동일한 Column의 동일한 Value를 가진 친구들은 같은 File로 묶어 주는 방식이죠. 어? 왜 Directory…","frontmatter":{"date":"2024-11-24","tags":["Data-Engineering"],"path":"/data-engineering/iceberg-table-optimization-2","title":"Iceberg Table의 성능 최적화 - 2. Partitioning, MOR, Others","img":"/post_image/thumbnail/iceberg-table-optimization-1.webp","summary":"File Merge를 통한 성능 최적화에 대해 알아보자."}}},{"node":{"id":"3c44d6b1-b341-5256-bb40-e6a58835b474","excerpt":"Intro 안녕하세요, 박민재입니다. 오늘은 Apache Iceberg의 Table에 수행 되는 쿼리가 최적의 성능으로 작동 될 수 있도록, File Compaction을 통해 이를 수행하는 방법에 대해 이야기 하는 시간을 가져 보도록 하겠습니다. File Compaction 우리가 쿼리를 수행 시, Hive Metastore의 정보를 이용하더라도, 혹은 Iceberg의 Metadata…","frontmatter":{"date":"2024-11-10","tags":["Data-Engineering"],"path":"/data-engineering/iceberg-table-optimization-1","title":"Iceberg Table의 성능 최적화 - 1. 압축","img":"/post_image/thumbnail/iceberg-table-optimization-1.webp","summary":"File Merge를 통한 성능 최적화에 대해 알아보자."}}},{"node":{"id":"6eaa7aeb-a4fe-5cd9-bbe5-309dde97514b","excerpt":"안녕하세요, 박민재입니다. 오늘은 Airflow DB를 관리하는 방법에 대해서 이야기 나눠 보도록 하겠습니다. Airflow Backend Database Airflow에서 Backend Database는 어떤 역할을 할까요? Airflow에서 DAG을 실행 하기 위해서, Airflow는 다음과 같은 정보들을 Backend Database에 저장하여 정합성을 유지 합니다. DagRun: 특정 Interval에 실행 된 DagRun…","frontmatter":{"date":"2024-10-27","tags":["Data-Engineering"],"path":"/data-engineering/airflow-db-management","title":"Airflow Backend Database Management (airflow db clean)","img":"/post_image/thumbnail/airflow-db-management.webp","summary":"Airflow의 Backend Database를 관리 하는 법"}}}]}}}}},"staticQueryHashes":["3819017183","63159454"],"slicesMap":{}}