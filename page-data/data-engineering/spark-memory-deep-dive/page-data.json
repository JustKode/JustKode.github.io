{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/data-engineering/spark-memory-deep-dive/","result":{"data":{"markdownRemark":{"html":"<h3>Intro</h3>\n<p>안녕하세요, 박민재입니다. 오늘은 <strong>Spark Memory</strong>에 관해 <strong>Deep Dive</strong>를 해 보도록 하겠습니다. Spark는 <strong>In-Memory를 이용</strong>하여, <strong>빠른 연산</strong>을 할 수 있도록 보장합니다. 하지만, In-Memory 연산은 빠른 대신, <strong>비싼 관계로 적은 리소스 만을 활용</strong>할 수 있습니다. 그렇기 때문에 우리는 <strong>효율적으로 Memory를 관리</strong> 하여, Spark Application이 빠르고, 안정적으로 Task를 수행 할 수 있도록 하여야 합니다.</p>\n<p>오늘은 이를 위해, 다음과 같은 목차로 Spark의 Memory를 효율적으로 사용 할 수 있는 방법에 대해 알아 보려고 합니다.</p>\n<ul>\n<li>Executor Memory Structure\n<ul>\n<li>User Memory는 왜 Usable Memory의 40% 가량을 사용 하는가?</li>\n</ul>\n</li>\n<li>Execution Memory와 Storage Memory는 얼마가 충분 한가요?\n<ul>\n<li>Caching 해야할 데이터가 너무 크다면?</li>\n</ul>\n</li>\n</ul>\n<h3>Executor Memory Structure</h3>\n<p>Executor Memory Structure는 <a href=\"https://justkode.kr/data-engineering/spark-performance-tuning-1/\">작년 Post</a>에 설명 드렸던 것과 동일 합니다.</p>\n<p><strong>Spark</strong>에서 <strong>Executor</strong>는 독립된 <strong>JVM</strong> 프로세스로 실행 되며, <strong>Executor</strong>는 클러스터 내부의, <strong>여러 개의 인스턴스에서 실행</strong> 될 수 있습니다. 또한, 각 <strong>인스턴스</strong>는 <strong>다수의 CPU 코어로 Task를 병렬로 실행</strong> 하여, <strong>다수의 Partition을 처리</strong> 할 수 있습니다. 또한, 각 <strong>인스턴스의 메모리</strong>는 다음과 같이 구성 되어 있습니다,</p>\n<p align=\"center\">\n    <img src=\"/post_image/spark-performance-tuning/1-2.jpeg\" max-width=\"400px\" />\n    <div align=\"center\" color=\"#aaaaaa\">Executor Memory</div>\n</p>\n<p><strong>Spark Executor Memory</strong>의 구조는 크게 세 가지 영역으로 나눌 수 있습니다. 이 세 가지 영역은 다음과 같습니다.</p>\n<ul>\n<li><strong>Reserved Memory</strong>: Executor의 기타 기능 및 운영체제에서 사용 하는 메모리 등을 위해 <strong>예약된 메모리 영역</strong>입니다. 예로 <strong>JVM 내에서 사용</strong> 하는 메모리, Java의 <strong>Garbage Collection</strong>에서 발생 하는 메모리 등이 저장 됩니다. <strong>300MB</strong>가 고정 되어 있습니다. <code class=\"language-text\">전체 Executor Memory - Reserved Memory</code>는 <strong>Usable Memory</strong>가 됩니다.</li>\n<li><strong>Spark Memory</strong>: Spark에서 사용하는 메모리 영역으로, <strong>RDD Cache, Broadcast 변수, 실행 계획</strong> 등이 이 영역에 저장 됩니다. <code class=\"language-text\">Usable Memory * spark.memory.fraction</code>이 <strong>Spark Memory</strong>가 됩니다. <code class=\"language-text\">spark.memory.fraction</code>의 기본 값은 0.6입니다.\n<ul>\n<li><strong>Storage Memory</strong>: <strong>Cache를 수행 한 데이터, Broadcast 변수</strong>가 저장 됩니다. 캐싱할 공간이 부족 하면, <strong>LRU(Least Recently Used) 방식으로 제거</strong>합니다. <code class=\"language-text\">Spark Memory * spark.memory.storageFraction</code> 이 <strong>Storage Memory</strong>가 됩니다. <code class=\"language-text\">spark.memory.storageFraction</code>의 기본 값은 0.5입니다.</li>\n<li><strong>Execution Memory</strong>: Spark가 task를 실행 하는 동안 생성되는 object들인 Map 수행 시 <strong>Shuffle Intermediate Buffer</strong>, Hash aggregation step에서의 <strong>Hash Table</strong> 등이 저장 됩니다. 메모리가 충분 하지 않으면, <strong>디스크로 Spill</strong>을 하기 때문에 <strong>디스크 I/O를 줄이기 위해서 고려</strong> 하여야 합니다 <code class=\"language-text\">Spark Memory * (1 - spark.memory.storageFraction)</code> 이 <strong>Execution Memory</strong>가 됩니다.</li>\n<li><strong>Storage Memory</strong>와 <strong>Execution Memory</strong>는 필요 시 서로의 메모리를 <strong>점유</strong> 할 수 있습니다.\n<ul>\n<li>여기서 Spark Memory가 꽉 차있는 상황이라면, Storage Memory는 Execution Memory를 축출할 수 없지만, <strong>Execution Memory</strong>는 <strong>Storage Memory의 임계값</strong>(<code class=\"language-text\">spark.memory.storageFraction</code>) 까지 <strong>축출</strong> 할 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><strong>User Memory</strong>: <strong>사용자 코드에서 직접 할당</strong> 되는 메모리 영역입니다. Spark 내부 메타데이터, 사용자 정의 데이터 구조, UDF, RDD 종속성 정보 등, 요약하면 <strong>사용자가 정의한 데이터구조, UDF가 저장되는 공간</strong>입니다. <code class=\"language-text\">Usable Memory * (1 - spark.memory.fraction)</code> 이 <strong>User Memory</strong>가 됩니다.</li>\n</ul>\n<h4>User Memory는 왜 Usable Memory의 40% 가량을 사용 하는가?</h4>\n<p>생각 해보면, User Memory는 사용자 코드에서 직접 할당되는 메모리 영역으로, SparkSQL API를 주로 사용 한다면 굳이 40%나 할당 할 필요는 없어 보이는데요, 왜 기본 옵션으로 40% 가량이 User Memory로 사용 되는 것 일까요? 그 이유는 <strong>JVM의 Garbage Collection Option</strong>과 관련이 있습니다.</p>\n<p>OpenJDK를 포함한 대부분의 JVM 기본 <strong>Old Generation과 Young Generation의 비율은 2:1</strong>로 설정이 되어 있는데요, Spark Memory는 기본적으로 Old Generation 영역에 있는 것이 GC의 효율상 좋기 때문입니다. 왜냐하면 연산에 필요한 <strong>Spark Memory에 해당 하는 영역들은 재사용 될 일이 훨씬 많기</strong> 때문이에요, 그렇기 때문에 성능 최적화를 위해서 <strong>Old Generation 영역 > Spark Memory 영역을 유지</strong>하여 주고, Spark Memory는 <strong>Old Generation 영역에 유지</strong> 시켜 주는 것이 좋죠.</p>\n<p>그러면, JVM <code class=\"language-text\">NewRatio</code> Parameter를 조정 해서 Old Generation의 비율을 무조건 늘리는게 좋을까요? 아닙니다. 새로 들어온 <strong>Input Partition</strong>은 <strong>Young Generation</strong> 영역에 들어 오게 될꺼에요. 그렇기 때문에, <a href=\"https://spark.apache.org/docs/latest/tuning.html#garbage-collection-tuning\">Spark Documentation</a>에서도, 가능한 해당 옵션은 건들지 말라고 합니다. 그럼에도 불구하고 <code class=\"language-text\">spark.memory.fraction</code>을 건드려야 할 일이 있다면, 아래의 원문 설명을 참조 하세요.</p>\n<blockquote>\n<p>In the GC stats that are printed, if the OldGen is close to being full, reduce the amount of memory used for caching by lowering spark.memory.fraction; it is better to cache fewer objects than to slow down task execution. Alternatively, consider decreasing the size of the Young generation. This means lowering -Xmn if you’ve set it as above. If not, try changing the value of the JVM’s NewRatio parameter. <strong>Many JVMs default this to 2, meaning that the Old generation occupies 2/3 of the heap. It should be large enough such that this fraction exceeds spark.memory.fraction.</strong></p>\n</blockquote>\n<blockquote>\n<p>As an example, if your task is reading data from HDFS, the amount of memory used by the task can be estimated using the size of the data block read from HDFS. <strong>Note that the size of a decompressed block is often 2 or 3 times the size of the block. So if we wish to have 3 or 4 tasks’ worth of working space, and the HDFS block size is 128 MiB, we can estimate the size of Eden to be 4 * 3 * 128MiB.</strong></p>\n</blockquote>\n<h3>Execution Memory와 Storage Memory는 얼마가 충분 한가요?</h3>\n<p>연산 과정에서 Input Partition (기본값 <code class=\"language-text\">spark.sql.files.maxPartitionBytes</code> 128mb)은 크기가 얼마인지, <strong>한 개의 Instance에 몇 개의 Core</strong>가 할당 되는지, 중간중간 <strong>Shuffle Partition의 크기</strong>는 얼마인지 (만약, Partition의 크기가 크다면, Shuffle Partition 갯수를 늘리거나, Spark 3의 AQE 기능을 사용 하면 됩니다.) 어떻게 사용 될 지 알아야, 적정 메모리를 산정 할 수 있습니다. 먼저 Execution Memory 부터 보겠습니다.</p>\n<p><strong>Execution Memory</strong>는 실질적으로 <strong>Shuffle, Join, Aggregation 등의 연산을 수행</strong> 하는 메모리입니다. 그렇기에 Task를 수행 하는 Instance가 <strong>Partition * Core 수의 크기를 버틸 수 있는 수준</strong>이어야 합니다. 위에서 언급 되었던 것처럼, Input Partition의 크기는 압축 되어 있는 File을 In-memory에서 Deserialize 하게 되면, 크기가 <strong>2~3배 가량으로 더 증가</strong> 하기 때문에 이를 고려 하여야 하고요, 그렇기 때문에 <strong>Input Partition 크기 * 3 * Core 수 + Aggregation에 사용되는 Object 객체가 저장 될 수 있는 공간</strong> 정도가 초반에는 있어야 겠군요. 중간중간 <strong>Shuffle Partition</strong>의 크기는 <strong>Shuffle Partition</strong>의 갯수 (<code class=\"language-text\">spark.sql.shuffle.partitions</code>)로 조정 할 수 있는데요, <strong>Shuffle Partition</strong>은 <strong>Core 수의 배수</strong>로 (Core 병렬 연산을 최대화 하기 위함), <strong>Memory Spill이 발생 하지 않는 크기 수준</strong>으로, 대신 Overhead를 고려하여 너무 많지는 않게 설정하면 됩니다. <del>그런데 사실 AQE (Adaptive Query Execution) 사용 하면 그만이긴 한데요</del></p>\n<p><strong>Storage Memory</strong> 같은 경우에는, <strong>Cache 되는 데이터의 크기 / Instance 갯수</strong> 정도로 러프하게 계산 할 수 있을 것 같은데요, 하지만, <strong>Data Skew</strong>가 발생하는 경우, 특정 <strong>Instance에만 더 많은 데이터가 Cache</strong> 될 수 있으니 유의 하시는 것이 좋습니다. 또한, <strong>Broadcast Variable</strong> 같은 경우도 해당 영역에서 저장 되기 때문에 더 넉넉하게 지정 하는 것이 좋습니다.</p>\n<h3>Caching 해야할 데이터가 너무 크다면?</h3>\n<p>기존 많이 사용하는 <code class=\"language-text\">df.cache()</code>를 통해서 메모리를 Cache하면, 내부적으로 <code class=\"language-text\">MEMORY_AND_DISK</code> <a href=\"https://spark.apache.org/docs/3.5.1/rdd-programming-guide.html#rdd-persistence\">Storage Level</a>로 저장 되는 데요, 이는 <strong>Deserialize 한 데이터를 그대로 저장</strong> 하고, 메모리 공간에 더 이상 저장할 수 없는 경우 <strong>디스크에 저장을 수행</strong> 합니다. 그렇기 때문에, 성능을 위해 In-Memory에 저장을 수행 하기 위해서는 <code class=\"language-text\">MEMORY_AND_DISK_SER</code> 혹은 <code class=\"language-text\">MEMORY_ONLY_SER</code> 옵션을 사용 하여, <strong>Serialize 된 데이터</strong>를 <strong>In-Memory에 Cache</strong> 하는 것도 방법 입니다. 이는 <code class=\"language-text\">df.persist(StorageLevel.MEMORY_AND_DISK_SER)</code> 과 같이, <code class=\"language-text\">df.persist()</code> 메서드를 이용 하여 수행 할 수 있습니다. 추가적으로 기존 Java Serialization 보다 더 성능이 좋은 <strong>Kryo Serialization</strong>을 이용 하여, Data를 더 Compact 하게 만들어 In-Memory에 저장 할 수 있습니다. 사용 하는 방법은 <code class=\"language-text\">spark.serializer</code>를 <code class=\"language-text\">org.apache.spark.serializer.KryoSerializer</code> 로 변경 하여 주면 됩니다.</p>","id":"5effd6a1-bfd8-5a86-b718-5eb1e534601e","frontmatter":{"date":"2024-04-12","path":"/data-engineering/spark-memory-deep-dive","title":"Deep Dive of Spark Memory","tags":["Data-Engineering"],"keyword":"Spark, 성능 튜닝, spark memory","summary":"Spark Memory의 깊은 이해를 위해 Deep Dive를 해 보자.","img":"/post_image/thumbnail/spark-memory-deep-dive.jpeg","series":null}}},"pageContext":{"postPath":"/data-engineering/spark-memory-deep-dive","series":{"data":{"allMarkdownRemark":{"edges":[]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"9ec3e659-5978-5311-b4d5-fc9d0902e008","excerpt":"안녕하세요, 박민재입니다. 아마 2년 전 즈음에 Spark on Kubernetes 관련 내용을 다뤘었는데요 (이 글 또한, 개정판을 작성 해 볼게요), 이번에는 Spark Job을 Kubernetes Cluster에 편리하게 제출할 수 있게 하는 Spark Operator에 대해 알아 보도록 하겠습니다. Spark on Kubernetes를 사용하는 이유? 그렇다고, 이 글에서 아예 설명 하지 않고 넘어가는 것은 아닌 것 같아, Spark…","frontmatter":{"date":"2025-01-19","tags":["Data-Engineering"],"path":"/data-engineering/spark-operator-1","title":"Spark Operator - 1. Spark Operator란?","img":"/post_image/thumbnail/spark-operator.jpg","summary":"Kubernetes Cluster로의 Spark Job 제출을 도와주는 Spark Operator가 무엇 인지 알아보자."}}},{"node":{"id":"2001438a-fe37-5b3c-8954-bce7d5e18a7a","excerpt":"안녕하세요? 박민재입니다. 오늘은 Iceberg Table을 관리하는 방법 중 하나인, Branching & Tagging 그리고 Rollback Action에 대해서 알아 보도록 하겠습니다. Isolation of Changes with Branches Iceberg에서는 git과 같은 방식으로 Branch를 만들어, 데이터 변경 사항을 관리 할 수 있습니다. 우리의 사례로 빗대어 보면 H/W 이슈, 혹은 Application…","frontmatter":{"date":"2025-01-03","tags":["Data-Engineering"],"path":"/data-engineering/iceberg-table-management-2","title":"Iceberg Table Management - 2. Branching, Tagging & Rollback","img":"/post_image/thumbnail/iceberg-table-management.png","summary":"Branching, Tagging & Rollback을 통해 Iceberg Table을 관리 해 보자"}}},{"node":{"id":"dcd44de2-0eff-56f8-ac2d-2a99250ab9cf","excerpt":"안녕하세요? 박민재입니다. 오늘은 Iceberg Table을 관리하는 방법을 Metadata Table의 사용을 중심으로 깊게 알아 보도록 하겠습니다. Apache Iceberg의 경우에는 Metadata Table 기능을 매우 강력하게 지원합니다. 이를 통해 Iceberg Table을 운영을 쉽게 수행 할 수 있죠. 예를 들어, Table의 Evolution이 어떻게 진행 되었는지, 파일들이 어떻게 Partitioning…","frontmatter":{"date":"2024-12-05","tags":["Data-Engineering"],"path":"/data-engineering/iceberg-table-management-1","title":"Iceberg Table Management - 1. Metadata Table ","img":"/post_image/thumbnail/iceberg-table-management.png","summary":"Metadata Table을 통해 Iceberg Table을 관리 해 보자"}}},{"node":{"id":"cbb6e851-d864-5552-86d7-08c81b4a54cc","excerpt":"Intro 안녕하세요, 박민재입니다. 저번 시간에는 Table Optimization을 위한 압축 기법에 대해 배웠습니다. 이번 시간에는 압축을 제외한 Table Optimization 기법을 알아 보도록 하겠습니다. Partitioning 역시, 기존의 방법을 꺼낼 때가 왔습니다. 바로 Partitioning입니다. 동일한 Column의 동일한 Value를 가진 친구들은 같은 File로 묶어 주는 방식이죠. 어? 왜 Directory…","frontmatter":{"date":"2024-11-24","tags":["Data-Engineering"],"path":"/data-engineering/iceberg-table-optimization-2","title":"Iceberg Table의 성능 최적화 - 2. Partitioning, MOR, Others","img":"/post_image/thumbnail/iceberg-table-optimization-1.webp","summary":"File Merge를 통한 성능 최적화에 대해 알아보자."}}}]}}}}},"staticQueryHashes":["3819017183","63159454"],"slicesMap":{}}