{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/data-engineering/spark-performance-tuning-2/","result":{"data":{"markdownRemark":{"html":"<p>오늘은 <strong>Spark</strong> 성능 튜닝에 필요한, <code class=\"language-text\">cache()</code>와 <code class=\"language-text\">persist()</code> 에 대해서 알아 보도록 하겠습니다.</p>\n<p><strong>RDD</strong>는 <strong>Transformation</strong> (ex: <code class=\"language-text\">map()</code>, <code class=\"language-text\">filter()</code>, <code class=\"language-text\">distinct()</code> 등)을 이용 하여 <strong>새로운 RDD</strong>를 만들 수 있습니다. 하지만, <strong>Action</strong> (ex: <code class=\"language-text\">collection()</code>, <code class=\"language-text\">count()</code>, <code class=\"language-text\">foreach()</code> 등)이 호출 되기 전까지는, <strong>실제 연산</strong>을 수행 하지 않죠.</p>\n<p>다음 예제를 함께 봅시다!</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token comment\">// 데이터 프레임 생성</span>\n<span class=\"token keyword\">val</span> data <span class=\"token operator\">=</span> Seq<span class=\"token punctuation\">(</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Alice\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">25</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Bob\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Charlie\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">35</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"David\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">40</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">val</span> df <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>createDataFrame<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>toDF<span class=\"token punctuation\">(</span><span class=\"token string\">\"name\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"age\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\">// age 컬럼이 30보다 큰 행만 선택 하는 Transformation, 하지만 여기는 계산이 되지 않아요!</span>\n<span class=\"token keyword\">val</span> filteredDf <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>filter<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"age\"</span> <span class=\"token operator\">></span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\">// 여기는 Action 함수 이므로, 실제 계산이 수행 됩니다!</span>\nfilteredDf<span class=\"token punctuation\">.</span>count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>하지만, 우리가 다음과 같은 상황을 가정해 보죠. 만약, 동일하게 <strong>Transformation</strong> 된 <strong>RDD에</strong> 대해, <strong>여러 개의 Action</strong>을 수행 한다고 가정 해 봅시다!</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token keyword\">val</span> data <span class=\"token operator\">=</span> Seq<span class=\"token punctuation\">(</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Alice\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">25</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Bob\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Charlie\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">35</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"David\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">40</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">val</span> df <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>createDataFrame<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>toDF<span class=\"token punctuation\">(</span><span class=\"token string\">\"name\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"age\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">val</span> filteredDf <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>filter<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"age\"</span> <span class=\"token operator\">></span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">val</span> resultCount <span class=\"token operator\">=</span> filteredDf<span class=\"token punctuation\">.</span>count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// action</span>\n<span class=\"token keyword\">val</span> maxAge <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>agg<span class=\"token punctuation\">(</span>max<span class=\"token punctuation\">(</span><span class=\"token string\">\"Age\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>first<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>getInt<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// action</span></code></pre></div>\n<p>그러면 연산이 몇 번 발생할까요? <code class=\"language-text\">filter()</code> 연산을 통해, 새로운 RDD를 생성 하고, <code class=\"language-text\">count()</code>, <code class=\"language-text\">agg()</code> 를 호출하였으니 이렇게 연산이 되었을 것이라 예상 할 수 있습니다.</p>\n<ul>\n<li><code class=\"language-text\">filter()</code> -> <code class=\"language-text\">count()</code>, <code class=\"language-text\">agg()</code></li>\n</ul>\n<p>아니요, 틀렸습니다. <code class=\"language-text\">count()</code>, <code class=\"language-text\">agg()</code> 같은 액션을 호출 할 때마다, 모든 의존성을 <strong>재연산</strong> 하게 됩니다. 실제 연산은 다음과 같습니다.</p>\n<ul>\n<li><code class=\"language-text\">filter()</code> -> <code class=\"language-text\">count()</code></li>\n<li><code class=\"language-text\">filter()</code> -> <code class=\"language-text\">agg()</code></li>\n</ul>\n<p>그렇다면, 우리는 어떻게 하는 것이 좋을까요? 이럴 때, 메모리 혹은 디스크에 계속 <strong>DataFrame</strong>을 저장 할 수 있도록 만든 것이 <code class=\"language-text\">cache()</code> 입니다. 다음과 같이 연산 하게 되면, <strong>재연산</strong>을 막을 수가 있겠죠. 더 이상 캐시가 필요 하지 않을 때는, <code class=\"language-text\">unpersist()</code>를 통해 캐시를 release 하여야 합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token keyword\">val</span> data <span class=\"token operator\">=</span> Seq<span class=\"token punctuation\">(</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Alice\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">25</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Bob\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Charlie\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">35</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"David\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">40</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">val</span> df <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>createDataFrame<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>toDF<span class=\"token punctuation\">(</span><span class=\"token string\">\"name\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"age\"</span><span class=\"token punctuation\">)</span>\n\ndf<span class=\"token punctuation\">.</span>cache<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// cache를 통해 연산 결과를 메모리에 남기기! df와, filteredDf 모두가 저장 된다.</span>\n\n<span class=\"token keyword\">val</span> filteredDf <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>filter<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"age\"</span> <span class=\"token operator\">></span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">val</span> resultCount <span class=\"token operator\">=</span> filteredDf<span class=\"token punctuation\">.</span>count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// action</span>\n<span class=\"token keyword\">val</span> maxAge <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>agg<span class=\"token punctuation\">(</span>max<span class=\"token punctuation\">(</span><span class=\"token string\">\"Age\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>first<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>getInt<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// action</span>\n\ndf<span class=\"token punctuation\">.</span>unpersist<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token comment\">// cache release</span></code></pre></div>\n<p><code class=\"language-text\">cache()</code>는 <code class=\"language-text\">persist(storageLevel = MEMORY_AND_DISK)</code>와 똑같습니다. <code class=\"language-text\">persist()</code> 도, 연산이 끝난 데이터에 대해 영속성을 유지 하게 해 줍니다. 사용 방법은 <code class=\"language-text\">cache()</code>와 같지만, <strong>storageLevel</strong>을 기입 하여 주어야 합니다. 이는 다음과 같으며, <code class=\"language-text\">_SER</code> 이 붙어 있는 옵션은, 데이터를 <strong>Serialize (직렬화)</strong> 하여 저장 하는지에 대한 여부 입니다. (직렬화를 하면 <strong>역직렬화</strong>도 진행 하여야 하기 때문에, 메모리를 적게 사용하게 되나, CPU를 많이 사용 하게 됩니다.)</p>\n<p><strong>DISK</strong> (HDD, SDD)는 당연히, <strong>MEMORY</strong> (RAM) 보다 <strong>가져오는 속도가 느릴 것</strong>이니, 참고 하여야 합니다! (<strong>Spark</strong>는 <strong>in-memory</strong> 연산 입니다.)</p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">Storage Level    Space used  CPU time  In memory  On-disk  Serialized   Recompute some partitions\n----------------------------------------------------------------------------------------------------\nMEMORY_ONLY          High        Low       Y          N        N         Y    \nMEMORY_ONLY_SER      Low         High      Y          N        Y         Y\nMEMORY_AND_DISK      High        Medium    Some       Some     Some      N\nMEMORY_AND_DISK_SER  Low         High      Some       Some     Y         N\nDISK_ONLY            Low         High      N          Y        Y         N\n</code></pre></div>\n<p>(출처: <a href=\"https://sparkbyexamples.com/spark/spark-persistence-storage-levels/\">https://sparkbyexamples.com/spark/spark-persistence-storage-levels/</a>)</p>","id":"6466dd9e-f217-5113-909a-150f27b482f3","frontmatter":{"date":"2023-05-24","path":"/data-engineering/spark-performance-tuning-2","title":"Spark 성능 튜닝 - 2. cache(), persist(), unpersist()","tags":["Data-Engineering"],"keyword":"Spark, 성능 튜닝, spark cache, spark persist","summary":"Spark의 Performance 튜닝을 수행 해 보자.","img":"/post_image/thumbnail/spark-performance-tuning-2.jpeg","series":"Spark Performance Tuning"}}},"pageContext":{"postPath":"/data-engineering/spark-performance-tuning-2","series":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"e4d8d6bf-2f58-5ff8-b46f-c47daed191dd","excerpt":"오늘은 Spark 성능 튜닝에서 가장 중요한 SQL Tuning에 대해서 알아 보도록 하겠습니다. 사실 파라미터(Shuffle Partition 갯수, Executor Instance, Core, Memory 조정) 튜닝 또한, 도움이 될 수 있겠습니다만, 그 전에 Execution Plan이 잘 짜여져 있지 않다면, 파라미터 튜닝이 큰 영향을 주지는 못할 것 입니다. What is Execution Plan? Exection Plan…","frontmatter":{"date":"2023-08-22","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-3","title":"Spark 성능 튜닝 - 3. Spark SQL Tuning","img":"/post_image/thumbnail/spark-performance-tuning-3.jpeg","summary":"Spark의 쿼리를 튜닝 해 보자"}}},{"node":{"id":"6466dd9e-f217-5113-909a-150f27b482f3","excerpt":"오늘은 Spark 성능 튜닝에 필요한, 와  에 대해서 알아 보도록 하겠습니다. RDD는 Transformation (ex: , ,  등)을 이용 하여 새로운 RDD를 만들 수 있습니다. 하지만, Action (ex: , ,  등)이 호출 되기 전까지는, 실제 연산을 수행 하지 않죠. 다음 예제를 함께 봅시다! 하지만, 우리가 다음과 같은 상황을 가정해 보죠. 만약, 동일하게 Transformation 된 RDD에 대해, 여러 개의 Action…","frontmatter":{"date":"2023-05-24","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-2","title":"Spark 성능 튜닝 - 2. cache(), persist(), unpersist()","img":"/post_image/thumbnail/spark-performance-tuning-2.jpeg","summary":"Spark의 Performance 튜닝을 수행 해 보자."}}},{"node":{"id":"6ac732dc-2528-5e90-b1f8-2d142ca3af25","excerpt":"오늘은 Spark의 성능 튜닝에 대해서 이야기 해 보겠습니다. Spark는 요약해서 말하면, **in-memory(RAM 위에서)**에서 작동 하는 분산 컴퓨팅을 쉽게 지원해 주는 프레임워크 입니다. in-memory 연산은 빠르지만, 불안정 합니다. 메모리 관리, CPU Core 수의 관리를 통해 Out of memory가 발생 하지 않는 선에서, Job…","frontmatter":{"date":"2023-04-07","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-1","title":"Spark 성능 튜닝 - 1. Partition, Shuffle","img":"/post_image/thumbnail/spark-performance-tuning-1.png","summary":"Spark에 성능 튜닝을 시도 해 보자."}}}]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"cbb6e851-d864-5552-86d7-08c81b4a54cc","excerpt":"Intro 안녕하세요, 박민재입니다. 저번 시간에는 Table Optimization을 위한 압축 기법에 대해 배웠습니다. 이번 시간에는 압축을 제외한 Table Optimization 기법을 알아 보도록 하겠습니다. Partitioning 역시, 기존의 방법을 꺼낼 때가 왔습니다. 바로 Partitioning입니다. 동일한 Column의 동일한 Value를 가진 친구들은 같은 File로 묶어 주는 방식이죠. 어? 왜 Directory…","frontmatter":{"date":"2024-11-24","tags":["Data-Engineering"],"path":"/data-engineering/iceberg-table-optimization-2","title":"Iceberg Table의 성능 최적화 - 2. Partitioning, MOR, Others","img":"/post_image/thumbnail/iceberg-table-optimization-1.webp","summary":"File Merge를 통한 성능 최적화에 대해 알아보자."}}},{"node":{"id":"3c44d6b1-b341-5256-bb40-e6a58835b474","excerpt":"Intro 안녕하세요, 박민재입니다. 오늘은 Apache Iceberg의 Table에 수행 되는 쿼리가 최적의 성능으로 작동 될 수 있도록, File Compaction을 통해 이를 수행하는 방법에 대해 이야기 하는 시간을 가져 보도록 하겠습니다. File Compaction 우리가 쿼리를 수행 시, Hive Metastore의 정보를 이용하더라도, 혹은 Iceberg의 Metadata…","frontmatter":{"date":"2024-11-10","tags":["Data-Engineering"],"path":"/data-engineering/iceberg-table-optimization-1","title":"Iceberg Table의 성능 최적화 - 1. 압축","img":"/post_image/thumbnail/iceberg-table-optimization-1.webp","summary":"File Merge를 통한 성능 최적화에 대해 알아보자."}}},{"node":{"id":"6eaa7aeb-a4fe-5cd9-bbe5-309dde97514b","excerpt":"안녕하세요, 박민재입니다. 오늘은 Airflow DB를 관리하는 방법에 대해서 이야기 나눠 보도록 하겠습니다. Airflow Backend Database Airflow에서 Backend Database는 어떤 역할을 할까요? Airflow에서 DAG을 실행 하기 위해서, Airflow는 다음과 같은 정보들을 Backend Database에 저장하여 정합성을 유지 합니다. DagRun: 특정 Interval에 실행 된 DagRun…","frontmatter":{"date":"2024-10-27","tags":["Data-Engineering"],"path":"/data-engineering/airflow-db-management","title":"Airflow Backend Database Management (airflow db clean)","img":"/post_image/thumbnail/airflow-db-management.webp","summary":"Airflow의 Backend Database를 관리 하는 법"}}},{"node":{"id":"b595b168-8160-5abf-9641-39746d2f9e82","excerpt":"안녕하세요, 박민재입니다. 오늘은 Spark Application내의 각 Executor 내의 Task에 제한적으로 변수를 공유하는 두 가지 방법에 대해서 알아 보도록 하겠습니다. 시작하기에 앞서 단편적으로 생각 해 보면, Spark Application에서 연산 과정의 변수를 공유 한다는 것은 어려운 일인가? 라는 질문을 던져 볼 수 있습니다. 우리는 Task…","frontmatter":{"date":"2024-10-11","tags":["Data-Engineering"],"path":"/data-engineering/spark-sharing-variables","title":"Sharing Variables in Spark - Broadcast, Accumulator","img":"/post_image/thumbnail/spark-sharing-variables.png","summary":"Spark Application에서 변수를 공유 하는 방법"}}}]}}}}},"staticQueryHashes":["3819017183","63159454"],"slicesMap":{}}