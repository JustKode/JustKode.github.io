{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/data-engineering/spark-performance-tuning-2/","result":{"data":{"markdownRemark":{"html":"<p>오늘은 <strong>Spark</strong> 성능 튜닝에 필요한, <code class=\"language-text\">cache()</code>와 <code class=\"language-text\">persist()</code> 에 대해서 알아 보도록 하겠습니다.</p>\n<p><strong>RDD</strong>는 <strong>Transformation</strong> (ex: <code class=\"language-text\">map()</code>, <code class=\"language-text\">filter()</code>, <code class=\"language-text\">distinct()</code> 등)을 이용 하여 <strong>새로운 RDD</strong>를 만들 수 있습니다. 하지만, <strong>Action</strong> (ex: <code class=\"language-text\">collection()</code>, <code class=\"language-text\">count()</code>, <code class=\"language-text\">foreach()</code> 등)이 호출 되기 전까지는, <strong>실제 연산</strong>을 수행 하지 않죠.</p>\n<p>다음 예제를 함께 봅시다!</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token comment\">// 데이터 프레임 생성</span>\n<span class=\"token keyword\">val</span> data <span class=\"token operator\">=</span> Seq<span class=\"token punctuation\">(</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Alice\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">25</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Bob\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Charlie\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">35</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"David\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">40</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">val</span> df <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>createDataFrame<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>toDF<span class=\"token punctuation\">(</span><span class=\"token string\">\"name\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"age\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\">// age 컬럼이 30보다 큰 행만 선택 하는 Transformation, 하지만 여기는 계산이 되지 않아요!</span>\n<span class=\"token keyword\">val</span> filteredDf <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>filter<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"age\"</span> <span class=\"token operator\">></span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\">// 여기는 Action 함수 이므로, 실제 계산이 수행 됩니다!</span>\nfilteredDf<span class=\"token punctuation\">.</span>count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>하지만, 우리가 다음과 같은 상황을 가정해 보죠. 만약, 동일하게 <strong>Transformation</strong> 된 <strong>RDD에</strong> 대해, <strong>여러 개의 Action</strong>을 수행 한다고 가정 해 봅시다!</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token keyword\">val</span> data <span class=\"token operator\">=</span> Seq<span class=\"token punctuation\">(</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Alice\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">25</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Bob\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Charlie\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">35</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"David\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">40</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">val</span> df <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>createDataFrame<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>toDF<span class=\"token punctuation\">(</span><span class=\"token string\">\"name\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"age\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">val</span> filteredDf <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>filter<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"age\"</span> <span class=\"token operator\">></span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">val</span> resultCount <span class=\"token operator\">=</span> filteredDf<span class=\"token punctuation\">.</span>count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// action</span>\n<span class=\"token keyword\">val</span> maxAge <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>agg<span class=\"token punctuation\">(</span>max<span class=\"token punctuation\">(</span><span class=\"token string\">\"Age\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>first<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>getInt<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// action</span></code></pre></div>\n<p>그러면 연산이 몇 번 발생할까요? <code class=\"language-text\">filter()</code> 연산을 통해, 새로운 RDD를 생성 하고, <code class=\"language-text\">count()</code>, <code class=\"language-text\">agg()</code> 를 호출하였으니 이렇게 연산이 되었을 것이라 예상 할 수 있습니다.</p>\n<ul>\n<li><code class=\"language-text\">filter()</code> -> <code class=\"language-text\">count()</code>, <code class=\"language-text\">agg()</code></li>\n</ul>\n<p>아니요, 틀렸습니다. <code class=\"language-text\">count()</code>, <code class=\"language-text\">agg()</code> 같은 액션을 호출 할 때마다, 모든 의존성을 <strong>재연산</strong> 하게 됩니다. 실제 연산은 다음과 같습니다.</p>\n<ul>\n<li><code class=\"language-text\">filter()</code> -> <code class=\"language-text\">count()</code></li>\n<li><code class=\"language-text\">filter()</code> -> <code class=\"language-text\">agg()</code></li>\n</ul>\n<p>그렇다면, 우리는 어떻게 하는 것이 좋을까요? 이럴 때, 메모리 혹은 디스크에 계속 <strong>DataFrame</strong>을 저장 할 수 있도록 만든 것이 <code class=\"language-text\">cache()</code> 입니다. 다음과 같이 연산 하게 되면, <strong>재연산</strong>을 막을 수가 있겠죠. 더 이상 캐시가 필요 하지 않을 때는, <code class=\"language-text\">unpersist()</code>를 통해 캐시를 release 하여야 합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token keyword\">val</span> data <span class=\"token operator\">=</span> Seq<span class=\"token punctuation\">(</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Alice\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">25</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Bob\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Charlie\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">35</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"David\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">40</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">val</span> df <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>createDataFrame<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>toDF<span class=\"token punctuation\">(</span><span class=\"token string\">\"name\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"age\"</span><span class=\"token punctuation\">)</span>\n\ndf<span class=\"token punctuation\">.</span>cache<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// cache를 통해 연산 결과를 메모리에 남기기! df와, filteredDf 모두가 저장 된다.</span>\n\n<span class=\"token keyword\">val</span> filteredDf <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>filter<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"age\"</span> <span class=\"token operator\">></span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">val</span> resultCount <span class=\"token operator\">=</span> filteredDf<span class=\"token punctuation\">.</span>count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// action</span>\n<span class=\"token keyword\">val</span> maxAge <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>agg<span class=\"token punctuation\">(</span>max<span class=\"token punctuation\">(</span><span class=\"token string\">\"Age\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>first<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>getInt<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// action</span>\n\ndf<span class=\"token punctuation\">.</span>unpersist<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token comment\">// cache release</span></code></pre></div>\n<p><code class=\"language-text\">cache()</code>는 <code class=\"language-text\">persist(storageLevel = MEMORY_AND_DISK)</code>와 똑같습니다. <code class=\"language-text\">persist()</code> 도, 연산이 끝난 데이터에 대해 영속성을 유지 하게 해 줍니다. 사용 방법은 <code class=\"language-text\">cache()</code>와 같지만, <strong>storageLevel</strong>을 기입 하여 주어야 합니다. 이는 다음과 같으며, <code class=\"language-text\">_SER</code> 이 붙어 있는 옵션은, 데이터를 <strong>Serialize (직렬화)</strong> 하여 저장 하는지에 대한 여부 입니다. (직렬화를 하면 <strong>역직렬화</strong>도 진행 하여야 하기 때문에, 메모리를 적게 사용하게 되나, CPU를 많이 사용 하게 됩니다.)</p>\n<p><strong>DISK</strong> (HDD, SDD)는 당연히, <strong>MEMORY</strong> (RAM) 보다 <strong>가져오는 속도가 느릴 것</strong>이니, 참고 하여야 합니다! (<strong>Spark</strong>는 <strong>in-memory</strong> 연산 입니다.)</p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">Storage Level    Space used  CPU time  In memory  On-disk  Serialized   Recompute some partitions\n----------------------------------------------------------------------------------------------------\nMEMORY_ONLY          High        Low       Y          N        N         Y    \nMEMORY_ONLY_SER      Low         High      Y          N        Y         Y\nMEMORY_AND_DISK      High        Medium    Some       Some     Some      N\nMEMORY_AND_DISK_SER  Low         High      Some       Some     Y         N\nDISK_ONLY            Low         High      N          Y        Y         N\n</code></pre></div>\n<p>(출처: <a href=\"https://sparkbyexamples.com/spark/spark-persistence-storage-levels/\">https://sparkbyexamples.com/spark/spark-persistence-storage-levels/</a>)</p>","id":"6466dd9e-f217-5113-909a-150f27b482f3","frontmatter":{"date":"2023-05-24","path":"/data-engineering/spark-performance-tuning-2","title":"Spark 성능 튜닝 - 2. cache(), persist(), unpersist()","tags":["Data-Engineering"],"keyword":"Spark, 성능 튜닝, spark cache, spark persist","summary":"Spark의 Performance 튜닝을 수행 해 보자.","img":"/post_image/thumbnail/spark-performance-tuning-2.jpeg","series":"Spark Performance Tuning"}}},"pageContext":{"postPath":"/data-engineering/spark-performance-tuning-2","series":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"e4d8d6bf-2f58-5ff8-b46f-c47daed191dd","excerpt":"오늘은 Spark 성능 튜닝에서 가장 중요한 SQL Tuning에 대해서 알아 보도록 하겠습니다. 사실 파라미터(Shuffle Partition 갯수, Executor Instance, Core, Memory 조정) 튜닝 또한, 도움이 될 수 있겠습니다만, 그 전에 Execution Plan이 잘 짜여져 있지 않다면, 파라미터 튜닝이 큰 영향을 주지는 못할 것 입니다. What is Execution Plan? Exection Plan…","frontmatter":{"date":"2023-08-22","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-3","title":"Spark 성능 튜닝 - 3. Spark SQL Tuning","img":"/post_image/thumbnail/spark-performance-tuning-3.jpeg","summary":"Spark의 쿼리를 튜닝 해 보자"}}},{"node":{"id":"6466dd9e-f217-5113-909a-150f27b482f3","excerpt":"오늘은 Spark 성능 튜닝에 필요한, 와  에 대해서 알아 보도록 하겠습니다. RDD는 Transformation (ex: , ,  등)을 이용 하여 새로운 RDD를 만들 수 있습니다. 하지만, Action (ex: , ,  등)이 호출 되기 전까지는, 실제 연산을 수행 하지 않죠. 다음 예제를 함께 봅시다! 하지만, 우리가 다음과 같은 상황을 가정해 보죠. 만약, 동일하게 Transformation 된 RDD에 대해, 여러 개의 Action…","frontmatter":{"date":"2023-05-24","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-2","title":"Spark 성능 튜닝 - 2. cache(), persist(), unpersist()","img":"/post_image/thumbnail/spark-performance-tuning-2.jpeg","summary":"Spark의 Performance 튜닝을 수행 해 보자."}}},{"node":{"id":"6ac732dc-2528-5e90-b1f8-2d142ca3af25","excerpt":"오늘은 Spark의 성능 튜닝에 대해서 이야기 해 보겠습니다. Spark는 요약해서 말하면, **in-memory(RAM 위에서)**에서 작동 하는 분산 컴퓨팅을 쉽게 지원해 주는 프레임워크 입니다. in-memory 연산은 빠르지만, 불안정 합니다. 메모리 관리, CPU Core 수의 관리를 통해 Out of memory가 발생 하지 않는 선에서, Job…","frontmatter":{"date":"2023-04-07","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-1","title":"Spark 성능 튜닝 - 1. Partition, Shuffle","img":"/post_image/thumbnail/spark-performance-tuning-1.png","summary":"Spark에 성능 튜닝을 시도 해 보자."}}}]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"dec9e599-9444-556f-99ba-7a1dc27a4dbb","excerpt":"안녕하세요, 박민재입니다. 혹시 Data Discovery에 중요성을 느껴, DataHub를 사용하려고 하시는 분이 있나요? 아마 그렇다면 DataHub를 도입한 사례를 몇 개 읽어 보셨을꺼라 생각합니다. 대표적으로 국내 기업에서는 뱅크샐러드, 소카, 베이글코드 등에서 성공적으로 도입한 사례들을 회사 사이트에 올리는 경우를 확인 할 수 있었어요. SOCAR BankSalad BagelCode DataHub…","frontmatter":{"date":"2025-03-02","tags":["Data-Engineering"],"path":"/data-engineering/to-datahub-user","title":"DataHub 도입을 고려 하시는 분들에게","img":"/post_image/thumbnail/to-datahub-user.webp","summary":"DataHub를 도입 하려고 할 때 알아야 할 점"}}},{"node":{"id":"24718dd5-aa23-578b-aa81-0ca11fcc0f06","excerpt":"안녕하세요, 박민재입니다. 저번 시간에는 Spark Operator가 무엇인지 간단하게 알아 보았는데요, 이번 시간에는 실제로 Spark Operator Helm Chart를 설치하여, Spark Operator 관련 구동 준비를 한 후, Spark Operator 관련 Resource를 작성 하여 실제 Job을 제출 해 보는 시간을 가져 보도록 하겠습니다. Spark Operator Helm Chart Spark Operator…","frontmatter":{"date":"2025-02-02","tags":["Data-Engineering"],"path":"/data-engineering/spark-operator-2","title":"Spark Operator - 2. Practice","img":"/post_image/thumbnail/spark-operator.jpg","summary":"Spark Operator를 사용 해 보자."}}},{"node":{"id":"9ec3e659-5978-5311-b4d5-fc9d0902e008","excerpt":"안녕하세요, 박민재입니다. 아마 2년 전 즈음에 Spark on Kubernetes 관련 내용을 다뤘었는데요 (이 글 또한, 개정판을 작성 해 볼게요), 이번에는 Spark Job을 Kubernetes Cluster에 편리하게 제출할 수 있게 하는 Spark Operator에 대해 알아 보도록 하겠습니다. Spark on Kubernetes를 사용하는 이유? 그렇다고, 이 글에서 아예 설명 하지 않고 넘어가는 것은 아닌 것 같아, Spark…","frontmatter":{"date":"2025-01-19","tags":["Data-Engineering"],"path":"/data-engineering/spark-operator-1","title":"Spark Operator - 1. Spark Operator란?","img":"/post_image/thumbnail/spark-operator.jpg","summary":"Kubernetes Cluster로의 Spark Job 제출을 도와주는 Spark Operator가 무엇 인지 알아보자."}}},{"node":{"id":"2001438a-fe37-5b3c-8954-bce7d5e18a7a","excerpt":"안녕하세요? 박민재입니다. 오늘은 Iceberg Table을 관리하는 방법 중 하나인, Branching & Tagging 그리고 Rollback Action에 대해서 알아 보도록 하겠습니다. Isolation of Changes with Branches Iceberg에서는 git과 같은 방식으로 Branch를 만들어, 데이터 변경 사항을 관리 할 수 있습니다. 우리의 사례로 빗대어 보면 H/W 이슈, 혹은 Application…","frontmatter":{"date":"2025-01-03","tags":["Data-Engineering"],"path":"/data-engineering/iceberg-table-management-2","title":"Iceberg Table Management - 2. Branching, Tagging & Rollback","img":"/post_image/thumbnail/iceberg-table-management.png","summary":"Branching, Tagging & Rollback을 통해 Iceberg Table을 관리 해 보자"}}}]}}}}},"staticQueryHashes":["3819017183","63159454"],"slicesMap":{}}