{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/data-engineering/spark-performance-tuning-2/","result":{"data":{"markdownRemark":{"html":"<p>오늘은 <strong>Spark</strong> 성능 튜닝에 필요한, <code class=\"language-text\">cache()</code>와 <code class=\"language-text\">persist()</code> 에 대해서 알아 보도록 하겠습니다.</p>\n<p><strong>RDD</strong>는 <strong>Transformation</strong> (ex: <code class=\"language-text\">map()</code>, <code class=\"language-text\">filter()</code>, <code class=\"language-text\">distinct()</code> 등)을 이용 하여 <strong>새로운 RDD</strong>를 만들 수 있습니다. 하지만, <strong>Action</strong> (ex: <code class=\"language-text\">collection()</code>, <code class=\"language-text\">count()</code>, <code class=\"language-text\">foreach()</code> 등)이 호출 되기 전까지는, <strong>실제 연산</strong>을 수행 하지 않죠.</p>\n<p>다음 예제를 함께 봅시다!</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token comment\">// 데이터 프레임 생성</span>\n<span class=\"token keyword\">val</span> data <span class=\"token operator\">=</span> Seq<span class=\"token punctuation\">(</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Alice\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">25</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Bob\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Charlie\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">35</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"David\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">40</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">val</span> df <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>createDataFrame<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>toDF<span class=\"token punctuation\">(</span><span class=\"token string\">\"name\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"age\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\">// age 컬럼이 30보다 큰 행만 선택 하는 Transformation, 하지만 여기는 계산이 되지 않아요!</span>\n<span class=\"token keyword\">val</span> filteredDf <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>filter<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"age\"</span> <span class=\"token operator\">></span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\">// 여기는 Action 함수 이므로, 실제 계산이 수행 됩니다!</span>\nfilteredDf<span class=\"token punctuation\">.</span>count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>하지만, 우리가 다음과 같은 상황을 가정해 보죠. 만약, 동일하게 <strong>Transformation</strong> 된 <strong>RDD에</strong> 대해, <strong>여러 개의 Action</strong>을 수행 한다고 가정 해 봅시다!</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token keyword\">val</span> data <span class=\"token operator\">=</span> Seq<span class=\"token punctuation\">(</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Alice\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">25</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Bob\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Charlie\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">35</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"David\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">40</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">val</span> df <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>createDataFrame<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>toDF<span class=\"token punctuation\">(</span><span class=\"token string\">\"name\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"age\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">val</span> filteredDf <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>filter<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"age\"</span> <span class=\"token operator\">></span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">val</span> resultCount <span class=\"token operator\">=</span> filteredDf<span class=\"token punctuation\">.</span>count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// action</span>\n<span class=\"token keyword\">val</span> maxAge <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>agg<span class=\"token punctuation\">(</span>max<span class=\"token punctuation\">(</span><span class=\"token string\">\"Age\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>first<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>getInt<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// action</span></code></pre></div>\n<p>그러면 연산이 몇 번 발생할까요? <code class=\"language-text\">filter()</code> 연산을 통해, 새로운 RDD를 생성 하고, <code class=\"language-text\">count()</code>, <code class=\"language-text\">agg()</code> 를 호출하였으니 이렇게 연산이 되었을 것이라 예상 할 수 있습니다.</p>\n<ul>\n<li><code class=\"language-text\">filter()</code> -> <code class=\"language-text\">count()</code>, <code class=\"language-text\">agg()</code></li>\n</ul>\n<p>아니요, 틀렸습니다. <code class=\"language-text\">count()</code>, <code class=\"language-text\">agg()</code> 같은 액션을 호출 할 때마다, 모든 의존성을 <strong>재연산</strong> 하게 됩니다. 실제 연산은 다음과 같습니다.</p>\n<ul>\n<li><code class=\"language-text\">filter()</code> -> <code class=\"language-text\">count()</code></li>\n<li><code class=\"language-text\">filter()</code> -> <code class=\"language-text\">agg()</code></li>\n</ul>\n<p>그렇다면, 우리는 어떻게 하는 것이 좋을까요? 이럴 때, 메모리 혹은 디스크에 계속 <strong>DataFrame</strong>을 저장 할 수 있도록 만든 것이 <code class=\"language-text\">cache()</code> 입니다. 다음과 같이 연산 하게 되면, <strong>재연산</strong>을 막을 수가 있겠죠. 더 이상 캐시가 필요 하지 않을 때는, <code class=\"language-text\">unpersist()</code>를 통해 캐시를 release 하여야 합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token keyword\">val</span> data <span class=\"token operator\">=</span> Seq<span class=\"token punctuation\">(</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Alice\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">25</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Bob\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Charlie\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">35</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"David\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">40</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">val</span> df <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>createDataFrame<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>toDF<span class=\"token punctuation\">(</span><span class=\"token string\">\"name\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"age\"</span><span class=\"token punctuation\">)</span>\n\ndf<span class=\"token punctuation\">.</span>cache<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// cache를 통해 연산 결과를 메모리에 남기기! df와, filteredDf 모두가 저장 된다.</span>\n\n<span class=\"token keyword\">val</span> filteredDf <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>filter<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"age\"</span> <span class=\"token operator\">></span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">val</span> resultCount <span class=\"token operator\">=</span> filteredDf<span class=\"token punctuation\">.</span>count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// action</span>\n<span class=\"token keyword\">val</span> maxAge <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>agg<span class=\"token punctuation\">(</span>max<span class=\"token punctuation\">(</span><span class=\"token string\">\"Age\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>first<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>getInt<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// action</span>\n\ndf<span class=\"token punctuation\">.</span>unpersist<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token comment\">// cache release</span></code></pre></div>\n<p><code class=\"language-text\">cache()</code>는 <code class=\"language-text\">persist(storageLevel = MEMORY_AND_DISK)</code>와 똑같습니다. <code class=\"language-text\">persist()</code> 도, 연산이 끝난 데이터에 대해 영속성을 유지 하게 해 줍니다. 사용 방법은 <code class=\"language-text\">cache()</code>와 같지만, <strong>storageLevel</strong>을 기입 하여 주어야 합니다. 이는 다음과 같으며, <code class=\"language-text\">_SER</code> 이 붙어 있는 옵션은, 데이터를 <strong>Serialize (직렬화)</strong> 하여 저장 하는지에 대한 여부 입니다. (직렬화를 하면 <strong>역직렬화</strong>도 진행 하여야 하기 때문에, 메모리를 적게 사용하게 되나, CPU를 많이 사용 하게 됩니다.)</p>\n<p><strong>DISK</strong> (HDD, SDD)는 당연히, <strong>MEMORY</strong> (RAM) 보다 <strong>가져오는 속도가 느릴 것</strong>이니, 참고 하여야 합니다! (<strong>Spark</strong>는 <strong>in-memory</strong> 연산 입니다.)</p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">Storage Level    Space used  CPU time  In memory  On-disk  Serialized   Recompute some partitions\n----------------------------------------------------------------------------------------------------\nMEMORY_ONLY          High        Low       Y          N        N         Y    \nMEMORY_ONLY_SER      Low         High      Y          N        Y         Y\nMEMORY_AND_DISK      High        Medium    Some       Some     Some      N\nMEMORY_AND_DISK_SER  Low         High      Some       Some     Y         N\nDISK_ONLY            Low         High      N          Y        Y         N\n</code></pre></div>\n<p>(출처: <a href=\"https://sparkbyexamples.com/spark/spark-persistence-storage-levels/\">https://sparkbyexamples.com/spark/spark-persistence-storage-levels/</a>)</p>","id":"6466dd9e-f217-5113-909a-150f27b482f3","frontmatter":{"date":"2023-05-24","path":"/data-engineering/spark-performance-tuning-2","title":"Spark 성능 튜닝 - 2. cache(), persist(), unpersist()","tags":["Data-Engineering"],"keyword":"Spark, 성능 튜닝, spark cache, spark persist","summary":"Spark의 Performance 튜닝을 수행 해 보자.","img":"/post_image/thumbnail/spark-performance-tuning-2.jpeg","series":"Spark Performance Tuning"}}},"pageContext":{"postPath":"/data-engineering/spark-performance-tuning-2","series":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"e4d8d6bf-2f58-5ff8-b46f-c47daed191dd","excerpt":"오늘은 Spark 성능 튜닝에서 가장 중요한 SQL Tuning에 대해서 알아 보도록 하겠습니다. 사실 파라미터(Shuffle Partition 갯수, Executor Instance, Core, Memory 조정) 튜닝 또한, 도움이 될 수 있겠습니다만, 그 전에 Execution Plan이 잘 짜여져 있지 않다면, 파라미터 튜닝이 큰 영향을 주지는 못할 것 입니다. What is Execution Plan? Exection Plan…","frontmatter":{"date":"2023-08-22","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-3","title":"Spark 성능 튜닝 - 3. Spark SQL Tuning","img":"/post_image/thumbnail/spark-performance-tuning-3.jpeg","summary":"Spark의 쿼리를 튜닝 해 보자"}}},{"node":{"id":"6466dd9e-f217-5113-909a-150f27b482f3","excerpt":"오늘은 Spark 성능 튜닝에 필요한, 와  에 대해서 알아 보도록 하겠습니다. RDD는 Transformation (ex: , ,  등)을 이용 하여 새로운 RDD를 만들 수 있습니다. 하지만, Action (ex: , ,  등)이 호출 되기 전까지는, 실제 연산을 수행 하지 않죠. 다음 예제를 함께 봅시다! 하지만, 우리가 다음과 같은 상황을 가정해 보죠. 만약, 동일하게 Transformation 된 RDD에 대해, 여러 개의 Action…","frontmatter":{"date":"2023-05-24","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-2","title":"Spark 성능 튜닝 - 2. cache(), persist(), unpersist()","img":"/post_image/thumbnail/spark-performance-tuning-2.jpeg","summary":"Spark의 Performance 튜닝을 수행 해 보자."}}},{"node":{"id":"6ac732dc-2528-5e90-b1f8-2d142ca3af25","excerpt":"오늘은 Spark의 성능 튜닝에 대해서 이야기 해 보겠습니다. Spark는 요약해서 말하면, **in-memory(RAM 위에서)**에서 작동 하는 분산 컴퓨팅을 쉽게 지원해 주는 프레임워크 입니다. in-memory 연산은 빠르지만, 불안정 합니다. 메모리 관리, CPU Core 수의 관리를 통해 Out of memory가 발생 하지 않는 선에서, Job…","frontmatter":{"date":"2023-04-07","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-1","title":"Spark 성능 튜닝 - 1. Partition, Shuffle","img":"/post_image/thumbnail/spark-performance-tuning-1.png","summary":"Spark에 성능 튜닝을 시도 해 보자."}}}]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"6eaa7aeb-a4fe-5cd9-bbe5-309dde97514b","excerpt":"안녕하세요, 박민재입니다. 오늘은 Airflow DB를 관리하는 방법에 대해서 이야기 나눠 보도록 하겠습니다. Airflow Backend Database Airflow에서 Backend Database는 어떤 역할을 할까요? Airflow에서 DAG을 실행 하기 위해서, Airflow는 다음과 같은 정보들을 Backend Database에 저장하여 정합성을 유지 합니다. DagRun: 특정 Interval에 실행 된 DagRun…","frontmatter":{"date":"2024-10-27","tags":["Data-Engineering"],"path":"/data-engineering/airflow-db-management","title":"Airflow Backend Database Management (airflow db clean)","img":"/post_image/thumbnail/airflow-db-management.webp","summary":"Airflow의 Backend Database를 관리 하는 법"}}},{"node":{"id":"b595b168-8160-5abf-9641-39746d2f9e82","excerpt":"안녕하세요, 박민재입니다. 오늘은 Spark Application내의 각 Executor 내의 Task에 제한적으로 변수를 공유하는 두 가지 방법에 대해서 알아 보도록 하겠습니다. 시작하기에 앞서 단편적으로 생각 해 보면, Spark Application에서 연산 과정의 변수를 공유 한다는 것은 어려운 일인가? 라는 질문을 던져 볼 수 있습니다. 우리는 Task…","frontmatter":{"date":"2024-10-11","tags":["Data-Engineering"],"path":"/data-engineering/spark-sharing-variables","title":"Sharing Variables in Spark - Broadcast, Accumulator","img":"/post_image/thumbnail/spark-sharing-variables.png","summary":"Spark Application에서 변수를 공유 하는 방법"}}},{"node":{"id":"5effd6a1-bfd8-5a86-b718-5eb1e534601e","excerpt":"Intro 안녕하세요, 박민재입니다. 오늘은 Spark Memory에 관해 Deep Dive를 해 보도록 하겠습니다. Spark는 In-Memory를 이용하여, 빠른 연산을 할 수 있도록 보장합니다. 하지만, In-Memory 연산은 빠른 대신, 비싼 관계로 적은 리소스 만을 활용할 수 있습니다. 그렇기 때문에 우리는 효율적으로 Memory를 관리 하여, Spark Application이 빠르고, 안정적으로 Task…","frontmatter":{"date":"2024-04-12","tags":["Data-Engineering"],"path":"/data-engineering/spark-memory-deep-dive","title":"Deep Dive of Spark Memory","img":"/post_image/thumbnail/spark-memory-deep-dive.jpeg","summary":"Spark Memory의 깊은 이해를 위해 Deep Dive를 해 보자."}}},{"node":{"id":"9a77a751-f1a2-59c2-9065-808620c51bbe","excerpt":"안녕하세요? 박민재입니다. 오늘은 Data Discovery에 대해서 알아 보도록 하겠습니다. What is Data Discovery? Data Discovery란 무엇 일까요? Data Discovery는 조직 내 데이터를 찾고 이해하는 프로세스를 의미 합니다. Data Engineer들은 요구사항을 수행하기 위해, 많은 Data Source에서 다양한 Data Table…","frontmatter":{"date":"2024-03-17","tags":["Data-Engineering"],"path":"/data-engineering/what-is-data-discovery","title":"Data Discovery란 무엇인가?","img":"/post_image/thumbnail/what-is-data-discovery.jpeg","summary":"데이터를 찾고 이해하는 프로세스인, Data Discovery에 대하여"}}}]}}}}},"staticQueryHashes":["3819017183","63159454"],"slicesMap":{}}