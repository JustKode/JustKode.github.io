{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/data-engineering/spark-performance-tuning-2/","result":{"data":{"markdownRemark":{"html":"<p>오늘은 <strong>Spark</strong> 성능 튜닝에 필요한, <code class=\"language-text\">cache()</code>와 <code class=\"language-text\">persist()</code> 에 대해서 알아 보도록 하겠습니다.</p>\n<p><strong>RDD</strong>는 <strong>Transformation</strong> (ex: <code class=\"language-text\">map()</code>, <code class=\"language-text\">filter()</code>, <code class=\"language-text\">distinct()</code> 등)을 이용 하여 <strong>새로운 RDD</strong>를 만들 수 있습니다. 하지만, <strong>Action</strong> (ex: <code class=\"language-text\">collection()</code>, <code class=\"language-text\">count()</code>, <code class=\"language-text\">foreach()</code> 등)이 호출 되기 전까지는, <strong>실제 연산</strong>을 수행 하지 않죠.</p>\n<p>다음 예제를 함께 봅시다!</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token comment\">// 데이터 프레임 생성</span>\n<span class=\"token keyword\">val</span> data <span class=\"token operator\">=</span> Seq<span class=\"token punctuation\">(</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Alice\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">25</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Bob\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Charlie\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">35</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"David\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">40</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">val</span> df <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>createDataFrame<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>toDF<span class=\"token punctuation\">(</span><span class=\"token string\">\"name\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"age\"</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\">// age 컬럼이 30보다 큰 행만 선택 하는 Transformation, 하지만 여기는 계산이 되지 않아요!</span>\n<span class=\"token keyword\">val</span> filteredDf <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>filter<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"age\"</span> <span class=\"token operator\">></span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\">// 여기는 Action 함수 이므로, 실제 계산이 수행 됩니다!</span>\nfilteredDf<span class=\"token punctuation\">.</span>count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>하지만, 우리가 다음과 같은 상황을 가정해 보죠. 만약, 동일하게 <strong>Transformation</strong> 된 <strong>RDD에</strong> 대해, <strong>여러 개의 Action</strong>을 수행 한다고 가정 해 봅시다!</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token keyword\">val</span> data <span class=\"token operator\">=</span> Seq<span class=\"token punctuation\">(</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Alice\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">25</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Bob\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Charlie\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">35</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"David\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">40</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">val</span> df <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>createDataFrame<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>toDF<span class=\"token punctuation\">(</span><span class=\"token string\">\"name\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"age\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">val</span> filteredDf <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>filter<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"age\"</span> <span class=\"token operator\">></span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">val</span> resultCount <span class=\"token operator\">=</span> filteredDf<span class=\"token punctuation\">.</span>count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// action</span>\n<span class=\"token keyword\">val</span> maxAge <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>agg<span class=\"token punctuation\">(</span>max<span class=\"token punctuation\">(</span><span class=\"token string\">\"Age\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>first<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>getInt<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// action</span></code></pre></div>\n<p>그러면 연산이 몇 번 발생할까요? <code class=\"language-text\">filter()</code> 연산을 통해, 새로운 RDD를 생성 하고, <code class=\"language-text\">count()</code>, <code class=\"language-text\">agg()</code> 를 호출하였으니 이렇게 연산이 되었을 것이라 예상 할 수 있습니다.</p>\n<ul>\n<li><code class=\"language-text\">filter()</code> -> <code class=\"language-text\">count()</code>, <code class=\"language-text\">agg()</code></li>\n</ul>\n<p>아니요, 틀렸습니다. <code class=\"language-text\">count()</code>, <code class=\"language-text\">agg()</code> 같은 액션을 호출 할 때마다, 모든 의존성을 <strong>재연산</strong> 하게 됩니다. 실제 연산은 다음과 같습니다.</p>\n<ul>\n<li><code class=\"language-text\">filter()</code> -> <code class=\"language-text\">count()</code></li>\n<li><code class=\"language-text\">filter()</code> -> <code class=\"language-text\">agg()</code></li>\n</ul>\n<p>그렇다면, 우리는 어떻게 하는 것이 좋을까요? 이럴 때, 메모리 혹은 디스크에 계속 <strong>DataFrame</strong>을 저장 할 수 있도록 만든 것이 <code class=\"language-text\">cache()</code> 입니다. 다음과 같이 연산 하게 되면, <strong>재연산</strong>을 막을 수가 있겠죠. 더 이상 캐시가 필요 하지 않을 때는, <code class=\"language-text\">unpersist()</code>를 통해 캐시를 release 하여야 합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\"><span class=\"token keyword\">val</span> data <span class=\"token operator\">=</span> Seq<span class=\"token punctuation\">(</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Alice\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">25</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Bob\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"Charlie\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">35</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  <span class=\"token punctuation\">(</span><span class=\"token string\">\"David\"</span><span class=\"token punctuation\">,</span> <span class=\"token number\">40</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">val</span> df <span class=\"token operator\">=</span> spark<span class=\"token punctuation\">.</span>createDataFrame<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>toDF<span class=\"token punctuation\">(</span><span class=\"token string\">\"name\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"age\"</span><span class=\"token punctuation\">)</span>\n\ndf<span class=\"token punctuation\">.</span>cache<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// cache를 통해 연산 결과를 메모리에 남기기! df와, filteredDf 모두가 저장 된다.</span>\n\n<span class=\"token keyword\">val</span> filteredDf <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>filter<span class=\"token punctuation\">(</span>$<span class=\"token string\">\"age\"</span> <span class=\"token operator\">></span> <span class=\"token number\">30</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">val</span> resultCount <span class=\"token operator\">=</span> filteredDf<span class=\"token punctuation\">.</span>count<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// action</span>\n<span class=\"token keyword\">val</span> maxAge <span class=\"token operator\">=</span> df<span class=\"token punctuation\">.</span>agg<span class=\"token punctuation\">(</span>max<span class=\"token punctuation\">(</span><span class=\"token string\">\"Age\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>first<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>getInt<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\">// action</span>\n\ndf<span class=\"token punctuation\">.</span>unpersist<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token comment\">// cache release</span></code></pre></div>\n<p><code class=\"language-text\">cache()</code>는 <code class=\"language-text\">persist(storageLevel = MEMORY_AND_DISK)</code>와 똑같습니다. <code class=\"language-text\">persist()</code> 도, 연산이 끝난 데이터에 대해 영속성을 유지 하게 해 줍니다. 사용 방법은 <code class=\"language-text\">cache()</code>와 같지만, <strong>storageLevel</strong>을 기입 하여 주어야 합니다. 이는 다음과 같으며, <code class=\"language-text\">_SER</code> 이 붙어 있는 옵션은, 데이터를 <strong>Serialize (직렬화)</strong> 하여 저장 하는지에 대한 여부 입니다. (직렬화를 하면 <strong>역직렬화</strong>도 진행 하여야 하기 때문에, 메모리를 적게 사용하게 되나, CPU를 많이 사용 하게 됩니다.)</p>\n<p><strong>DISK</strong> (HDD, SDD)는 당연히, <strong>MEMORY</strong> (RAM) 보다 <strong>가져오는 속도가 느릴 것</strong>이니, 참고 하여야 합니다! (<strong>Spark</strong>는 <strong>in-memory</strong> 연산 입니다.)</p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">Storage Level    Space used  CPU time  In memory  On-disk  Serialized   Recompute some partitions\n----------------------------------------------------------------------------------------------------\nMEMORY_ONLY          High        Low       Y          N        N         Y    \nMEMORY_ONLY_SER      Low         High      Y          N        Y         Y\nMEMORY_AND_DISK      High        Medium    Some       Some     Some      N\nMEMORY_AND_DISK_SER  Low         High      Some       Some     Y         N\nDISK_ONLY            Low         High      N          Y        Y         N\n</code></pre></div>\n<p>(출처: <a href=\"https://sparkbyexamples.com/spark/spark-persistence-storage-levels/\">https://sparkbyexamples.com/spark/spark-persistence-storage-levels/</a>)</p>","id":"2f3abeaf-1ba1-5817-8329-0ef461b07b9c","frontmatter":{"date":"2023-05-24","path":"/data-engineering/spark-performance-tuning-2","title":"Spark 성능 튜닝 - 2. cache(), persist(), unpersist()","tags":["Data-Engineering"],"keyword":"Spark, 성능 튜닝, spark cache, spark persist","summary":"Spark의 Performance 튜닝을 수행 해 보자.","img":"/post_image/thumbnail/spark-performance-tuning-2.jpeg","series":"Spark Performance Tuning"}}},"pageContext":{"postPath":"/data-engineering/spark-performance-tuning-2","series":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"f2d9685d-9388-518e-b7b2-c03a527f3c85","excerpt":"오늘은 Spark 성능 튜닝에서 가장 중요한 SQL Tuning에 대해서 알아 보도록 하겠습니다. 사실 파라미터(Shuffle Partition 갯수, Executor Instance, Core, Memory 조정) 튜닝 또한, 도움이 될 수 있겠습니다만, 그 전에 Execution Plan이 잘 짜여져 있지 않다면, 파라미터 튜닝이 큰 영향을 주지는 못할 것 입니다. What is Execution Plan? Exection Plan…","frontmatter":{"date":"2023-08-22","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-3","title":"Spark 성능 튜닝 - 3. Spark SQL Tuning","img":"/post_image/thumbnail/spark-performance-tuning-3.jpeg","summary":"Spark의 쿼리를 튜닝 해 보자"}}},{"node":{"id":"2f3abeaf-1ba1-5817-8329-0ef461b07b9c","excerpt":"오늘은 Spark 성능 튜닝에 필요한, 와  에 대해서 알아 보도록 하겠습니다. RDD는 Transformation (ex: , ,  등)을 이용 하여 새로운 RDD를 만들 수 있습니다. 하지만, Action (ex: , ,  등)이 호출 되기 전까지는, 실제 연산을 수행 하지 않죠. 다음 예제를 함께 봅시다! 하지만, 우리가 다음과 같은 상황을 가정해 보죠. 만약, 동일하게 Transformation 된 RDD에 대해, 여러 개의 Action…","frontmatter":{"date":"2023-05-24","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-2","title":"Spark 성능 튜닝 - 2. cache(), persist(), unpersist()","img":"/post_image/thumbnail/spark-performance-tuning-2.jpeg","summary":"Spark의 Performance 튜닝을 수행 해 보자."}}},{"node":{"id":"fb3c924f-4a7c-50c2-8404-09a28f1e98ec","excerpt":"오늘은 Spark의 성능 튜닝에 대해서 이야기 해 보겠습니다. Spark는 요약해서 말하면, **in-memory(RAM 위에서)**에서 작동 하는 분산 컴퓨팅을 쉽게 지원해 주는 프레임워크 입니다. in-memory 연산은 빠르지만, 불안정 합니다. 메모리 관리, CPU Core 수의 관리를 통해 Out of memory가 발생 하지 않는 선에서, Job…","frontmatter":{"date":"2023-04-07","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-1","title":"Spark 성능 튜닝 - 1. Partition, Shuffle","img":"/post_image/thumbnail/spark-performance-tuning-1.png","summary":"Spark에 성능 튜닝을 시도 해 보자."}}}]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"5a1c922e-fff4-5443-87e1-15282d06c381","excerpt":"Summary Data Engineering이 구체적으로 무엇인지 알아 봅니다. 왜 Data Engineering이 필요 하게 되었는지 알아 봅니다. Data Engineer는 팀 혹은 회사에서 어떤 역할을 수행 하는지 알아 봅니다. 머릿말 안녕하세요? JustKode, 박민재입니다. 오늘은 Data Engineering이 구체적으로 무엇 인지, Data Engineer…","frontmatter":{"date":"2023-12-05","tags":["Data-Engineering"],"path":"/data-engineering/data-engineering-intro","title":"Data Engineering이란?","img":"/post_image/data-engineering-intro.png","summary":"Data Engineering에 대하여"}}},{"node":{"id":"f2d9685d-9388-518e-b7b2-c03a527f3c85","excerpt":"오늘은 Spark 성능 튜닝에서 가장 중요한 SQL Tuning에 대해서 알아 보도록 하겠습니다. 사실 파라미터(Shuffle Partition 갯수, Executor Instance, Core, Memory 조정) 튜닝 또한, 도움이 될 수 있겠습니다만, 그 전에 Execution Plan이 잘 짜여져 있지 않다면, 파라미터 튜닝이 큰 영향을 주지는 못할 것 입니다. What is Execution Plan? Exection Plan…","frontmatter":{"date":"2023-08-22","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-3","title":"Spark 성능 튜닝 - 3. Spark SQL Tuning","img":"/post_image/thumbnail/spark-performance-tuning-3.jpeg","summary":"Spark의 쿼리를 튜닝 해 보자"}}},{"node":{"id":"2f3abeaf-1ba1-5817-8329-0ef461b07b9c","excerpt":"오늘은 Spark 성능 튜닝에 필요한, 와  에 대해서 알아 보도록 하겠습니다. RDD는 Transformation (ex: , ,  등)을 이용 하여 새로운 RDD를 만들 수 있습니다. 하지만, Action (ex: , ,  등)이 호출 되기 전까지는, 실제 연산을 수행 하지 않죠. 다음 예제를 함께 봅시다! 하지만, 우리가 다음과 같은 상황을 가정해 보죠. 만약, 동일하게 Transformation 된 RDD에 대해, 여러 개의 Action…","frontmatter":{"date":"2023-05-24","tags":["Data-Engineering"],"path":"/data-engineering/spark-performance-tuning-2","title":"Spark 성능 튜닝 - 2. cache(), persist(), unpersist()","img":"/post_image/thumbnail/spark-performance-tuning-2.jpeg","summary":"Spark의 Performance 튜닝을 수행 해 보자."}}},{"node":{"id":"9dbd0b47-b274-5813-960b-805e0f92e2d0","excerpt":"안녕하세요? 오늘은 HDFS의 Architecture에 대해서 알아 보도록 하겠습니다. Hadoop Distributed File System(HDFS) 는 상용 하드웨어에서 동작하게 만든 오픈소스 SW입니다. 장애 발생에 강하며, 저비용 하드웨어 안에서도, 잘 작동 하게 설계 되었습니다. 또한, 많은 데이터 셋을 지닌 어플리케이션에 적합하며, 높은 throughput을 가지고 있습니다. Assumptions and Goals HDFS…","frontmatter":{"date":"2023-05-16","tags":["Data-Engineering"],"path":"/data-engineering/hdfs-architecture","title":"HDFS Architectrue","img":"/post_image/thumbnail/hdfs-architecture.png","summary":"HDFS의 구조에 대해서 알아보자."}}}]}}}}},"staticQueryHashes":["3819017183","63159454"],"slicesMap":{}}