{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/data-engineering/great-expectations/","result":{"data":{"markdownRemark":{"html":"<h3>Intro</h3>\n<p>안녕하세요! 박민재, JustKode 입니다. 오늘은 <strong>Data Quality Tool</strong>인 <strong>Great Expectations</strong>에 대해서 알아 볼 것 입니다.</p>\n<p><strong>Data Quality</strong>란 무엇일까요? 한국어로 직역하면 <strong>데이터 품질</strong>입니다. 우리가 많은 양의 데이터를 다루게 될 때, 우리는 <strong>기대하지 않은 값이 들어오는</strong> 다양한 <strong>데이터 품질 이슈</strong>를 겪게 됩니다. 데이터의 중복, 누락된 데이터 등, 데이터를 다루시는 분들은 이런 경험들을 많이 하게 되는데요. 만약 데이터 사용처에서 잘못 된 데이터를 사용하여 문제가 발생 한다면 어떻게 될까요? ML 쪽이라면 고객에게 잘못 학습된 모델로 서비스가 제공 될 것이며, 분석 쪽이라면 잘못된 의사결정을, 사용자에게 요금을 청구 하는 쪽과 연결되었다면... 일은 더 커질 것 입니다. 그렇기 때문에, 잘못된 데이터를 사용 하기 전, 우리는 <strong>데이터의 품질</strong>을 확인 할 필요가 있는 것이지요.</p>\n<p>그렇기 때문에, 우리는 수집된 데이터가 <strong>예상된 결과를 반환</strong>하는지 확인 할 수 있는 툴이 필요 하게 되는데요, 이를 위해서 우리는 <strong>Great Expectations</strong>를 사용 할 것입니다.</p>\n<h3>What is Great Expectations?</h3>\n<p><strong>Great Expectations</strong>는 <strong>Python</strong>을 기반으로 한 <strong>Data Quality Open Source Framework</strong> 입니다. 다양한 데이터 소스를 사용 할 수 있으며 (S3, FileSystem, RDBMS 등) Pandas, Spark 등의 API를 이용할 수도 있습니다. 제공하는 핵심 기능을 요약 하면 다음과 같습니다.</p>\n<ul>\n<li>Data Validation: Expectation을 생성 하여, 특정 데이터 Batch 단위에 대해 <strong>원하는 결과가 반환 되는지</strong> 검증 합니다. (ex. null 여부, unique 여부, 문자열 길이, 값의 범위 등)</li>\n<li>Data Profiling: Data의 <strong>각 Column에 대한 통계</strong> 등을 반환 해 줍니다. (ex. 최대값, 최솟값, 중간값 등)</li>\n<li>Data Docs: <strong>Data Validation, Data Profiling</strong>에 대한 결과를 <strong>HTML Document로 반환</strong> 하여 줍니다.</li>\n</ul>\n<h3>Core Concepts of Great Expectations</h3>\n<p><strong>Great Expectations</strong>를 사용 하기 위해서 이해 해야 할 <strong>Core Concepts</strong>들이 있습니다. Core Concepts는 다음과 같습니다.</p>\n<ul>\n<li><strong>Data Context</strong>: Great Expectations Project를 실행 하기 위한 <strong>메타 데이터 정보</strong>들이 포함 되어 있습니다.</li>\n<li><strong>Data Sources</strong>: Great Expectations Project와 <strong>실제 데이터를 연결</strong> 해 주는 컴포넌트입니다.</li>\n<li><strong>Checkpoints</strong>: Checkpoint는 Great Expectations에서 <strong>데이터를 검증</strong>하는 추상적인 계층입니다.</li>\n</ul>\n<h3>Data Context</h3>\n<p><strong>Data Context</strong>는 <strong>Great Expectations (이하 GX) Project</strong>의 파일 접근 등에 필요한 <strong>메타 데이터</strong>와 <strong>설정 값</strong> 등을 가지고 있습니다.</p>\n<p><strong>GX Project Python API</strong>의 <strong>Entrypoint 역할</strong>을 수행 하며, 대부분의 작업이 Data Context의 <strong>메타 데이터를 이용</strong>하여 수행 됩니다.</p>\n<p><strong>Data Context</strong>에는, 추후 설명할 <strong>Data Docs</strong>의 접근에 필요 한 정보들과 <strong>Stores</strong>의 접근에 필요 한 정보들을 지정해 줄 수 있습니다.</p>\n<p align=\"center\">\n    <img src=\"/post_image/great-expectations/01-01.png\" width=\"50%\" min-width=\"280px\"/>\n</p>\n<p>Data Context는 세 가지 종류가 있습니다.</p>\n<ul>\n<li><strong>Ephemeral Data Context</strong>: <strong>In-Memory</strong> 내에 존재하는 Data Context 입니다. GX Context를 실행하는 Python Session 외에서는 존재 하지 않습니다.</li>\n<li><strong>File Data Context</strong>: <strong>File</strong> 형태로 Configuration을 설정 하는 방식입니다. GX Context를 실행하는 Python Session 간 해당 설정을 공유 할 수 있습니다. 큰 프로젝트를 관리 한다면 권장 하는 형식 입니다.</li>\n<li><strong>Cloud Data Context</strong>: GX Cloud를 사용 할 때만 해당합니다. GX Cloud에 Context를 저장하는 형태 입니다.</li>\n</ul>\n<h4>Stores</h4>\n<p><strong>Data Context</strong>는 각 <strong>Core Concepts</strong>에서 필요한 저장 공간에 대한 정보들도 지정 해 줄 수 있습니다. 저장 공간으로는 다음을 사용 할 수 있습니다.</p>\n<ul>\n<li>Local File System</li>\n<li>Amazon S3</li>\n</ul>\n<p><strong>Store</strong>의 종류는 다음과 같습니다.</p>\n<ul>\n<li>Expectation Store: 데이터 검증을 담당하는 <strong>Expectation 정보</strong>를 저장합니다.</li>\n<li>Validation Store: <strong>데이터 검증 결과</strong>를 저장합니다.</li>\n<li>Checkpoint Store: <strong>Checkpoint의 메타 데이터</strong>를 저장합니다.</li>\n<li>Data Docs Store: <strong>Data Docs</strong>를 저장합니다.</li>\n<li>Metric Store: <strong>Anomaly Detection</strong>을 위해 사용 되며, 계산 완료 된 Metric (e.g. 열의 평균, 레코드 수)를 저장 한 후, <strong>데이터의 추이</strong>를 분석 하는데 사용 됩니다.</li>\n</ul>\n<h4>Data Docs</h4>\n<p><strong>Data Docs</strong>를 이용하여 Great Expectations Application으로 생성 된 데이터 (<strong>Data Validation, Data Profiling 결과</strong> 등)를 <strong>HTML 문서로 확인</strong> 할 수 있습니다.</p>\n<p>원하는 <strong>File System에 호스팅</strong> 하여 사용 가능합니다.</p>\n<h3>Data Sources</h3>\n<p><strong>Data Sources</strong>는 Great Expectations와 <strong>실제 데이터 (Local file, S3, HDFS, RDBMS 등)를 연결</strong> 해 주는 Component입니다.</p>\n<p align=\"center\">\n    <img src=\"/post_image/great-expectations/01-02.png\" width=\"50%\" min-width=\"280px\"/>\n</p>\n<h4>Data Asset(s)</h4>\n<p><strong>Data Asset</strong>은 Data Sources 내의 <strong>Data Record의 추상적인 모음</strong> 입니다. Data Asset > Batch > Batch Request로 <strong>Hierarchy</strong>가 설정 되어 있습니다.</p>\n<h4>Batch(es)</h4>\n<p><strong>Batch</strong>는 <strong>Data Asset</strong>의 <strong>고유한 부분 집합</strong>입니다.\n예를 들면 특정 테이블을 <strong>Data Asset이라고 가정</strong> 하였을 때,\nmonth, day, hour 등의 partition으로 나누어진 데이터가 <strong>Batch</strong>라고 볼 수 있습니다.</p>\n<h4>Batch Requests</h4>\n<p><strong>Great Expectations Application</strong>에서 <strong>한 개 이상의 Batch 데이터</strong>를 가져올 수 있습니다.\n<strong>Batch 객체</strong>를 가져와, <strong>Batch Requests</strong>로 <strong>파티션 별 데이터를 추출</strong> 하여, 이를 추후 <strong>Expectations Suite</strong>에서 사용, <strong>검증 로직을 수행</strong> 하는 방식으로 사용 할 수 있습니다.</p>\n<h3>Expectations</h3>\n<p><strong>Expectations</strong>는 <strong>데이터를 검증</strong>하는 컴포넌트입니다.\n입력 되는 데이터에 대해 (Batch) <strong>검증을 수행</strong> 합니다.</p>\n<h4>Expectation Suites</h4>\n<p><strong>Expectation Suites</strong>는 <strong>Expectation의 집합</strong>입니다.\n<strong>Great Expectations</strong>에서는 Expectation Suites에 있는 Expectation들을 바탕으로,\n<strong>Data Verification을 수행</strong>합니다.</p>\n<h4>Data Assistants</h4>\n<p><strong>Data Assistant</strong>는 <strong>Batch Data를 분석</strong> 하여 <strong>Expectation Suites를 빠르게 생성</strong> 할 수 있도록 돕는 도구입니다.</p>\n<p>Use Case는 <strong>Automatic Constraints Suggestion</strong>등이 있습니다.</p>\n<h3>Checkpoints</h3>\n<p><strong>Checkpoint</strong>는 Great Expectations에서 <strong>데이터를 검증</strong>하는 수단입니다.\n<strong>Data Batch와 Expectation Suites를 묶은</strong> 추상 계층입니다.</p>\n<p align=\"center\">\n    <img src=\"/post_image/great-expectations/01-03.png\" width=\"50%\" min-width=\"280px\"/>\n</p>\n<h4>Validation Results</h4>\n<p><strong>Checkpoint</strong>를 통해 수행한 <strong>Validation 결과</strong> 입니다.\nData Context를 통해 <strong>Data Docs로 Export</strong> 가능 합니다.</p>\n<h4>Actions</h4>\n<p><strong>Validation Result</strong>에 따라서, 사용자가 설정한 <strong>Action</strong>을 수행 할 수 있습니다.\n<strong>Email Notification, Slack Notification</strong> 등을 수행 할 수 있습니다.</p>\n<h3>마치며</h3>\n<p>이렇게 간단하게 Great Expectations가 필요 한 이유와, 핵심 컨셉에 대해서 알아 보았는데요. 다음 시간에는 실제 코드로 작성 해 보고, 실제로 어떻게 반환 되는지 확인 해 보는 시간을 가져보도록 하겠습니다.</p>\n<p>긴 글 읽어 주셔서 감사합니다.</p>","id":"e9fb6855-ce94-588e-9305-34d25a15f063","frontmatter":{"date":"2024-02-18","path":"/data-engineering/great-expectations","title":"Great Expectations란?","tags":["Data-Engineering"],"keyword":"Data Engineering, Data Quality Tool, Great Expectations","summary":"Data Quality Tool인 Great Expectations에 대해 알아 봅니다.","img":"/post_image/thumbnail/great-expectations.png","series":"Great Expectations"}}},"pageContext":{"postPath":"/data-engineering/great-expectations","series":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"e9fb6855-ce94-588e-9305-34d25a15f063","excerpt":"Intro 안녕하세요! 박민재, JustKode 입니다. 오늘은 Data Quality Tool인 Great Expectations에 대해서 알아 볼 것 입니다. Data Quality…","frontmatter":{"date":"2024-02-18","tags":["Data-Engineering"],"path":"/data-engineering/great-expectations","title":"Great Expectations란?","img":"/post_image/thumbnail/great-expectations.png","summary":"Data Quality Tool인 Great Expectations에 대해 알아 봅니다."}}}]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"6eaa7aeb-a4fe-5cd9-bbe5-309dde97514b","excerpt":"안녕하세요, 박민재입니다. 오늘은 Airflow DB를 관리하는 방법에 대해서 이야기 나눠 보도록 하겠습니다. Airflow Backend Database Airflow에서 Backend Database는 어떤 역할을 할까요? Airflow에서 DAG을 실행 하기 위해서, Airflow는 다음과 같은 정보들을 Backend Database에 저장하여 정합성을 유지 합니다. DagRun: 특정 Interval에 실행 된 DagRun…","frontmatter":{"date":"2024-10-27","tags":["Data-Engineering"],"path":"/data-engineering/airflow-db-management","title":"Airflow Backend Database Management (airflow db clean)","img":"/post_image/thumbnail/airflow-db-management.webp","summary":"Airflow의 Backend Database를 관리 하는 법"}}},{"node":{"id":"b595b168-8160-5abf-9641-39746d2f9e82","excerpt":"안녕하세요, 박민재입니다. 오늘은 Spark Application내의 각 Executor 내의 Task에 제한적으로 변수를 공유하는 두 가지 방법에 대해서 알아 보도록 하겠습니다. 시작하기에 앞서 단편적으로 생각 해 보면, Spark Application에서 연산 과정의 변수를 공유 한다는 것은 어려운 일인가? 라는 질문을 던져 볼 수 있습니다. 우리는 Task…","frontmatter":{"date":"2024-10-11","tags":["Data-Engineering"],"path":"/data-engineering/spark-sharing-variables","title":"Sharing Variables in Spark - Broadcast, Accumulator","img":"/post_image/thumbnail/spark-sharing-variables.png","summary":"Spark Application에서 변수를 공유 하는 방법"}}},{"node":{"id":"5effd6a1-bfd8-5a86-b718-5eb1e534601e","excerpt":"Intro 안녕하세요, 박민재입니다. 오늘은 Spark Memory에 관해 Deep Dive를 해 보도록 하겠습니다. Spark는 In-Memory를 이용하여, 빠른 연산을 할 수 있도록 보장합니다. 하지만, In-Memory 연산은 빠른 대신, 비싼 관계로 적은 리소스 만을 활용할 수 있습니다. 그렇기 때문에 우리는 효율적으로 Memory를 관리 하여, Spark Application이 빠르고, 안정적으로 Task…","frontmatter":{"date":"2024-04-12","tags":["Data-Engineering"],"path":"/data-engineering/spark-memory-deep-dive","title":"Deep Dive of Spark Memory","img":"/post_image/thumbnail/spark-memory-deep-dive.jpeg","summary":"Spark Memory의 깊은 이해를 위해 Deep Dive를 해 보자."}}},{"node":{"id":"9a77a751-f1a2-59c2-9065-808620c51bbe","excerpt":"안녕하세요? 박민재입니다. 오늘은 Data Discovery에 대해서 알아 보도록 하겠습니다. What is Data Discovery? Data Discovery란 무엇 일까요? Data Discovery는 조직 내 데이터를 찾고 이해하는 프로세스를 의미 합니다. Data Engineer들은 요구사항을 수행하기 위해, 많은 Data Source에서 다양한 Data Table…","frontmatter":{"date":"2024-03-17","tags":["Data-Engineering"],"path":"/data-engineering/what-is-data-discovery","title":"Data Discovery란 무엇인가?","img":"/post_image/thumbnail/what-is-data-discovery.jpeg","summary":"데이터를 찾고 이해하는 프로세스인, Data Discovery에 대하여"}}}]}}}}},"staticQueryHashes":["3819017183","63159454"],"slicesMap":{}}