{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/data-engineering/apache-iceberg/","result":{"data":{"markdownRemark":{"html":"<h3>Intro</h3>\n<p>안녕하세요? <strong>JustKode, 박민재</strong>입니다. 오늘은 <strong>차세대 Table Format</strong>인 <strong>Apache Iceberg</strong>에 대해서 알아 보도록 하겠습니다.</p>\n<p align=\"center\">\n    <img src=\"/post_image/apache-iceberg.png\" style=\"width:30%;max-width:280px;\"/>\n    <div align=\"center\" color=\"#aaaaaa\">Apache Iceberg Logo</div>\n</p>\n<p>왜 우리는 <strong>Table Format</strong>이 필요할까요? 사용자들은 여러 가지의 이유로 생성된, 여러 개의 테이블을 사용하게 됩니다. 우리들은 테이블의 다양한 사용처를 위해 <strong>데이터 구조화, 스키마 정의 및 확장, 인덱싱, 데이터 일관성 보장 및 버전 관리</strong>를 수행 할 수 있어야 하는데요, 이는 <strong>Apache Hive</strong>의 <strong>Meta Store</strong> 등으로 수행 하는데 있어, 제약사항이 존재 합니다. (ACID 미지원, Schema Evolution 미지원 등) 이를 원활하게 수행 할 수 있도록 도와주는 것이 <strong>Apache Iceberg, Apache Hudi, Delta Lake</strong>와 같은 <strong>Table Format</strong> 입니다.</p>\n<p>그 중, <strong>Apache Iceberg</strong>는 <strong>Open Source</strong> 기반의 <strong>Table Format</strong> 입니다. <strong>Hive, Spark, Flink, Presto</strong> 등, 원하는 <strong>Engine</strong>에서 SQL을 사용 하더라도, <strong>동일한 논리적 테이블</strong>에 동일 하게 데이터를 저장 할 수 있으며, <strong>ACID 지원, 동시성 제공, 메타데이터 확장 기능, Snapshot 기능을 통한 Rollback 기능 등</strong>을 제공 합니다. 이것들이 어떻게 가능 할 수 있었는지 한 번 알아 볼까요?</p>\n<h3>Architecture</h3>\n<p><strong>Apache Iceberg</strong>는 <strong>Metadata</strong>을 기반으로 <strong>Snapshot</strong>을 관리 하며 데이터를 저장 합니다. 기존 <strong>Hive</strong>나 <strong>Spark</strong>의 파티셔닝 기능을 사용 한다면, Dictionary를 이용하여 파일을 저장 하였겠지만 (ex: <code class=\"language-text\">month</code>, <code class=\"language-text\">day</code>, <code class=\"language-text\">hour</code>로 파티셔닝을 수행 한다면, <code class=\"language-text\">/month=202401/day=20240101/hour=2024010100</code>) <strong>Apache Iceberg</strong>는 <strong>Metadata File</strong>과 <strong>Manifest List, Manifest File</strong>을 이용하여, 특정 <strong>Snapshot</strong>에 해당하는 파일들을 관리 합니다. 여기서 <strong>Metadata File</strong>은 <strong>Table의 특정 Snapshot</strong> 정보를 저장 합니다.</p>\n<p align=\"center\">\n    <img src=\"/post_image/apache-iceberg/01-01.png\" style=\"width:70%;max-width 540px;\"/>\n    <div align=\"center\" color=\"#aaaaaa\">Apache Iceberg의 내부 Spec</div>\n</p>\n<p>이렇게 <strong>Metadata</strong> 기반으로 관리를 하게 되면, 각 <strong>Snapshot</strong>의 데이터를 디렉토리가 아닌, <strong>Metadata</strong>가 각 개별 파일을 바라보는 방식으로로 관리를 수행 할 수 있습니다.</p>\n<h3>Features</h3>\n<p>기본적으로 <strong>Apache Iceberg</strong>는 <strong>Spark SQL</strong>을 사용하는 경우에는 <strong>Catalog, Database, Table</strong>의 Hierarchy로 작동 합니다. 해당 예제들은 <strong>Spark SQL</strong>을 사용 한다고 가정 합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">SELECT</span> <span class=\"token operator\">*</span> <span class=\"token keyword\">FROM</span> prod<span class=\"token punctuation\">.</span>db<span class=\"token punctuation\">.</span><span class=\"token keyword\">table</span><span class=\"token punctuation\">;</span>  <span class=\"token comment\">-- catalog: prod, namespace: db, table: table</span></code></pre></div>\n<h4>Expressive SQL</h4>\n<p>Iceberg는 기존에 제공하지 않는 다양한 <code class=\"language-text\">SQL</code> 문들을 제공합니다. <code class=\"language-text\">MERGE INTO</code>와 같이 조건에 따라 데이터를 업데이트 하거나 추가 할 수 있는 문법을 제공 합니다. 다음은 기존 테이블의 id와 겹치지 않는다면, 데이터를 삽입 해 주는 예시입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">MERGE</span> <span class=\"token keyword\">INTO</span> prod<span class=\"token punctuation\">.</span>nyc<span class=\"token punctuation\">.</span>taxis pt\n<span class=\"token keyword\">USING</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">SELECT</span> <span class=\"token operator\">*</span> <span class=\"token keyword\">FROM</span> staging<span class=\"token punctuation\">.</span>nyc<span class=\"token punctuation\">.</span>taxis<span class=\"token punctuation\">)</span> st\n<span class=\"token keyword\">ON</span> pt<span class=\"token punctuation\">.</span>id <span class=\"token operator\">=</span> st<span class=\"token punctuation\">.</span>id\n<span class=\"token keyword\">WHEN</span> <span class=\"token operator\">NOT</span> <span class=\"token keyword\">MATCHED</span> <span class=\"token keyword\">THEN</span> <span class=\"token keyword\">INSERT</span> <span class=\"token operator\">*</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>또한, 테이블 내에서 데이터를 삭제할 때, Metadata 상에서만 삭제를 수행 한 뒤, 추후에 File을 Rewrite 하는 방식으로 <strong>Read &#x26; Update Performance</strong>를 높이기도 합니다.</p>\n<h4>Schema Evolution</h4>\n<p>기존 <strong>Hive Metastore</strong>를 이용 하는 것과 다르게, <strong>Apache Iceberg</strong> 에서는 유연한 <strong>Schema Evolution</strong>이 가능 합니다. <code class=\"language-text\">ADD</code>, <code class=\"language-text\">DROP</code>, <code class=\"language-text\">RENAME</code>, <code class=\"language-text\">UPDATE</code>, <code class=\"language-text\">REORDER</code> 등의 기능을 지원 하는데요, 이가 가능한 이유는 <strong>Iceberg</strong> 내부적으로 이를 <strong>Metadata</strong>로 저장 하기 때문에 가능 합니다. <strong>Iceberg</strong>를 사용 하는 각 Engine (Spark, Hive 등)에서는, <strong>Metadata</strong>에 저장 되어 있는 <strong>테이블 정보</strong>와 <strong>데이터가 일치</strong> 하도록 <strong>File Read를 수행</strong> 합니다. 없는 Column은 읽지 않고, 추가된 Column으로 과거 데이터를 읽는다면 <code class=\"language-text\">null</code> 혹은 <strong>기본 값</strong>으로 데이터를 반환 합니다.</p>\n<p>Partition에 대한 <strong>Schema Evolution</strong>도 유연하게 수행 할 수 있는데요, 이가 가능한 이유는, <strong>Iceberg</strong>는 Partitioning을 <strong>Metadata</strong> 기반으로 수행 하기 때문입니다. Partition 관련 정보가 수정 되면, 그 이후 부터는 파일 저장을 내부적으로 <strong>Hash, Range 기법</strong>으로 특정 Value 값을 가진 Row 끼리 묶어서 파일을 저장하고 (Bucketing), 해당 정보들을 <strong>Metadata</strong>에 저장 합니다. 그 이후 쿼리 수행 시, <strong>Metadata</strong>를 참조한 후, 필요한 파일 만을 이용하여 쿼리를 수행 합니다. (<a href=\"https://iceberg.apache.org/spec/#partition-transforms\">관련 Spec</a>)</p>\n<p align=\"center\">\n    <img src=\"/post_image/apache-iceberg/01-02.png\" style=\"width:70%;max-width 540px;\"/>\n    <div align=\"center\" color=\"#aaaaaa\">Apache Iceberg의 Partition Evolution</div>\n</p>\n<h3>Hidden Partitioning</h3>\n<p>아까 설명한 <strong>Schema Evolution</strong>의 근본이 되는 원리입니다. <strong>Iceberg</strong> 에서는 물리적으로 <strong>Partitioning</strong>을 수행 하지 않아도, 내부적으로 <strong>Hidden Partitioning</strong>을 수행 하여, Hive와 다르게, 파일을 Dictionary 등을 이용 하여 물리적으로 <strong>Partitioning</strong>을 수행 할 필요가 없습니다. <strong>Metadata</strong>에 <strong>특정 파일</strong>이 가지고 있는 <strong>특정 Column의 Upper Bound, Lower Bound 등</strong>을 가지고 있어, 해당 값을 바탕으로 필요한 파일만 Read 하여, <strong>Partitioning의 효과</strong>를 냅니다.</p>\n<p>다음은 테이블 내부의 <strong>Metadata</strong> 정보를 반환 하는 예제입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">SELECT</span> <span class=\"token operator\">*</span> <span class=\"token keyword\">FROM</span> prod<span class=\"token punctuation\">.</span>db<span class=\"token punctuation\">.</span><span class=\"token keyword\">table</span><span class=\"token punctuation\">.</span>files<span class=\"token punctuation\">;</span>  <span class=\"token comment\">-- catalog: prod, namespace: db, table: table</span></code></pre></div>\n<table>\n<thead>\n<tr>\n<th>content</th>\n<th>file_path</th>\n<th>file_format</th>\n<th>spec_id</th>\n<th>partition</th>\n<th>record_count</th>\n<th>file_size_in_bytes</th>\n<th>column_sizes</th>\n<th>value_counts</th>\n<th>null_value_counts</th>\n<th>nan_value_counts</th>\n<th>lower_bounds</th>\n<th>upper_bounds</th>\n<th>key_metadata</th>\n<th>split_offsets</th>\n<th>equality_ids</th>\n<th>sort_order_id</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0</td>\n<td>s3:/.../table/data/00000-3-8d6d60e8-d427-4809-bcf0-f5d45a4aad96.parquet</td>\n<td>PARQUET</td>\n<td>0</td>\n<td>{1999-01-01, 01}</td>\n<td>1</td>\n<td>597</td>\n<td>[1 -> 90, 2 -> 62]</td>\n<td>[1 -> 1, 2 -> 1]</td>\n<td>[1 -> 0, 2 -> 0]</td>\n<td>[]</td>\n<td>[1 -> , 2 -> c]</td>\n<td>[1 -> , 2 -> c]</td>\n<td>null</td>\n<td>[4]</td>\n<td>null</td>\n<td>null</td>\n</tr>\n<tr>\n<td>0</td>\n<td>s3:/.../table/data/00001-4-8d6d60e8-d427-4809-bcf0-f5d45a4aad96.parquet</td>\n<td>PARQUET</td>\n<td>0</td>\n<td>{1999-01-01, 02}</td>\n<td>1</td>\n<td>597</td>\n<td>[1 -> 90, 2 -> 62]</td>\n<td>[1 -> 1, 2 -> 1]</td>\n<td>[1 -> 0, 2 -> 0]</td>\n<td>[]</td>\n<td>[1 -> , 2 -> b]</td>\n<td>[1 -> , 2 -> b]</td>\n<td>null</td>\n<td>[4]</td>\n<td>null</td>\n<td>null</td>\n</tr>\n<tr>\n<td>0</td>\n<td>s3:/.../table/data/00002-5-8d6d60e8-d427-4809-bcf0-f5d45a4aad96.parquet</td>\n<td>PARQUET</td>\n<td>0</td>\n<td>{1999-01-01, 03}</td>\n<td>1</td>\n<td>597</td>\n<td>[1 -> 90, 2 -> 62]</td>\n<td>[1 -> 1, 2 -> 1]</td>\n<td>[1 -> 0, 2 -> 0]</td>\n<td>[]</td>\n<td>[1 -> , 2 -> a]</td>\n<td>[1 -> , 2 -> a]</td>\n<td>null</td>\n<td>[4]</td>\n<td>null</td>\n<td>null</td>\n</tr>\n</tbody>\n</table>\n<h3>Travel And Rollback</h3>\n<p><strong>Apache Iceberg</strong>는 <strong>Snapshot</strong> 기반으로 <strong>Metadata</strong>를 저장하고 운영 합니다. 그렇기에 <strong>Snapshot Metadata</strong>와 해당하는 Snapshot의 <strong>파일 들</strong>이 남아있다면, 해당 시점의 데이터에 대해서 조회가 가능합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">sql</span><span class=\"token operator\">></span> <span class=\"token keyword\">SELECT</span> <span class=\"token function\">count</span><span class=\"token punctuation\">(</span><span class=\"token operator\">*</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">FROM</span> nyc<span class=\"token punctuation\">.</span>taxis<span class=\"token punctuation\">;</span>\n<span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">853</span><span class=\"token punctuation\">,</span><span class=\"token number\">020</span>\n<span class=\"token keyword\">sql</span><span class=\"token operator\">></span> <span class=\"token keyword\">SELECT</span> <span class=\"token function\">count</span><span class=\"token punctuation\">(</span><span class=\"token operator\">*</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">FROM</span> nyc<span class=\"token punctuation\">.</span>taxis <span class=\"token keyword\">FOR</span> VERSION <span class=\"token keyword\">AS</span> <span class=\"token keyword\">OF</span> <span class=\"token number\">2188465307835585443</span><span class=\"token punctuation\">;</span>\n<span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">798</span><span class=\"token punctuation\">,</span><span class=\"token number\">371</span>\n<span class=\"token keyword\">sql</span><span class=\"token operator\">></span> <span class=\"token keyword\">SELECT</span> <span class=\"token function\">count</span><span class=\"token punctuation\">(</span><span class=\"token operator\">*</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">FROM</span> nyc<span class=\"token punctuation\">.</span>taxis <span class=\"token keyword\">FOR</span> <span class=\"token keyword\">TIMESTAMP</span> <span class=\"token keyword\">AS</span> <span class=\"token keyword\">OF</span> <span class=\"token keyword\">TIMESTAMP</span> <span class=\"token string\">'2022-01-01 00:00:00.000000 Z'</span><span class=\"token punctuation\">;</span>\n<span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">798</span><span class=\"token punctuation\">,</span><span class=\"token number\">371</span></code></pre></div>\n<h3>File Compaction</h3>\n<p><strong>Apache Iceberg</strong> 에는 <strong>File Compaction</strong> 기능이 존재 하는데요, 이는 <strong>Hadoop</strong> 등의 <strong>Block Size</strong>를 관리 해 주어야 하는 경우에 유용 합니다. 특정 테이블에 산개해 있는 파일을 <strong>미리 설정한 Size로 Compaction</strong> 할 수 있습니다. 이는 <strong>HDFS</strong>의 <strong>Block Size</strong>와 유사 할 수록 좋겠죠?</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\">Table table <span class=\"token operator\">=</span> <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\nSparkActions\n    <span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">.</span>rewriteDataFiles<span class=\"token punctuation\">(</span>table<span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">.</span>filter<span class=\"token punctuation\">(</span>Expressions<span class=\"token punctuation\">.</span>equal<span class=\"token punctuation\">(</span><span class=\"token string\">\"date\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"2020-08-18\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">.</span>option<span class=\"token punctuation\">(</span><span class=\"token string\">\"target-file-size-bytes\"</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">Long</span><span class=\"token punctuation\">.</span>toString<span class=\"token punctuation\">(</span><span class=\"token number\">500</span> <span class=\"token operator\">*</span> <span class=\"token number\">1024</span> <span class=\"token operator\">*</span> <span class=\"token number\">1024</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\">// 500 MB</span>\n    <span class=\"token punctuation\">.</span>execute<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>당연히, Metadata 또한 HDFS에서 File로 관리 될 터이니, 이 또한 잘 관리 해 주는 것이 좋습니다. <strong>Manifest 또한 Compaction</strong>을 수행 할 수 있어요.</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\">Table table <span class=\"token operator\">=</span> <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\nSparkActions\n    <span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">.</span>rewriteManifests<span class=\"token punctuation\">(</span>table<span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">.</span>rewriteIf<span class=\"token punctuation\">(</span>file <span class=\"token operator\">-></span> file<span class=\"token punctuation\">.</span>length<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">&lt;</span> <span class=\"token number\">10</span> <span class=\"token operator\">*</span> <span class=\"token number\">1024</span> <span class=\"token operator\">*</span> <span class=\"token number\">1024</span><span class=\"token punctuation\">)</span> <span class=\"token comment\">// 10 MB</span>\n    <span class=\"token punctuation\">.</span>execute<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<h3>마치며</h3>\n<p>이렇게 <strong>Apache Iceberg</strong>에 대해서 <strong>전반적인 원리와 기능들</strong>에 훑어 보는 시간을 가졌습니다. 다음 시간에는 <strong>이를 Spark와 연동하는 예제</strong>로 찾아 뵙겠습니다. 감사합니다.</p>","id":"45689878-4254-5d64-9235-459fee37023f","frontmatter":{"date":"2024-01-18","path":"/data-engineering/apache-iceberg","title":"Apache Iceberg란?","tags":["Data-Engineering"],"keyword":"Iceberg,Apache Iceberg,빅데이터,Big Data","summary":"차세대 Table Format인 Iceberg에 대해서","img":"/post_image/apache-iceberg.png","series":null}}},"pageContext":{"postPath":"/data-engineering/apache-iceberg","series":{"data":{"allMarkdownRemark":{"edges":[]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"45689878-4254-5d64-9235-459fee37023f","excerpt":"Intro 안녕하세요? JustKode, 박민재입니다. 오늘은 차세대 Table Format인 Apache Iceberg에 대해서 알아 보도록 하겠습니다. 왜 우리는 Table Format…","frontmatter":{"date":"2024-01-18","tags":["Data-Engineering"],"path":"/data-engineering/apache-iceberg","title":"Apache Iceberg란?","img":"/post_image/apache-iceberg.png","summary":"차세대 Table Format인 Iceberg에 대해서"}}},{"node":{"id":"861432bb-8f64-567c-93b9-306d6599cb57","excerpt":"머릿말 안녕하세요? 새해부터 찾아온 JustKode, 박민재입니다. 오늘은 Spark Structured Streaming에 대한 Unit Test를 수행 하는 법에 대해서 공유 드려 보려고 합니다. What is Spark Structured Streaming? Spark Structured Streaming은 Spark SQL API (Dataframe, Dataset)를 이용하여, Streaming…","frontmatter":{"date":"2024-01-07","tags":["Data-Engineering"],"path":"/data-engineering/spark-structured-streaming-unit-test","title":"Unit Test of Spark Structured Streaming","img":"/post_image/thumbnail/spark-structured-streaming-unit-test.jpeg","summary":"더 정밀한 Streaming Unit Test를 위해"}}},{"node":{"id":"e46743f9-8a4c-5573-9ff5-b8e1b50ff808","excerpt":"머릿말 안녕하세요? JustKode, 박민재 입니다. 이 글을 쓰는 지금, Data Engineer로 LINE Plus에 입사한지 벌써 만으로 1년이 다 되어 가네요. 올해 1월에 입사 했으니까요. 첫 사회 생활, 첫 회사에서 (첫 인턴, 첫 회사가 LINE Plus…","frontmatter":{"date":"2023-12-21","tags":["etc","Data-Engineering"],"path":"/etc/2023-retrospect","title":"1년차 Data Engineer의 회고","img":"/post_image/2023-retrospect.jpeg","summary":"2023년을 되돌아 봅니다."}}},{"node":{"id":"5a1c922e-fff4-5443-87e1-15282d06c381","excerpt":"Summary Data Engineering이 구체적으로 무엇인지 알아 봅니다. 왜 Data Engineering이 필요 하게 되었는지 알아 봅니다. Data Engineer는 팀 혹은 회사에서 어떤 역할을 수행 하는지 알아 봅니다. 머릿말 안녕하세요? JustKode, 박민재입니다. 오늘은 Data Engineering이 구체적으로 무엇 인지, Data Engineer…","frontmatter":{"date":"2023-12-05","tags":["Data-Engineering"],"path":"/data-engineering/data-engineering-intro","title":"Data Engineering이란?","img":"/post_image/data-engineering-intro.png","summary":"Data Engineering에 대하여"}}}]}}}}},"staticQueryHashes":["3819017183","63159454"],"slicesMap":{}}