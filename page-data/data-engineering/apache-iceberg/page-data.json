{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/data-engineering/apache-iceberg/","result":{"data":{"markdownRemark":{"html":"<h3>Intro</h3>\n<p>안녕하세요? <strong>JustKode, 박민재</strong>입니다. 오늘은 <strong>차세대 Table Format</strong>인 <strong>Apache Iceberg</strong>에 대해서 알아 보도록 하겠습니다.</p>\n<p align=\"center\">\n    <img src=\"/post_image/apache-iceberg.png\" style=\"width:30%;max-width:280px;\"/>\n    <div align=\"center\" color=\"#aaaaaa\">Apache Iceberg Logo</div>\n</p>\n<p>왜 우리는 <strong>Table Format</strong>이 필요할까요? 사용자들은 여러 가지의 이유로 생성된, 여러 개의 테이블을 사용하게 됩니다. 우리들은 테이블의 다양한 사용처를 위해 <strong>데이터 구조화, 스키마 정의 및 확장, 인덱싱, 데이터 일관성 보장 및 버전 관리</strong>를 수행 할 수 있어야 하는데요, 이는 <strong>Apache Hive</strong>의 <strong>Meta Store</strong> 등으로 수행 하는데 있어, 제약사항이 존재 합니다. (ACID 미지원, Schema Evolution 미지원 등) 이를 원활하게 수행 할 수 있도록 도와주는 것이 <strong>Apache Iceberg, Apache Hudi, Delta Lake</strong>와 같은 <strong>Table Format</strong> 입니다.</p>\n<p>그 중, <strong>Apache Iceberg</strong>는 <strong>Open Source</strong> 기반의 <strong>Table Format</strong> 입니다. <strong>Hive, Spark, Flink, Presto</strong> 등, 원하는 <strong>Engine</strong>에서 SQL을 사용 하더라도, <strong>동일한 논리적 테이블</strong>에 동일 하게 데이터를 저장 할 수 있으며, <strong>ACID 지원, 동시성 제공, 메타데이터 확장 기능, Snapshot 기능을 통한 Rollback 기능 등</strong>을 제공 합니다. 이것들이 어떻게 가능 할 수 있었는지 한 번 알아 볼까요?</p>\n<h3>Architecture</h3>\n<p><strong>Apache Iceberg</strong>는 <strong>Metadata</strong>을 기반으로 <strong>Snapshot</strong>을 관리 하며 데이터를 저장 합니다. 기존 <strong>Hive</strong>나 <strong>Spark</strong>의 파티셔닝 기능을 사용 한다면, Dictionary를 이용하여 파일을 저장 하였겠지만 (ex: <code class=\"language-text\">month</code>, <code class=\"language-text\">day</code>, <code class=\"language-text\">hour</code>로 파티셔닝을 수행 한다면, <code class=\"language-text\">/month=202401/day=20240101/hour=2024010100</code>) <strong>Apache Iceberg</strong>는 <strong>Metadata File</strong>과 <strong>Manifest List, Manifest File</strong>을 이용하여, 특정 <strong>Snapshot</strong>에 해당하는 파일들을 관리 합니다. 여기서 <strong>Metadata File</strong>은 <strong>Table의 특정 Snapshot</strong> 정보를 저장 합니다.</p>\n<p align=\"center\">\n    <img src=\"/post_image/apache-iceberg/01-01.png\" style=\"width:70%;max-width 540px;\"/>\n    <div align=\"center\" color=\"#aaaaaa\">Apache Iceberg의 내부 Spec</div>\n</p>\n<p>이렇게 <strong>Metadata</strong> 기반으로 관리를 하게 되면, 각 <strong>Snapshot</strong>의 데이터를 디렉토리가 아닌, <strong>Metadata</strong>가 각 개별 파일을 바라보는 방식으로로 관리를 수행 할 수 있습니다.</p>\n<h3>Features</h3>\n<p>기본적으로 <strong>Apache Iceberg</strong>는 <strong>Spark SQL</strong>을 사용하는 경우에는 <strong>Catalog, Database, Table</strong>의 Hierarchy로 작동 합니다. 해당 예제들은 <strong>Spark SQL</strong>을 사용 한다고 가정 합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">SELECT</span> <span class=\"token operator\">*</span> <span class=\"token keyword\">FROM</span> prod<span class=\"token punctuation\">.</span>db<span class=\"token punctuation\">.</span><span class=\"token keyword\">table</span><span class=\"token punctuation\">;</span>  <span class=\"token comment\">-- catalog: prod, namespace: db, table: table</span></code></pre></div>\n<h4>Expressive SQL</h4>\n<p>Iceberg는 기존에 제공하지 않는 다양한 <code class=\"language-text\">SQL</code> 문들을 제공합니다. <code class=\"language-text\">MERGE INTO</code>와 같이 조건에 따라 데이터를 업데이트 하거나 추가 할 수 있는 문법을 제공 합니다. 다음은 기존 테이블의 id와 겹치지 않는다면, 데이터를 삽입 해 주는 예시입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">MERGE</span> <span class=\"token keyword\">INTO</span> prod<span class=\"token punctuation\">.</span>nyc<span class=\"token punctuation\">.</span>taxis pt\n<span class=\"token keyword\">USING</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">SELECT</span> <span class=\"token operator\">*</span> <span class=\"token keyword\">FROM</span> staging<span class=\"token punctuation\">.</span>nyc<span class=\"token punctuation\">.</span>taxis<span class=\"token punctuation\">)</span> st\n<span class=\"token keyword\">ON</span> pt<span class=\"token punctuation\">.</span>id <span class=\"token operator\">=</span> st<span class=\"token punctuation\">.</span>id\n<span class=\"token keyword\">WHEN</span> <span class=\"token operator\">NOT</span> <span class=\"token keyword\">MATCHED</span> <span class=\"token keyword\">THEN</span> <span class=\"token keyword\">INSERT</span> <span class=\"token operator\">*</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>또한, 테이블 내에서 데이터를 삭제할 때, Metadata 상에서만 삭제를 수행 한 뒤, 추후에 File을 Rewrite 하는 방식으로 <strong>Read &#x26; Update Performance</strong>를 높이기도 합니다.</p>\n<h4>Schema Evolution</h4>\n<p>기존 <strong>Hive Metastore</strong>를 이용 하는 것과 다르게, <strong>Apache Iceberg</strong> 에서는 유연한 <strong>Schema Evolution</strong>이 가능 합니다. <code class=\"language-text\">ADD</code>, <code class=\"language-text\">DROP</code>, <code class=\"language-text\">RENAME</code>, <code class=\"language-text\">UPDATE</code>, <code class=\"language-text\">REORDER</code> 등의 기능을 지원 하는데요, 이가 가능한 이유는 <strong>Iceberg</strong> 내부적으로 이를 <strong>Metadata</strong>로 저장 하기 때문에 가능 합니다. <strong>Iceberg</strong>를 사용 하는 각 Engine (Spark, Hive 등)에서는, <strong>Metadata</strong>에 저장 되어 있는 <strong>테이블 정보</strong>와 <strong>데이터가 일치</strong> 하도록 <strong>File Read를 수행</strong> 합니다. 없는 Column은 읽지 않고, 추가된 Column으로 과거 데이터를 읽는다면 <code class=\"language-text\">null</code> 혹은 <strong>기본 값</strong>으로 데이터를 반환 합니다.</p>\n<p>Partition에 대한 <strong>Schema Evolution</strong>도 유연하게 수행 할 수 있는데요, 이가 가능한 이유는, <strong>Iceberg</strong>는 Partitioning을 <strong>Metadata</strong> 기반으로 수행 하기 때문입니다. Partition 관련 정보가 수정 되면, 그 이후 부터는 파일 저장을 내부적으로 <strong>Hash, Range 기법</strong>으로 특정 Value 값을 가진 Row 끼리 묶어서 파일을 저장하고 (Bucketing), 해당 정보들을 <strong>Metadata</strong>에 저장 합니다. 그 이후 쿼리 수행 시, <strong>Metadata</strong>를 참조한 후, 필요한 파일 만을 이용하여 쿼리를 수행 합니다. (<a href=\"https://iceberg.apache.org/spec/#partition-transforms\">관련 Spec</a>)</p>\n<p align=\"center\">\n    <img src=\"/post_image/apache-iceberg/01-02.png\" style=\"width:70%;max-width 540px;\"/>\n    <div align=\"center\" color=\"#aaaaaa\">Apache Iceberg의 Partition Evolution</div>\n</p>\n<h3>Hidden Partitioning</h3>\n<p>아까 설명한 <strong>Schema Evolution</strong>의 근본이 되는 원리입니다. <strong>Iceberg</strong> 에서는 물리적으로 <strong>Partitioning</strong>을 수행 하지 않아도, 내부적으로 <strong>Hidden Partitioning</strong>을 수행 하여, Hive와 다르게, 파일을 Dictionary 등을 이용 하여 물리적으로 <strong>Partitioning</strong>을 수행 할 필요가 없습니다. <strong>Metadata</strong>에 <strong>특정 파일</strong>이 가지고 있는 <strong>특정 Column의 Upper Bound, Lower Bound 등</strong>을 가지고 있어, 해당 값을 바탕으로 필요한 파일만 Read 하여, <strong>Partitioning의 효과</strong>를 냅니다.</p>\n<p>다음은 테이블 내부의 <strong>Metadata</strong> 정보를 반환 하는 예제입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">SELECT</span> <span class=\"token operator\">*</span> <span class=\"token keyword\">FROM</span> prod<span class=\"token punctuation\">.</span>db<span class=\"token punctuation\">.</span><span class=\"token keyword\">table</span><span class=\"token punctuation\">.</span>files<span class=\"token punctuation\">;</span>  <span class=\"token comment\">-- catalog: prod, namespace: db, table: table</span></code></pre></div>\n<table>\n<thead>\n<tr>\n<th>content</th>\n<th>file_path</th>\n<th>file_format</th>\n<th>spec_id</th>\n<th>partition</th>\n<th>record_count</th>\n<th>file_size_in_bytes</th>\n<th>column_sizes</th>\n<th>value_counts</th>\n<th>null_value_counts</th>\n<th>nan_value_counts</th>\n<th>lower_bounds</th>\n<th>upper_bounds</th>\n<th>key_metadata</th>\n<th>split_offsets</th>\n<th>equality_ids</th>\n<th>sort_order_id</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>0</td>\n<td>s3:/.../table/data/00000-3-8d6d60e8-d427-4809-bcf0-f5d45a4aad96.parquet</td>\n<td>PARQUET</td>\n<td>0</td>\n<td>{1999-01-01, 01}</td>\n<td>1</td>\n<td>597</td>\n<td>[1 -> 90, 2 -> 62]</td>\n<td>[1 -> 1, 2 -> 1]</td>\n<td>[1 -> 0, 2 -> 0]</td>\n<td>[]</td>\n<td>[1 -> , 2 -> c]</td>\n<td>[1 -> , 2 -> c]</td>\n<td>null</td>\n<td>[4]</td>\n<td>null</td>\n<td>null</td>\n</tr>\n<tr>\n<td>0</td>\n<td>s3:/.../table/data/00001-4-8d6d60e8-d427-4809-bcf0-f5d45a4aad96.parquet</td>\n<td>PARQUET</td>\n<td>0</td>\n<td>{1999-01-01, 02}</td>\n<td>1</td>\n<td>597</td>\n<td>[1 -> 90, 2 -> 62]</td>\n<td>[1 -> 1, 2 -> 1]</td>\n<td>[1 -> 0, 2 -> 0]</td>\n<td>[]</td>\n<td>[1 -> , 2 -> b]</td>\n<td>[1 -> , 2 -> b]</td>\n<td>null</td>\n<td>[4]</td>\n<td>null</td>\n<td>null</td>\n</tr>\n<tr>\n<td>0</td>\n<td>s3:/.../table/data/00002-5-8d6d60e8-d427-4809-bcf0-f5d45a4aad96.parquet</td>\n<td>PARQUET</td>\n<td>0</td>\n<td>{1999-01-01, 03}</td>\n<td>1</td>\n<td>597</td>\n<td>[1 -> 90, 2 -> 62]</td>\n<td>[1 -> 1, 2 -> 1]</td>\n<td>[1 -> 0, 2 -> 0]</td>\n<td>[]</td>\n<td>[1 -> , 2 -> a]</td>\n<td>[1 -> , 2 -> a]</td>\n<td>null</td>\n<td>[4]</td>\n<td>null</td>\n<td>null</td>\n</tr>\n</tbody>\n</table>\n<h3>Travel And Rollback</h3>\n<p><strong>Apache Iceberg</strong>는 <strong>Snapshot</strong> 기반으로 <strong>Metadata</strong>를 저장하고 운영 합니다. 그렇기에 <strong>Snapshot Metadata</strong>와 해당하는 Snapshot의 <strong>파일 들</strong>이 남아있다면, 해당 시점의 데이터에 대해서 조회가 가능합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">sql</span><span class=\"token operator\">></span> <span class=\"token keyword\">SELECT</span> <span class=\"token function\">count</span><span class=\"token punctuation\">(</span><span class=\"token operator\">*</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">FROM</span> nyc<span class=\"token punctuation\">.</span>taxis<span class=\"token punctuation\">;</span>\n<span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">853</span><span class=\"token punctuation\">,</span><span class=\"token number\">020</span>\n<span class=\"token keyword\">sql</span><span class=\"token operator\">></span> <span class=\"token keyword\">SELECT</span> <span class=\"token function\">count</span><span class=\"token punctuation\">(</span><span class=\"token operator\">*</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">FROM</span> nyc<span class=\"token punctuation\">.</span>taxis <span class=\"token keyword\">FOR</span> VERSION <span class=\"token keyword\">AS</span> <span class=\"token keyword\">OF</span> <span class=\"token number\">2188465307835585443</span><span class=\"token punctuation\">;</span>\n<span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">798</span><span class=\"token punctuation\">,</span><span class=\"token number\">371</span>\n<span class=\"token keyword\">sql</span><span class=\"token operator\">></span> <span class=\"token keyword\">SELECT</span> <span class=\"token function\">count</span><span class=\"token punctuation\">(</span><span class=\"token operator\">*</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">FROM</span> nyc<span class=\"token punctuation\">.</span>taxis <span class=\"token keyword\">FOR</span> <span class=\"token keyword\">TIMESTAMP</span> <span class=\"token keyword\">AS</span> <span class=\"token keyword\">OF</span> <span class=\"token keyword\">TIMESTAMP</span> <span class=\"token string\">'2022-01-01 00:00:00.000000 Z'</span><span class=\"token punctuation\">;</span>\n<span class=\"token number\">2</span><span class=\"token punctuation\">,</span><span class=\"token number\">798</span><span class=\"token punctuation\">,</span><span class=\"token number\">371</span></code></pre></div>\n<h3>File Compaction</h3>\n<p><strong>Apache Iceberg</strong> 에는 <strong>File Compaction</strong> 기능이 존재 하는데요, 이는 <strong>Hadoop</strong> 등의 <strong>Block Size</strong>를 관리 해 주어야 하는 경우에 유용 합니다. 특정 테이블에 산개해 있는 파일을 <strong>미리 설정한 Size로 Compaction</strong> 할 수 있습니다. 이는 <strong>HDFS</strong>의 <strong>Block Size</strong>와 유사 할 수록 좋겠죠?</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\">Table table <span class=\"token operator\">=</span> <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\nSparkActions\n    <span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">.</span>rewriteDataFiles<span class=\"token punctuation\">(</span>table<span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">.</span>filter<span class=\"token punctuation\">(</span>Expressions<span class=\"token punctuation\">.</span>equal<span class=\"token punctuation\">(</span><span class=\"token string\">\"date\"</span><span class=\"token punctuation\">,</span> <span class=\"token string\">\"2020-08-18\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">.</span>option<span class=\"token punctuation\">(</span><span class=\"token string\">\"target-file-size-bytes\"</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">Long</span><span class=\"token punctuation\">.</span>toString<span class=\"token punctuation\">(</span><span class=\"token number\">500</span> <span class=\"token operator\">*</span> <span class=\"token number\">1024</span> <span class=\"token operator\">*</span> <span class=\"token number\">1024</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token comment\">// 500 MB</span>\n    <span class=\"token punctuation\">.</span>execute<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>당연히, Metadata 또한 HDFS에서 File로 관리 될 터이니, 이 또한 잘 관리 해 주는 것이 좋습니다. <strong>Manifest 또한 Compaction</strong>을 수행 할 수 있어요.</p>\n<div class=\"gatsby-highlight\" data-language=\"scala\"><pre class=\"language-scala\"><code class=\"language-scala\">Table table <span class=\"token operator\">=</span> <span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\nSparkActions\n    <span class=\"token punctuation\">.</span>get<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">.</span>rewriteManifests<span class=\"token punctuation\">(</span>table<span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">.</span>rewriteIf<span class=\"token punctuation\">(</span>file <span class=\"token operator\">-></span> file<span class=\"token punctuation\">.</span>length<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">&lt;</span> <span class=\"token number\">10</span> <span class=\"token operator\">*</span> <span class=\"token number\">1024</span> <span class=\"token operator\">*</span> <span class=\"token number\">1024</span><span class=\"token punctuation\">)</span> <span class=\"token comment\">// 10 MB</span>\n    <span class=\"token punctuation\">.</span>execute<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<h3>마치며</h3>\n<p>이렇게 <strong>Apache Iceberg</strong>에 대해서 <strong>전반적인 원리와 기능들</strong>에 훑어 보는 시간을 가졌습니다. 다음 시간에는 <strong>이를 Spark와 연동하는 예제</strong>로 찾아 뵙겠습니다. 감사합니다.</p>","id":"18bc17e8-9e80-5ea8-a268-d2f17d897b30","frontmatter":{"date":"2024-01-18","path":"/data-engineering/apache-iceberg","title":"Apache Iceberg란?","tags":["Data-Engineering"],"keyword":"Iceberg,Apache Iceberg,빅데이터,Big Data","summary":"차세대 Table Format인 Iceberg에 대해서","img":"/post_image/apache-iceberg.png","series":null}}},"pageContext":{"postPath":"/data-engineering/apache-iceberg","series":{"data":{"allMarkdownRemark":{"edges":[]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"2001438a-fe37-5b3c-8954-bce7d5e18a7a","excerpt":"안녕하세요? 박민재입니다. 오늘은 Iceberg Table을 관리하는 방법 중 하나인, Branching & Tagging 그리고 Rollback Action에 대해서 알아 보도록 하겠습니다. Isolation of Changes with Branches Iceberg에서는 git과 같은 방식으로 Branch를 만들어, 데이터 변경 사항을 관리 할 수 있습니다. 우리의 사례로 빗대어 보면 H/W 이슈, 혹은 Application…","frontmatter":{"date":"2025-01-03","tags":["Data-Engineering"],"path":"/data-engineering/iceberg-table-management-2","title":"Iceberg Table Management - 2. Branching, Tagging & Rollback","img":"/post_image/thumbnail/iceberg-table-management.png","summary":"Branching, Tagging & Rollback을 통해 Iceberg Table을 관리 해 보자"}}},{"node":{"id":"dcd44de2-0eff-56f8-ac2d-2a99250ab9cf","excerpt":"안녕하세요? 박민재입니다. 오늘은 Iceberg Table을 관리하는 방법을 Metadata Table의 사용을 중심으로 깊게 알아 보도록 하겠습니다. Apache Iceberg의 경우에는 Metadata Table 기능을 매우 강력하게 지원합니다. 이를 통해 Iceberg Table을 운영을 쉽게 수행 할 수 있죠. 예를 들어, Table의 Evolution이 어떻게 진행 되었는지, 파일들이 어떻게 Partitioning…","frontmatter":{"date":"2024-12-05","tags":["Data-Engineering"],"path":"/data-engineering/iceberg-table-management-1","title":"Iceberg Table Management - 1. Metadata Table ","img":"/post_image/thumbnail/iceberg-table-management.png","summary":"Metadata Table을 통해 Iceberg Table을 관리 해 보자"}}},{"node":{"id":"cbb6e851-d864-5552-86d7-08c81b4a54cc","excerpt":"Intro 안녕하세요, 박민재입니다. 저번 시간에는 Table Optimization을 위한 압축 기법에 대해 배웠습니다. 이번 시간에는 압축을 제외한 Table Optimization 기법을 알아 보도록 하겠습니다. Partitioning 역시, 기존의 방법을 꺼낼 때가 왔습니다. 바로 Partitioning입니다. 동일한 Column의 동일한 Value를 가진 친구들은 같은 File로 묶어 주는 방식이죠. 어? 왜 Directory…","frontmatter":{"date":"2024-11-24","tags":["Data-Engineering"],"path":"/data-engineering/iceberg-table-optimization-2","title":"Iceberg Table의 성능 최적화 - 2. Partitioning, MOR, Others","img":"/post_image/thumbnail/iceberg-table-optimization-1.webp","summary":"File Merge를 통한 성능 최적화에 대해 알아보자."}}},{"node":{"id":"3c44d6b1-b341-5256-bb40-e6a58835b474","excerpt":"Intro 안녕하세요, 박민재입니다. 오늘은 Apache Iceberg의 Table에 수행 되는 쿼리가 최적의 성능으로 작동 될 수 있도록, File Compaction을 통해 이를 수행하는 방법에 대해 이야기 하는 시간을 가져 보도록 하겠습니다. File Compaction 우리가 쿼리를 수행 시, Hive Metastore의 정보를 이용하더라도, 혹은 Iceberg의 Metadata…","frontmatter":{"date":"2024-11-10","tags":["Data-Engineering"],"path":"/data-engineering/iceberg-table-optimization-1","title":"Iceberg Table의 성능 최적화 - 1. 압축","img":"/post_image/thumbnail/iceberg-table-optimization-1.webp","summary":"File Merge를 통한 성능 최적화에 대해 알아보자."}}}]}}}}},"staticQueryHashes":["3819017183","63159454"],"slicesMap":{}}