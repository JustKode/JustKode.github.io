{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/deep-learning/pytorch-autograd/","result":{"data":{"markdownRemark":{"html":"<h1>Pytorch로 행렬을 미분해보자, autograd</h1>\n<p>딥러닝 이론에 대해서 공부 해 보신 분들은, 딥러닝의 핵심은 <strong>미분</strong>을 통해서 **손실 함수(loss function)**의 값을 최소화 하는 것을 알고 계실 겁니다. 그렇기 때문에 <strong>Deep Learning 연구 플랫폼</strong>인 <code class=\"language-text\">Pytorch</code>에선, 당연하게도 행렬 미분을 위한 기능들을 <code class=\"language-text\">Tensor</code> 객체에 내장 시켜 놓았습니다. 한 번 알아볼까요?</p>\n<h2>autograd 활성화 시키기</h2>\n<h3>requires_grad=True</h3>\n<p>첫 번째 방법은, <code class=\"language-text\">Tensor</code>를 생성하기 위해 사용하는 함수들의 파라미터로 <code class=\"language-text\">requires_grad=True</code> 를 넘겨 주는 것 입니다.</p>\n<p><strong>입력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">x <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> requires_grad<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span></code></pre></div>\n<p><strong>출력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">tensor([[1., 1.],\n        [1., 1.]], requires_grad=True)</code></pre></div>\n<h3>Tensor.requires_grad_(True)</h3>\n<p>두 번째 방법은, 이미 생성된 <code class=\"language-text\">Tensor</code>의 멤버 함수인 <code class=\"language-text\">requires_grad_</code>를 이용하여 <code class=\"language-text\">autograd</code>를 활성화 시키는 것입니다.</p>\n<p><strong>입력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">x <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\nx<span class=\"token punctuation\">.</span>requires_grad_<span class=\"token punctuation\">(</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span></code></pre></div>\n<p><strong>출력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">tensor([[1., 1.],\n        [1., 1.]], requires_grad=True)</code></pre></div>\n<h2>역전파 시키기</h2>\n<p>**역전파 (Back Propagation)**를 시키는 방법은 생각보다 간단합니다. 최종 연산 된 <code class=\"language-text\">Tensor</code> 객체의 <code class=\"language-text\">backward()</code> 함수만 호출해 주면 됩니다.</p>\n<h3>미분한 값 알아보기.</h3>\n<p>그렇다면, <strong>역전파</strong>를 시킨 후, 미분한 값을 알기 위해선, 어떻게 하면 될까요?</p>\n<ol>\n<li><code class=\"language-text\">Tensor</code> 객체로 연산을 끝낸 후, 마지막 연산 결과로 나온 <code class=\"language-text\">Tensor</code>의 <code class=\"language-text\">backward()</code> 함수를 호출해 줍니다. <code class=\"language-text\">backward()</code> 함수의 파라미터 값으로는 d(최종결과 전 행렬)/d(최종결과 직전 행렬) 을 삽입해 줍니다. <code class=\"language-text\">backward()</code> 함수는 <a href=\"https://ko.wikipedia.org/wiki/%EC%95%BC%EC%BD%94%EB%B9%84_%ED%96%89%EB%A0%AC\">야코비안 행렬</a>과 <a href=\"https://ko.wikipedia.org/wiki/%EC%97%B0%EC%87%84_%EB%B2%95%EC%B9%99\">연쇄 법칙</a>을 이용, <strong>역전파</strong>를 통해 <strong>경사 하강법</strong>을 시도 하기 위해 사용합니다.</li>\n<li>최종 연산 결과의 <code class=\"language-text\">Tensor</code>를 최초에 <code class=\"language-text\">requires_grad=True</code>를 설정해준 <code class=\"language-text\">Tensor</code>로 미분한 값을 알고 싶다면, 최초 <code class=\"language-text\">Tensor</code> 객체의 <code class=\"language-text\">grad</code> 속성을 통해 알 수 있습니다.</li>\n</ol>\n<h3>스칼라 연산</h3>\n<p><strong>입력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">x <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> requires_grad<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 최초 Tensor 객체</span>\ny <span class=\"token operator\">=</span> x <span class=\"token operator\">+</span> <span class=\"token number\">2</span>\nz <span class=\"token operator\">=</span> y <span class=\"token operator\">*</span> y <span class=\"token operator\">*</span> <span class=\"token number\">3</span>\nout <span class=\"token operator\">=</span> z<span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 미분 대상</span>\n\nout<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># out.backward(torch.tensor(1.)) 과 동일</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>grad<span class=\"token punctuation\">)</span>   <span class=\"token comment\"># dout/dx</span></code></pre></div>\n<p><strong>출력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">tensor([[4.5000, 4.5000],\n        [4.5000, 4.5000]])</code></pre></div>\n<p><code class=\"language-text\">out = z/4 = y * y * 3/4 = (x + 2)^2 * 3 / 4</code> 이고, 이를 <strong>x</strong>에 대해서 미분하면\n<code class=\"language-text\">dout/dx = 3(2x + 4) / 4</code> 입니다. <strong>x = 1</strong>일 경우 <strong>18 / 4 = 4.5</strong> 가 정답이므로, 잘 작동 하고 있다고 볼 수 있습니다.</p>\n<p>주의: <strong>out</strong>에 한 연산은 <strong>스칼라 연산</strong>입니다. <strong>행렬 곱</strong>등을 사용하지 않았습니다.</p>\n<h3>행렬 연산</h3>\n<p><strong>입력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">x <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">,</span> requires_grad<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 최초 Tensor 객체</span>\ny <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">6</span><span class=\"token punctuation\">)</span>\nz <span class=\"token operator\">=</span> x @ y\t<span class=\"token comment\"># x @ y 행렬 곱, 더 이상 스칼라 값이 아니다.</span>\nout <span class=\"token operator\">=</span> z <span class=\"token operator\">*</span> <span class=\"token number\">2</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>out<span class=\"token punctuation\">)</span>\n\nout<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">6</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>   <span class=\"token comment\"># dout/dz</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>grad<span class=\"token punctuation\">)</span>   <span class=\"token comment\"># dout/dx</span></code></pre></div>\n<p><strong>출력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">tensor([[6., 6., 6., 6., 6., 6.],\n        [6., 6., 6., 6., 6., 6.]], grad_fn=&lt;MulBackward0&gt;)\ntensor([[24., 24., 24.],\n        [24., 24., 24.]])</code></pre></div>\n<p><strong>행렬 미분</strong>에 대해서 궁금한 점이 있다면, <a href=\"https://datascienceschool.net/view-notebook/8595892721714eb68be24727b5323778/\">해당 링크</a>의 글을 참고 해 주시기 바랍니다.</p>\n<h3>autograd 연산 멈추기</h3>\n<p><code class=\"language-text\">autograd</code> 연산과 별개로, 다른 연산을 통해서 테스트를 하고 싶을 때가 있을 수 있습니다. 이럴 때에는 두가지 방법이 있습니다.</p>\n<ol>\n<li><code class=\"language-text\">torch.no_grad()</code> 를 이용하여 연산하기.</li>\n<li><code class=\"language-text\">Tensor.detech()</code> 를 이용하여, <code class=\"language-text\">autograd</code> 연산을 하지 않은 <code class=\"language-text\">Tensor</code> 복사하기.</li>\n</ol>\n<p><strong>입력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">x <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> requires_grad<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>requires_grad<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>x <span class=\"token operator\">**</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>requires_grad<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">with</span> torch<span class=\"token punctuation\">.</span>no_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\t<span class=\"token comment\"># autograd 연산 생략</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>x <span class=\"token operator\">**</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>requires_grad<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">''</span><span class=\"token punctuation\">)</span>\n\ny <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>detach<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\t\t\t<span class=\"token comment\"># autograd가 없이 내용물만 복사.</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">.</span>requires_grad<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>eq<span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">all</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># x와 y의 내용물은 같다.</span></code></pre></div>\n<p><strong>출력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">True\nTrue\nFalse\n\nFalse\ntensor(True)</code></pre></div>\n<h2>마치며</h2>\n<p>다음 시간에는, <strong>신경망 학습</strong>을 위한, <code class=\"language-text\">torch.nn</code> 모듈에 대해서 알아보겠습니다. 잘못된 내용이 있다면, 이메일 <strong><a href=\"mailto:justkode@kakao.com\">justkode@kakao.com</a></strong> 이나, 댓글을 통해 알려주세요!</p>\n<h3>Reference</h3>\n<ul>\n<li><a href=\"https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\">Pytorch Tutorial</a></li>\n</ul>","id":"acfc397a-afcb-58f3-9c3b-50018d10577e","frontmatter":{"date":"2020-03-28","path":"/deep-learning/pytorch-autograd","title":"Pytorch로 행렬을 미분해보자, autograd","tags":["Deep-Learning","Python"],"keyword":"Python, python, 파이썬, Pytorch, 파이토치, pytorch, 딥러닝, autograd","summary":"Pytorch로 행렬을 미분하는 방법","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png","series":"Pytorch Basic"}}},"pageContext":{"postPath":"/deep-learning/pytorch-autograd","series":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"dce11971-6f51-5362-93e4-f590d01fa449","excerpt":"에서 모델의 가중치를 저장하기 위해선 3가지 함수만 알면 충분 합니다. : 객체를 디스크에 저장합니다.  모듈을 이용하여 객체를 직렬화 하며, 이 함수를 사용하여 모든 종류의 모델, Tensor 등을 저장할 수 있습니다. :  모듈을 이용하여 객체를 역직렬화하여 메모리에 할당합니다. : 역직렬화된 state_dict를 사용, 모델의 매개변수들을 불러옵니다. state_dict는 간단히 말해 각 체층을 매개변수 Tensor로 매핑한 Python…","frontmatter":{"date":"2020-04-26","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-save","title":"Pytorch에서 학습한 모델 저장 및 불러오기","img":"/post_image/pytorch-save.png","summary":"Pytorch 모델을 저장하고, 불러와 보기"}}},{"node":{"id":"0665a18f-3c7f-58c4-ac24-a41c6cb4298a","excerpt":"에서는 과 마찬가지로, 과 관련 된 를 제공합니다. 이를 이용해 손쉽게  네트워크를 구축 할 수 있습니다. Recurrent Neural Network 를 위한 는  입니다. 일단  시퀀스의 각 요소에 대해, 각 레이어에서는 다음 연산을 수행합니다.  Parameters : 의 사이즈에 해당 하는 수를 입력하면 됩니다. : 은닉층의 사이즈에 해당 하는 수를 입력하면 됩니다. : 의 은닉층 레이어 갯수를 나타냅니다. 기본 값은…","frontmatter":{"date":"2020-04-19","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-rnn","title":"Pytorch로 RNN, LSTM 구현하기","img":"https://lionbridge.ai/wp-content/uploads/2020/03/2020-02-21_difference-between-cnn-rnn-1.png","summary":"자연어 처리, 감성 분류 등에 사용되는 RNN, LSTM"}}},{"node":{"id":"c7af02ab-61b0-56d3-a5a4-b21b7fc7dac7","excerpt":"CNN In Pytorch 에는 을 개발 하기 위한 들이 있습니다. 다채널로 구현 되어 있는 CNN 신경망을 위한 Layers, Max pooling, Avg pooling 등, 이번 시간에는 여러 가지 을 위한 를 알아 보겠습니다. 또한,  데이터 또한 학습 해 보겠습니다. Convolution Layers  연산을 위한 레이어들은 다음과 같습니다. Conv1d (Text-CNN에서 많이 사용) Conv2d…","frontmatter":{"date":"2020-04-08","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-cnn","title":"Pytorch로 CNN 구현하기","img":"/post_image/MnistExamples.png","summary":"Pytorch로 MNIST 그림 식별을 해보자."}}},{"node":{"id":"8813fbea-c362-527f-b9ea-c5df537d2b94","excerpt":"Neural Network in Pytorch 딥 러닝 프레임워크인 에선 신경망 구축을 위한 API…","frontmatter":{"date":"2020-04-04","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-nn","title":"Pytorch의 Neural Network","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png","summary":"Pytorch에서 신경망 구현하기"}}},{"node":{"id":"acfc397a-afcb-58f3-9c3b-50018d10577e","excerpt":"Pytorch로 행렬을 미분해보자, autograd 딥러닝 이론에 대해서 공부 해 보신 분들은, 딥러닝의 핵심은 미분을 통해서 **손실 함수(loss function)**의 값을 최소화 하는 것을 알고 계실 겁니다. 그렇기 때문에 Deep Learning 연구 플랫폼인 에선, 당연하게도 행렬 미분을 위한 기능들을  객체에 내장 시켜 놓았습니다. 한 번 알아볼까요? autograd 활성화 시키기 requires_grad=True…","frontmatter":{"date":"2020-03-28","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-autograd","title":"Pytorch로 행렬을 미분해보자, autograd","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png","summary":"Pytorch로 행렬을 미분하는 방법"}}},{"node":{"id":"5f70505d-ab23-5437-bb1e-121fcb5e4077","excerpt":"주의 필자인 저 또한 배운 것들을 정리 하면서 쓰는 글이기 때문에, 틀린 부분이 있을 수도 있습니다. 또한, 이 글은 에 대한 기본 적인 배경 지식을 필요로 합니다. 오타, 지적사항 발생 시, 댓글 혹은 이메일로 남겨 주시면 감사하겠습니다! Pytorch Pytorch는 두 가지 목표를 달성하기 위해 만들어진 오픈 소스 라이브러리이자, Python Package입니다. 두 가지 목표는 다음과 같습니다. Numpy가 기존에 하던 연산들을 GPU…","frontmatter":{"date":"2020-03-23","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-basic","title":"Pytorch에 대해서 알아보자.araboza","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png","summary":"딥러닝 플랫폼인 Pytorch에 대하여"}}}]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"27025625-955c-59f8-a503-8fed1476a811","excerpt":"Regularization 안녕하세요? 이번 시간에는 Regularization에 대해서 알아 보도록 하겠습니다. 우리가 모델을 만들 때, 많은 분들이 Overfitting을 경험 해 보셨을 것 입니다. 여러분들은 Overfitting을 경험할 때, 다음과 같은 것들을 시도 해 볼 것입니다. 모델의 크기를 줄인다. (네트워크의 깊이, 노드 갯수 등등) 데이터의 크기를 늘린다. 하지만, 이 외에도 다른 방법들이 있습니다. DropOut…","frontmatter":{"date":"2021-08-31","tags":["Data-Science","Deep-Learning"],"path":"/data-science/regularization","title":"Regularization: 모델의 과적합을 막는 방법","img":"/post_image/thumbnail/regularization.jpg","summary":"모델의 과적합을 막는 Regularization에 대해서 알아 보자."}}},{"node":{"id":"eb1ceaed-6ecf-5f3b-92fb-8127695445ff","excerpt":"Generative Adversarial Network 안녕하세요? 오늘은 GAN, Generative Adversarial Network에 대해서 알아 보도록 하겠습니다. Generative Adversarial Network…","frontmatter":{"date":"2021-08-07","tags":["Data-Science","Deep-Learning"],"path":"/data-science/just-data-science-12","title":"[찍먹 Data Science] 12. Generative Adversarial Network","img":"/post_image/thumbnail/just-data-science-12.jpg","summary":"서로가 적대적으로 학습하는 GAN에 대해서 알아 보자."}}},{"node":{"id":"db153076-f16f-5986-b197-dba957f79e77","excerpt":"Recurrent Neural Network 안녕하세요? 오늘은 RNN, Recurrent Neural Network에 대해서 알아 보도록 하겠습니다. RNN…","frontmatter":{"date":"2021-08-05","tags":["Data-Science","Deep-Learning"],"path":"/data-science/just-data-science-11","title":"[찍먹 Data Science] 11. Recurrent Neural Network","img":"/post_image/thumbnail/just-data-science-11.jpg","summary":"시계열 데이터를 처리하는 RNN을 알아보자."}}},{"node":{"id":"7dd4bd42-52b6-5870-a66d-2d948e672b26","excerpt":"Convolutional Neural Network 안녕하세요? 오늘은 CNN, Convolutional Neural Network에 대해서 알아 보도록 하겠습니다. 저번 시간에는 DNN…","frontmatter":{"date":"2021-07-31","tags":["Data-Science","Deep-Learning"],"path":"/data-science/just-data-science-10","title":"[찍먹 Data Science] 10. Convolutional Neural Network","img":"/post_image/thumbnail/just-data-science-10.jpg","summary":"국소적인 정보를 추출하는 CNN에 대해 알아 보자."}}}]}}}}},"staticQueryHashes":["2876327880","63159454"],"slicesMap":{}}