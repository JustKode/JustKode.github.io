{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/deep-learning/pytorch-autograd","result":{"data":{"markdownRemark":{"html":"<h1>Pytorch로 행렬을 미분해보자, autograd</h1>\n<p>딥러닝 이론에 대해서 공부 해 보신 분들은, 딥러닝의 핵심은 <strong>미분</strong>을 통해서 <strong>손실 함수(loss function)</strong>의 값을 최소화 하는 것 입니다. 그렇기 때문에 <strong>Deep Learning 연구 플랫폼</strong>인 <code class=\"language-text\">Pytorch</code>에선, 당연하게도 행렬 미분을 위한 기능들을 <code class=\"language-text\">Tensor</code> 객체에 내장 시켜 놓았습니다. 한 번 알아볼까요?</p>\n<h2>autograd 활성화 시키기</h2>\n<h3>requires_grad=True</h3>\n<p>첫 번째 방법은, <code class=\"language-text\">Tensor</code>를 생성하기 위해 사용하는 함수들의 파라미터로 <code class=\"language-text\">requires_grad=True</code> 를 넘겨 주는 것 입니다.</p>\n<p><strong>입력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">x <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> requires_grad<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span></code></pre></div>\n<p><strong>출력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">tensor([[1., 1.],\n        [1., 1.]], requires_grad=True)</code></pre></div>\n<h3>Tensor.requires_grad_(True)</h3>\n<p>두 번째 방법은, 이미 생성된 <code class=\"language-text\">Tensor</code>의 멤버 함수인 <code class=\"language-text\">requires_grad_</code>를 이용하여 <code class=\"language-text\">autograd</code>를 활성화 시키는 것입니다.</p>\n<p><strong>입력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">x <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\nx<span class=\"token punctuation\">.</span>requires_grad_<span class=\"token punctuation\">(</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span></code></pre></div>\n<p><strong>출력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">tensor([[1., 1.],\n        [1., 1.]], requires_grad=True)</code></pre></div>\n<h2>역전파 시키기</h2>\n<p><strong>역전파 (Back Propagation)</strong>를 시키는 방법은 생각보다 간단합니다. 최종 연산 된 <code class=\"language-text\">Tensor</code> 객체의 <code class=\"language-text\">backward()</code> 함수만 호출해 주면 됩니다.</p>\n<h3>미분한 값 알아보기.</h3>\n<p>그렇다면, <strong>역전파</strong>를 시킨 후, 미분한 값을 알기 위해선, 어떻게 하면 될까요?</p>\n<ol>\n<li><code class=\"language-text\">Tensor</code> 객체로 연산을 끝낸 후, 마지막 연산 결과로 나온 <code class=\"language-text\">Tensor</code>의 <code class=\"language-text\">backward()</code> 함수를 호출해 줍니다. <code class=\"language-text\">backward()</code> 함수의 파라미터 값으로는 d(최종결과 전 행렬)/d(최종결과 직전 행렬) 을 삽입해 줍니다. <code class=\"language-text\">backward()</code> 함수는 <a href=\"https://ko.wikipedia.org/wiki/%EC%95%BC%EC%BD%94%EB%B9%84_%ED%96%89%EB%A0%AC\">야코비안 행렬</a>과 <a href=\"https://ko.wikipedia.org/wiki/%EC%97%B0%EC%87%84_%EB%B2%95%EC%B9%99\">연쇄 법칙</a>을 이용, <strong>역전파</strong>를 통해 <strong>경사 하강법</strong>을 시도 하기 위해 사용합니다. </li>\n<li>최종 연산 결과의 <code class=\"language-text\">Tensor</code>를 최초에 <code class=\"language-text\">requires_grad=True</code>를 설정해준 <code class=\"language-text\">Tensor</code>로 미분한 값을 알고 싶다면, 최초 <code class=\"language-text\">Tensor</code> 객체의 <code class=\"language-text\">grad</code> 속성을 통해 알 수 있습니다.</li>\n</ol>\n<h3>스칼라 연산</h3>\n<p><strong>입력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">x <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> requires_grad<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 최초 Tensor 객체</span>\ny <span class=\"token operator\">=</span> x <span class=\"token operator\">+</span> <span class=\"token number\">2</span>\nz <span class=\"token operator\">=</span> y <span class=\"token operator\">*</span> y <span class=\"token operator\">*</span> <span class=\"token number\">3</span>\nout <span class=\"token operator\">=</span> z<span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 미분 대상</span>\n\nout<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># out.backward(torch.tensor(1.)) 과 동일</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>grad<span class=\"token punctuation\">)</span>   <span class=\"token comment\"># dout/dx</span></code></pre></div>\n<p><strong>출력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">tensor([[4.5000, 4.5000],\n        [4.5000, 4.5000]])</code></pre></div>\n<p><code class=\"language-text\">out = z/4 = y * y * 3/4 = (x + 2)^2 * 3 / 4</code> 이고, 이를 <strong>x</strong>에 대해서 미분하면\n<code class=\"language-text\">dout/dx = 3(2x + 4) / 4</code> 입니다. <strong>x = 1</strong>일 경우 <strong>18 / 4 = 4.5</strong> 가 정답이므로, 잘 작동 하고 있다고 볼 수 있습니다.</p>\n<p>주의: <strong>out</strong>에 한 연산은 <strong>스칼라 연산</strong>입니다. <strong>행렬 곱</strong>등을 사용하지 않았습니다.</p>\n<h3>행렬 연산</h3>\n<p><strong>입력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">x <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">,</span> requires_grad<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 최초 Tensor 객체</span>\ny <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">6</span><span class=\"token punctuation\">)</span>\nz <span class=\"token operator\">=</span> x @ y\t<span class=\"token comment\"># x @ y 행렬 곱, 더 이상 스칼라 값이 아니다.</span>\nout <span class=\"token operator\">=</span> z <span class=\"token operator\">*</span> <span class=\"token number\">2</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>out<span class=\"token punctuation\">)</span>\n\nout<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">6</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>   <span class=\"token comment\"># dout/dz</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>grad<span class=\"token punctuation\">)</span>   <span class=\"token comment\"># dout/dx</span></code></pre></div>\n<p><strong>출력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">tensor([[6., 6., 6., 6., 6., 6.],\n        [6., 6., 6., 6., 6., 6.]], grad_fn=&lt;MulBackward0&gt;)\ntensor([[24., 24., 24.],\n        [24., 24., 24.]])</code></pre></div>\n<p><strong>행렬 미분</strong>에 대해서 궁금한 점이 있다면, <a href=\"https://datascienceschool.net/view-notebook/8595892721714eb68be24727b5323778/\">해당 링크</a>의 글을 참고 해 주시기 바랍니다.</p>\n<h3>autograd 연산 멈추기</h3>\n<p><code class=\"language-text\">autograd</code> 연산과 별개로, 다른 연산을 통해서 테스트를 하고 싶을 때가 있을 수 있습니다. 이럴 때에는 두가지 방법이 있습니다.</p>\n<ol>\n<li><code class=\"language-text\">torch.no_grad()</code> 를 이용하여 연산하기.</li>\n<li><code class=\"language-text\">Tensor.detech()</code> 를 이용하여, <code class=\"language-text\">autograd</code> 연산을 하지 않은 <code class=\"language-text\">Tensor</code> 복사하기.</li>\n</ol>\n<p><strong>입력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">x <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> requires_grad<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>requires_grad<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>x <span class=\"token operator\">**</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>requires_grad<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">with</span> torch<span class=\"token punctuation\">.</span>no_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\t<span class=\"token comment\"># autograd 연산 생략</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>x <span class=\"token operator\">**</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>requires_grad<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">''</span><span class=\"token punctuation\">)</span>\n\ny <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>detach<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\t\t\t<span class=\"token comment\"># autograd가 없이 내용물만 복사.</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">.</span>requires_grad<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>eq<span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">all</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># x와 y의 내용물은 같다.</span></code></pre></div>\n<p><strong>출력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">True\nTrue\nFalse\n\nFalse\ntensor(True)</code></pre></div>\n<h2>마치며</h2>\n<p>다음 시간에는, <strong>신경망 학습</strong>을 위한, <code class=\"language-text\">torch.nn</code> 모듈에 대해서 알아보겠습니다. 잘못된 내용이 있다면, 이메일 <strong>justkode@kakao.com</strong> 이나, 댓글을 통해 알려주세요!</p>\n<h3>Reference</h3>\n<ul>\n<li><a href=\"https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\">Pytorch Tutorial</a></li>\n</ul>","id":"18eeab8b-863e-5ce3-b980-5e9e957c180a","frontmatter":{"date":"2020-03-28","path":"/deep-learning/pytorch-autograd","title":"Pytorch로 행렬을 미분해보자, autograd","tags":["Deep-Learning","Python"],"keyword":"Python, python, 파이썬, Pytorch, 파이토치, pytorch, 딥러닝, autograd","summary":"Pytorch로 행렬을 미분하는 방법","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png","series":"Pytorch Basic"}}},"pageContext":{"series":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"3e5a78d9-ea4d-5a77-a082-281403358a61","excerpt":"에서 모델의 가중치를 저장하기 위해선 3가지 함수만 알면 충분 합니다. : 객체를 디스크에 저장합니다.  모듈을 이용하여 객체를 직렬화 하며, 이 함수를 사용하여 모든 종류의 모델, Tensor 등을 저장할 수 있습니다. :  모듈을 이용하여 객체를 역직렬화하여 메모리에 할당합니다. : 역직렬화된 state_dict를 사용, 모델의 매개변수들을 불러옵니다. state_dict는 간단히 말해 각 체층을 매개변수 Tensor로 매핑한 Python…","frontmatter":{"date":"2020-04-26","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-save","title":"Pytorch에서 학습한 모델 저장 및 불러오기","img":"/post_image/pytorch-save.png","summary":"Pytorch 모델을 저장하고, 불러와 보기"}}},{"node":{"id":"ce0ab714-91d8-5592-afc1-72bb2a4c56c4","excerpt":"에서는 과 마찬가지로, 과 관련 된 를 제공합니다. 이를 이용해 손쉽게  네트워크를 구축 할 수 있습니다. Recurrent Neural Network 를 위한 는  입니다. 일단  시퀀스의 각 요소에 대해, 각 레이어에서는 다음 연산을 수행합니다.  Parameters : 의 사이즈에 해당 하는 수를 입력하면 됩니다. : 은닉층의 사이즈에 해당 하는 수를 입력하면 됩니다. : 의 은닉층 레이어 갯수를 나타냅니다. 기본 값은…","frontmatter":{"date":"2020-04-19","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-rnn","title":"Pytorch로 RNN, LSTM 구현하기","img":"https://lionbridge.ai/wp-content/uploads/2020/03/2020-02-21_difference-between-cnn-rnn-1.png","summary":"자연어 처리, 감성 분류 등에 사용되는 RNN, LSTM"}}},{"node":{"id":"ddf2f489-870b-5d07-a72f-68cefd3da0c4","excerpt":"CNN In Pytorch 에는 을 개발 하기 위한 들이 있습니다. 다채널로 구현 되어 있는 CNN 신경망을 위한 Layers, Max pooling, Avg pooling 등, 이번 시간에는 여러 가지 을 위한 를 알아 보겠습니다. 또한,  데이터 또한 학습 해 보겠습니다. Convolution Layers  연산을 위한 레이어들은 다음과 같습니다. Conv1d (Text-CNN에서 많이 사용) Conv2d…","frontmatter":{"date":"2020-04-08","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-cnn","title":"Pytorch로 CNN 구현하기","img":"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/02/Plot-of-a-Subset-of-Images-from-the-MNIST-Dataset-1024x768.png","summary":"Pytorch로 MNIST 그림 식별을 해보자."}}},{"node":{"id":"5b78f1ab-69ff-56fb-b9d3-f76b6905f5a1","excerpt":"Neural Network in Pytorch 딥 러닝 프레임워크인 에선 신경망 구축을 위한 API…","frontmatter":{"date":"2020-04-04","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-nn","title":"Pytorch의 Neural Network","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png","summary":"Pytorch에서 신경망 구현하기"}}},{"node":{"id":"18eeab8b-863e-5ce3-b980-5e9e957c180a","excerpt":"Pytorch로 행렬을 미분해보자, autograd 딥러닝 이론에 대해서 공부 해 보신 분들은, 딥러닝의 핵심은 미분을 통해서 손실 함수(loss function)의 값을 최소화 하는 것 입니다. 그렇기 때문에 Deep Learning 연구 플랫폼인 에선, 당연하게도 행렬 미분을 위한 기능들을  객체에 내장 시켜 놓았습니다. 한 번 알아볼까요? autograd 활성화 시키기 requires_grad=True…","frontmatter":{"date":"2020-03-28","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-autograd","title":"Pytorch로 행렬을 미분해보자, autograd","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png","summary":"Pytorch로 행렬을 미분하는 방법"}}},{"node":{"id":"d3d26a0d-b647-5094-816f-d4c7a9c8e5d9","excerpt":"주의 필자인 저 또한 배운 것들을 정리 하면서 쓰는 글이기 때문에, 틀린 부분이 있을 수도 있습니다. 또한, 이 글은 에 대한 기본 적인 배경 지식을 필요로 합니다. 오타, 지적사항 발생 시, 댓글 혹은 이메일로 남겨 주시면 감사하겠습니다! Pytorch Pytorch는 두 가지 목표를 달성하기 위해 만들어진 오픈 소스 라이브러리이자, Python Package입니다. 두 가지 목표는 다음과 같습니다. Numpy가 기존에 하던 연산들을 GPU…","frontmatter":{"date":"2020-03-23","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-basic","title":"Pytorch에 대해서 알아보자.araboza","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png","summary":"딥러닝 플랫폼인 Pytorch에 대하여"}}}]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"3e5a78d9-ea4d-5a77-a082-281403358a61","excerpt":"에서 모델의 가중치를 저장하기 위해선 3가지 함수만 알면 충분 합니다. : 객체를 디스크에 저장합니다.  모듈을 이용하여 객체를 직렬화 하며, 이 함수를 사용하여 모든 종류의 모델, Tensor 등을 저장할 수 있습니다. :  모듈을 이용하여 객체를 역직렬화하여 메모리에 할당합니다. : 역직렬화된 state_dict를 사용, 모델의 매개변수들을 불러옵니다. state_dict는 간단히 말해 각 체층을 매개변수 Tensor로 매핑한 Python…","frontmatter":{"date":"2020-04-26","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-save","title":"Pytorch에서 학습한 모델 저장 및 불러오기","img":"/post_image/pytorch-save.png","summary":"Pytorch 모델을 저장하고, 불러와 보기"}}},{"node":{"id":"ce0ab714-91d8-5592-afc1-72bb2a4c56c4","excerpt":"에서는 과 마찬가지로, 과 관련 된 를 제공합니다. 이를 이용해 손쉽게  네트워크를 구축 할 수 있습니다. Recurrent Neural Network 를 위한 는  입니다. 일단  시퀀스의 각 요소에 대해, 각 레이어에서는 다음 연산을 수행합니다.  Parameters : 의 사이즈에 해당 하는 수를 입력하면 됩니다. : 은닉층의 사이즈에 해당 하는 수를 입력하면 됩니다. : 의 은닉층 레이어 갯수를 나타냅니다. 기본 값은…","frontmatter":{"date":"2020-04-19","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-rnn","title":"Pytorch로 RNN, LSTM 구현하기","img":"https://lionbridge.ai/wp-content/uploads/2020/03/2020-02-21_difference-between-cnn-rnn-1.png","summary":"자연어 처리, 감성 분류 등에 사용되는 RNN, LSTM"}}},{"node":{"id":"ddf2f489-870b-5d07-a72f-68cefd3da0c4","excerpt":"CNN In Pytorch 에는 을 개발 하기 위한 들이 있습니다. 다채널로 구현 되어 있는 CNN 신경망을 위한 Layers, Max pooling, Avg pooling 등, 이번 시간에는 여러 가지 을 위한 를 알아 보겠습니다. 또한,  데이터 또한 학습 해 보겠습니다. Convolution Layers  연산을 위한 레이어들은 다음과 같습니다. Conv1d (Text-CNN에서 많이 사용) Conv2d…","frontmatter":{"date":"2020-04-08","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-cnn","title":"Pytorch로 CNN 구현하기","img":"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/02/Plot-of-a-Subset-of-Images-from-the-MNIST-Dataset-1024x768.png","summary":"Pytorch로 MNIST 그림 식별을 해보자."}}},{"node":{"id":"5b78f1ab-69ff-56fb-b9d3-f76b6905f5a1","excerpt":"Neural Network in Pytorch 딥 러닝 프레임워크인 에선 신경망 구축을 위한 API…","frontmatter":{"date":"2020-04-04","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-nn","title":"Pytorch의 Neural Network","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png","summary":"Pytorch에서 신경망 구현하기"}}}]}}}}}}