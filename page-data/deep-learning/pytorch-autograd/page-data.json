{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/deep-learning/pytorch-autograd","result":{"data":{"markdownRemark":{"html":"<h1>Pytorch로 행렬을 미분해보자, autograd</h1>\n<p>딥러닝 이론에 대해서 공부 해 보신 분들은, 딥러닝의 핵심은 <strong>미분</strong>을 통해서 <strong>손실 함수(loss function)</strong>의 값을 최소화 하는 것 입니다. 그렇기 때문에 <strong>Deep Learning 연구 플랫폼</strong>인 <code class=\"language-text\">Pytorch</code>에선, 당연하게도 행렬 미분을 위한 기능들을 <code class=\"language-text\">Tensor</code> 객체에 내장 시켜 놓았습니다. 한 번 알아볼까요?</p>\n<h2>autograd 활성화 시키기</h2>\n<h3>requires_grad=True</h3>\n<p>첫 번째 방법은, <code class=\"language-text\">Tensor</code>를 생성하기 위해 사용하는 함수들의 파라미터로 <code class=\"language-text\">requires_grad=True</code> 를 넘겨 주는 것 입니다.</p>\n<p><strong>입력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">x <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> requires_grad<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span></code></pre></div>\n<p><strong>출력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">tensor([[1., 1.],\n        [1., 1.]], requires_grad=True)</code></pre></div>\n<h3>Tensor.requires_grad_(True)</h3>\n<p>두 번째 방법은, 이미 생성된 <code class=\"language-text\">Tensor</code>의 멤버 함수인 <code class=\"language-text\">requires_grad_</code>를 이용하여 <code class=\"language-text\">autograd</code>를 활성화 시키는 것입니다.</p>\n<p><strong>입력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">x <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\nx<span class=\"token punctuation\">.</span>requires_grad_<span class=\"token punctuation\">(</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span></code></pre></div>\n<p><strong>출력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">tensor([[1., 1.],\n        [1., 1.]], requires_grad=True)</code></pre></div>\n<h2>역전파 시키기</h2>\n<p><strong>역전파 (Back Propagation)</strong>를 시키는 방법은 생각보다 간단합니다. 최종 연산 된 <code class=\"language-text\">Tensor</code> 객체의 <code class=\"language-text\">backward()</code> 함수만 호출해 주면 됩니다.</p>\n<h3>미분한 값 알아보기.</h3>\n<p>그렇다면, <strong>역전파</strong>를 시킨 후, 미분한 값을 알기 위해선, 어떻게 하면 될까요?</p>\n<ol>\n<li><code class=\"language-text\">Tensor</code> 객체로 연산을 끝낸 후, 마지막 연산 결과로 나온 <code class=\"language-text\">Tensor</code>의 <code class=\"language-text\">backward()</code> 함수를 호출해 줍니다. <code class=\"language-text\">backward()</code> 함수의 파라미터 값으로는 d(최종결과 전 행렬)/d(최종결과 직전 행렬) 을 삽입해 줍니다. <code class=\"language-text\">backward()</code> 함수는 <a href=\"https://ko.wikipedia.org/wiki/%EC%95%BC%EC%BD%94%EB%B9%84_%ED%96%89%EB%A0%AC\">야코비안 행렬</a>과 <a href=\"https://ko.wikipedia.org/wiki/%EC%97%B0%EC%87%84_%EB%B2%95%EC%B9%99\">연쇄 법칙</a>을 이용, <strong>역전파</strong>를 통해 <strong>경사 하강법</strong>을 시도 하기 위해 사용합니다. </li>\n<li>최종 연산 결과의 <code class=\"language-text\">Tensor</code>를 최초에 <code class=\"language-text\">requires_grad=True</code>를 설정해준 <code class=\"language-text\">Tensor</code>로 미분한 값을 알고 싶다면, 최초 <code class=\"language-text\">Tensor</code> 객체의 <code class=\"language-text\">grad</code> 속성을 통해 알 수 있습니다.</li>\n</ol>\n<h3>스칼라 연산</h3>\n<p><strong>입력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">x <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> requires_grad<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 최초 Tensor 객체</span>\ny <span class=\"token operator\">=</span> x <span class=\"token operator\">+</span> <span class=\"token number\">2</span>\nz <span class=\"token operator\">=</span> y <span class=\"token operator\">*</span> y <span class=\"token operator\">*</span> <span class=\"token number\">3</span>\nout <span class=\"token operator\">=</span> z<span class=\"token punctuation\">.</span>mean<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 미분 대상</span>\n\nout<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># out.backward(torch.tensor(1.)) 과 동일</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>grad<span class=\"token punctuation\">)</span>   <span class=\"token comment\"># dout/dx</span></code></pre></div>\n<p><strong>출력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">tensor([[4.5000, 4.5000],\n        [4.5000, 4.5000]])</code></pre></div>\n<p><code class=\"language-text\">out = z/4 = y * y * 3/4 = (x + 2)^2 * 3 / 4</code> 이고, 이를 <strong>x</strong>에 대해서 미분하면\n<code class=\"language-text\">dout/dx = 3(2x + 4) / 4</code> 입니다. <strong>x = 1</strong>일 경우 <strong>18 / 4 = 4.5</strong> 가 정답이므로, 잘 작동 하고 있다고 볼 수 있습니다.</p>\n<p>주의: <strong>out</strong>에 한 연산은 <strong>스칼라 연산</strong>입니다. <strong>행렬 곱</strong>등을 사용하지 않았습니다.</p>\n<h3>행렬 연산</h3>\n<p><strong>입력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">x <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">,</span> requires_grad<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 최초 Tensor 객체</span>\ny <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">6</span><span class=\"token punctuation\">)</span>\nz <span class=\"token operator\">=</span> x @ y\t<span class=\"token comment\"># x @ y 행렬 곱, 더 이상 스칼라 값이 아니다.</span>\nout <span class=\"token operator\">=</span> z <span class=\"token operator\">*</span> <span class=\"token number\">2</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>out<span class=\"token punctuation\">)</span>\n\nout<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">6</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>   <span class=\"token comment\"># dout/dz</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>grad<span class=\"token punctuation\">)</span>   <span class=\"token comment\"># dout/dx</span></code></pre></div>\n<p><strong>출력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">tensor([[6., 6., 6., 6., 6., 6.],\n        [6., 6., 6., 6., 6., 6.]], grad_fn=&lt;MulBackward0&gt;)\ntensor([[24., 24., 24.],\n        [24., 24., 24.]])</code></pre></div>\n<p><strong>행렬 미분</strong>에 대해서 궁금한 점이 있다면, <a href=\"https://datascienceschool.net/view-notebook/8595892721714eb68be24727b5323778/\">해당 링크</a>의 글을 참고 해 주시기 바랍니다.</p>\n<h3>autograd 연산 멈추기</h3>\n<p><code class=\"language-text\">autograd</code> 연산과 별개로, 다른 연산을 통해서 테스트를 하고 싶을 때가 있을 수 있습니다. 이럴 때에는 두가지 방법이 있습니다.</p>\n<ol>\n<li><code class=\"language-text\">torch.no_grad()</code> 를 이용하여 연산하기.</li>\n<li><code class=\"language-text\">Tensor.detech()</code> 를 이용하여, <code class=\"language-text\">autograd</code> 연산을 하지 않은 <code class=\"language-text\">Tensor</code> 복사하기.</li>\n</ol>\n<p><strong>입력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">x <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> requires_grad<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>requires_grad<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>x <span class=\"token operator\">**</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>requires_grad<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">with</span> torch<span class=\"token punctuation\">.</span>no_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\t<span class=\"token comment\"># autograd 연산 생략</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>x <span class=\"token operator\">**</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>requires_grad<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">''</span><span class=\"token punctuation\">)</span>\n\ny <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>detach<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\t\t\t<span class=\"token comment\"># autograd가 없이 내용물만 복사.</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">.</span>requires_grad<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">.</span>eq<span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">all</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># x와 y의 내용물은 같다.</span></code></pre></div>\n<p><strong>출력</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">True\nTrue\nFalse\n\nFalse\ntensor(True)</code></pre></div>\n<h2>마치며</h2>\n<p>다음 시간에는, <strong>신경망 학습</strong>을 위한, <code class=\"language-text\">torch.nn</code> 모듈에 대해서 알아보겠습니다. 잘못된 내용이 있다면, 이메일 <strong>justkode@kakao.com</strong> 이나, 댓글을 통해 알려주세요!</p>","id":"18eeab8b-863e-5ce3-b980-5e9e957c180a","frontmatter":{"date":"2020-03-28","path":"/deep-learning/pytorch-autograd","title":"Pytorch로 행렬을 미분해보자, autograd","tags":["Deep-Learning","Python"],"keyword":"Python, python, 파이썬, Pytorch, 파이토치, pytorch, 딥러닝, autograd","summary":"Pytorch로 행렬을 미분하는 방법","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png"}}},"pageContext":{}}}