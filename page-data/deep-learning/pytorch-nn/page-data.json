{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/deep-learning/pytorch-nn","result":{"data":{"markdownRemark":{"html":"<h1>Neural Network in Pytorch</h1>\n<p>딥 러닝 프레임워크인 <code class=\"language-text\">Pytorch</code>에선 <strong>신경망 구축을 위한 API</strong>를 제공합니다. 이는 <code class=\"language-text\">torch.nn</code> 패키지를 <code class=\"language-text\">import</code> 하여 사용 가능합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn <span class=\"token keyword\">as</span> nn\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>functional <span class=\"token keyword\">as</span> F</code></pre></div>\n<br>\n<p>이번 시간의 <strong>목차</strong>는 다음과 같습니다.</p>\n<ul>\n<li><strong>신경망 클래스 선언</strong>하기</li>\n<li><strong>신경망 클래스의 멤버 변수, 멤버 함수</strong> 만들기</li>\n<li><strong>신경망 학습</strong>시키기</li>\n<li><strong>신경망 파라미터 확인</strong>하기</li>\n</ul>\n<h2>신경망 클래스 선언하기</h2>\n<p><strong>신경망 클래스</strong>를 선언하는 방법은 <code class=\"language-text\">nn.Module</code>을 <strong>상속</strong>받음으로써 가능합니다. 이는 <strong>신경망 클래스</strong>의 기본이 되는 클래스 입니다. 사용 방법은 다음과 같습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">DNN</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span>DNN<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n  <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> x</code></pre></div>\n<p><code class=\"language-text\">super(클래스명, self).__init__()</code>은 무조건 <strong>클래스 생성자</strong>에 입력 해 주어야 합니다.</p>\n<h2>신경망 클래스의 멤버 변수, 멤버 함수 만들기</h2>\n<p>신경망 클래스의 <strong>멤버 변수, 멤버 함수</strong>를 선언 하기 전에, 어떻게 신경망을 설계를 할 것인지 먼저 정해야 합니다.</p>\n<ul>\n<li><strong>가중치의 크기</strong>는 어느 정도로 할 것인지?</li>\n<li>신경망을 <strong>몇 층으로 설계</strong> 할 것인지?</li>\n<li><strong>Dropout</strong>은 몇 퍼센트로 할 것인지?</li>\n<li><strong>어떤 방식</strong>을 사용 하는 신경망인지? (CNN, RNN 등)</li>\n<li>기타 등등...</li>\n</ul>\n<p>이에 따라서 <strong>신경망 클래스</strong>의 <strong>멤버 변수</strong>를 어떻게 설정 할 것인지는 천지차이로 달라 질 수 있습니다.</p>\n<h3>상황 설정</h3>\n<p>일단 털, 날개의 유무를 통해 종을 분류하는 <strong>신경망</strong>을 만든다고 가정 해 보겠습니다. 그렇다면 <code class=\"language-text\">input</code>의 size는 (n, 2) 가 될 것입니다. 그리고 종의 종류가 3개라고 가정 하면, 최종 <code class=\"language-text\">output</code>은   <code class=\"language-text\">softmax</code> 함수를 사용하여 분류된 세가지 값 중에 하나 일 것입니다. 또한, <strong>은닉층을 한 개</strong> 놓는다고 가정하고, 활성화 함수로는 <strong>ReLU</strong>를 사용 하겠습니다. 그리고 <strong>Dropout</strong>을 이용하여, <strong>Overfitting</strong> 현상을 막아 보겠습니다. 대충 과정을 요약하면 이렇게 되겠군요.</p>\n<blockquote>\n<p>input -> affine1 -> ReLU -> dropout -> affine2 -> softmax</p>\n</blockquote>\n<p>한 번 구현해 볼까요?</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">DNN</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span>DNN<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># 필수</span>\n    self<span class=\"token punctuation\">.</span>w1 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">20</span><span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># Linear한 1차원 값의 행렬곱을 할 때 사용 한다.</span>\n    self<span class=\"token punctuation\">.</span>w2 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># y = xA + b, bias 값을 가지고 있다.bias 비활성화를 원하면 bias=False를 파라미터에 입력 하면 된다.</span>\n    self<span class=\"token punctuation\">.</span>relu <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>ReLU<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># ReLU 활성화 함수</span>\n    self<span class=\"token punctuation\">.</span>softmax <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Softmax<span class=\"token punctuation\">(</span>dim<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># Softmax 함수</span>\n    self<span class=\"token punctuation\">.</span>dropout <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Dropout<span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># Dropout을 위함</span>\n    \n  <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\t<span class=\"token comment\"># 값을 도출하는 함수</span>\n    y <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>w1<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> self<span class=\"token punctuation\">.</span>bias1\n    y <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">)</span>\n    y <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>dropout<span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">)</span>\n    y <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>w2<span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> self<span class=\"token punctuation\">.</span>bias2\n    y <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>softmax<span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> y</code></pre></div>\n<br>\n<h2>신경망 학습 시키기</h2>\n<p>이제 <strong>학습</strong>을 시킬 차례입니다. 순서는 다음과 같습니다.</p>\n<ul>\n<li><strong>신경망 객체 선언</strong></li>\n<li><strong>loss function</strong> 설정</li>\n<li><strong>Optimizer</strong> 설정</li>\n<li><strong>역전파</strong>를 통한 <strong>반복 학습</strong></li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> DNN<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 객체 선언</span>\ncriterion <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>CrossEntropyLoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># loss function 설정</span>\noptim <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>optim<span class=\"token punctuation\">.</span>Adam<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">0.01</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># Optimizer 설정</span>\n\nx_data <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>Tensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\ny_data <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>LongTensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n    <span class=\"token number\">0</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># etc</span>\n    <span class=\"token number\">1</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># mammal</span>\n    <span class=\"token number\">2</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># birds</span>\n    <span class=\"token number\">0</span><span class=\"token punctuation\">,</span>\n    <span class=\"token number\">0</span><span class=\"token punctuation\">,</span>\n    <span class=\"token number\">2</span>\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">for</span> epoch <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">1000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  output <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>x_data<span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># model의 forward 함수 호출</span>\n  loss <span class=\"token operator\">=</span> criterion<span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">,</span> y_data<span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># loss function으로 값 계산</span>\n\n  optim<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># 변화도를 0으로 만듦</span>\n  loss<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 역전파</span>\n  optim<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># 역전파로 알아낸 변화도를 model에 적용</span>\n\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"progress:\"</span><span class=\"token punctuation\">,</span> epoch<span class=\"token punctuation\">,</span> <span class=\"token string\">\"loss=\"</span><span class=\"token punctuation\">,</span> loss<span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> x_data<span class=\"token punctuation\">:</span>\n  y_pred <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># 결과</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>y_pred<span class=\"token punctuation\">.</span><span class=\"token builtin\">max</span><span class=\"token punctuation\">(</span>dim<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># Softmax로 나온 최고 값의 Item 반환</span></code></pre></div>\n<p>결과는 다음과 같습니다. 원래 분류값과 같습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">...\nprogress: 999 loss= 0.8048997521400452\n0\n1\n2\n0\n0\n2</code></pre></div>\n<br>\n<h2>신경망 파라미터 보기</h2>\n<p><code class=\"language-text\">nn.Module.parameters()</code> 함수를 이용하여, <strong>신경망 내 파라미터</strong>를 확인 할 수 있습니다. 이는 <code class=\"language-text\">iterable</code>한 객체를 반환 합니다. <code class=\"language-text\">nn.Linear</code> 같은 경우 <code class=\"language-text\">Bias</code> 값도 같이 가지고 있습니다.</p>\n<p><strong>In</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">for</span> p <span class=\"token keyword\">in</span> model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>p<span class=\"token punctuation\">)</span></code></pre></div>\n<p><strong>Out</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">Parameter containing:\ntensor([[ 1.7096, -1.7123],\n        [-0.6805,  0.0985],\n        [ 0.1440,  1.9562],\n        [ 1.8654,  0.4950],\n        [ 0.5621,  1.7321],\n        [ 1.0138,  1.2981],\n        [-0.5106, -0.3613],\n        [-0.7362,  1.9065],\n        [ 0.4817,  1.8496],\n        [-0.2222,  0.3298],\n        [-0.2911, -0.0461],\n        [ 0.0025, -0.4711],\n        [ 0.1412,  0.3408],\n        [ 1.4825, -1.5597],\n        [-0.3180,  1.9836],\n        [-1.2678, -1.1919],\n        [ 0.2207, -0.2972],\n        [ 1.6377, -1.6504],\n        [ 1.5434, -1.5627],\n        [-0.6680,  0.4963]], requires_grad=True)\nParameter containing:\ntensor([-0.0138, -0.3932, -0.1902, -0.0206, -0.0475, -0.0151, -0.3948,  0.5054,\n        -0.0722, -0.4243, -0.0933, -0.6166, -0.6943, -0.0642,  0.0584,  1.1297,\n        -0.6891, -0.0328, -0.0174, -0.5061], requires_grad=True)\nParameter containing:\ntensor([[-4.4640e-01, -9.9519e-02, -1.4001e+00, -1.3389e+00, -1.5929e+00,\n         -1.4062e+00, -1.8384e-01, -2.6385e-01, -8.5617e-01, -1.5454e-01,\n          2.1536e-01, -1.6306e-01, -1.1615e-01, -1.0810e+00, -8.3832e-01,\n          2.2192e-01, -1.3481e-01, -5.9061e-01, -1.2475e+00,  1.1666e-01],\n        [ 1.2246e+00, -1.6060e-03, -5.1957e-01,  5.3110e-01, -1.5496e-01,\n          6.3452e-02,  9.0574e-03, -2.1007e+00, -1.9217e-01,  1.7074e-01,\n          2.0107e-01,  8.5908e-02, -6.8430e-02,  1.3706e+00, -1.3454e+00,\n         -1.6898e+00, -7.8466e-02,  1.2772e+00,  1.5808e+00, -4.7245e-02],\n        [-7.1529e-01,  2.0121e-01,  6.2016e-01,  3.4047e-01,  5.1581e-01,\n          4.6319e-01, -7.1984e-02,  5.3509e-01,  4.9709e-01, -1.2095e-01,\n          1.3305e-01, -1.1810e-01, -1.4068e-01, -2.6007e-01,  6.0926e-01,\n         -9.6860e-01,  3.2639e-02, -5.9985e-01, -9.6767e-01, -8.5310e-02]],\n       requires_grad=True)\nParameter containing:\ntensor([ 0.0040, -0.0931, -0.2043], requires_grad=True)</code></pre></div>\n<h2>마치며</h2>\n<p>여러 가지 <code class=\"language-text\">Loss Function</code>들은 <a href=\"https://pytorch.org/docs/stable/nn.html#loss-functions\">해당 링크</a>에서 확인 할 수 있고, 다른 <code class=\"language-text\">Optimizer</code>들은 <a href=\"https://pytorch.org/docs/stable/optim.html\">해당 링크</a>에서 확인 할 수 있습니다. 다음 시간에는 <code class=\"language-text\">Pytorch</code>로 <code class=\"language-text\">CNN</code> 모델을 설계 하는 방법을 알아 보겠습니다.</p>\n<h3>Reference</h3>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/nn.html\">Pytorch Document</a></li>\n<li><a href=\"https://github.com/graykode/DeepLearning-Study\">graykode/DeepLearning-Study</a></li>\n</ul>","id":"4ab26bed-547f-5562-b400-8d703fc2e734","frontmatter":{"date":"2020-04-04","path":"/deep-learning/pytorch-nn","title":"Pytorch의 Neural Network","tags":["Deep-Learning","Python"],"keyword":"Python, python, 파이썬, Pytorch, 파이토치, pytorch, 딥러닝, Neural Network, 신경망","summary":"Pytorch에서 신경망 구현하기","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png","series":"Pytorch Basic"}}},"pageContext":{"series":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"ee196dff-4c4c-5d59-bf3c-c4726466fd60","excerpt":"에서 모델의 가중치를 저장하기 위해선 3가지 함수만 알면 충분 합니다. : 객체를 디스크에 저장합니다.  모듈을 이용하여 객체를 직렬화 하며, 이 함수를 사용하여 모든 종류의 모델, Tensor 등을 저장할 수 있습니다. :  모듈을 이용하여 객체를 역직렬화하여 메모리에 할당합니다. : 역직렬화된 state_dict를 사용, 모델의 매개변수들을 불러옵니다. state_dict는 간단히 말해 각 체층을 매개변수 Tensor로 매핑한 Python…","frontmatter":{"date":"2020-04-26","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-save","title":"Pytorch에서 학습한 모델 저장 및 불러오기","img":"/post_image/pytorch-save.png","summary":"Pytorch 모델을 저장하고, 불러와 보기"}}},{"node":{"id":"cdf662e2-a4e9-58ba-84d0-e5847182d188","excerpt":"에서는 과 마찬가지로, 과 관련 된 를 제공합니다. 이를 이용해 손쉽게  네트워크를 구축 할 수 있습니다. Recurrent Neural Network 를 위한 는  입니다. 일단  시퀀스의 각 요소에 대해, 각 레이어에서는 다음 연산을 수행합니다.  Parameters : 의 사이즈에 해당 하는 수를 입력하면 됩니다. : 은닉층의 사이즈에 해당 하는 수를 입력하면 됩니다. : 의 은닉층 레이어 갯수를 나타냅니다. 기본 값은…","frontmatter":{"date":"2020-04-19","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-rnn","title":"Pytorch로 RNN, LSTM 구현하기","img":"https://lionbridge.ai/wp-content/uploads/2020/03/2020-02-21_difference-between-cnn-rnn-1.png","summary":"자연어 처리, 감성 분류 등에 사용되는 RNN, LSTM"}}},{"node":{"id":"4cdc9d08-db94-566b-88e6-c01dc47394f4","excerpt":"CNN In Pytorch 에는 을 개발 하기 위한 들이 있습니다. 다채널로 구현 되어 있는 CNN 신경망을 위한 Layers, Max pooling, Avg pooling 등, 이번 시간에는 여러 가지 을 위한 를 알아 보겠습니다. 또한,  데이터 또한 학습 해 보겠습니다. Convolution Layers  연산을 위한 레이어들은 다음과 같습니다. Conv1d (Text-CNN에서 많이 사용) Conv2d…","frontmatter":{"date":"2020-04-08","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-cnn","title":"Pytorch로 CNN 구현하기","img":"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/02/Plot-of-a-Subset-of-Images-from-the-MNIST-Dataset-1024x768.png","summary":"Pytorch로 MNIST 그림 식별을 해보자."}}},{"node":{"id":"4ab26bed-547f-5562-b400-8d703fc2e734","excerpt":"Neural Network in Pytorch 딥 러닝 프레임워크인 에선 신경망 구축을 위한 API…","frontmatter":{"date":"2020-04-04","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-nn","title":"Pytorch의 Neural Network","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png","summary":"Pytorch에서 신경망 구현하기"}}},{"node":{"id":"1699f140-b9f3-5aaf-b462-9e99cda20297","excerpt":"Pytorch로 행렬을 미분해보자, autograd 딥러닝 이론에 대해서 공부 해 보신 분들은, 딥러닝의 핵심은 미분을 통해서 손실 함수(loss function)의 값을 최소화 하는 것 입니다. 그렇기 때문에 Deep Learning 연구 플랫폼인 에선, 당연하게도 행렬 미분을 위한 기능들을  객체에 내장 시켜 놓았습니다. 한 번 알아볼까요? autograd 활성화 시키기 requires_grad=True…","frontmatter":{"date":"2020-03-28","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-autograd","title":"Pytorch로 행렬을 미분해보자, autograd","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png","summary":"Pytorch로 행렬을 미분하는 방법"}}},{"node":{"id":"ad43d234-197a-52d8-89f2-d33c3a14f01f","excerpt":"주의 필자인 저 또한 배운 것들을 정리 하면서 쓰는 글이기 때문에, 틀린 부분이 있을 수도 있습니다. 또한, 이 글은 에 대한 기본 적인 배경 지식을 필요로 합니다. 오타, 지적사항 발생 시, 댓글 혹은 이메일로 남겨 주시면 감사하겠습니다! Pytorch Pytorch는 두 가지 목표를 달성하기 위해 만들어진 오픈 소스 라이브러리이자, Python Package입니다. 두 가지 목표는 다음과 같습니다. Numpy가 기존에 하던 연산들을 GPU…","frontmatter":{"date":"2020-03-23","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-basic","title":"Pytorch에 대해서 알아보자.araboza","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png","summary":"딥러닝 플랫폼인 Pytorch에 대하여"}}}]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"23362913-2fa3-5e03-b669-2b9b1481bdf4","excerpt":"이번 시간에는 Deep Learning의 Network들을 정리 해 보는 시간을 가져보겠습니다. DNN(Deep Neural Network) 심층 신경망(Deep Neural Network)는 딥러닝의 가장 기본이 되는 신경망이며, 입력층(input layer)과 출력층(output layer) 사이에 여러개의 은닉층(hidden layer…","frontmatter":{"date":"2020-09-19","tags":["Deep-Learning"],"path":"/deep-learning/dl-summary-2","title":"Deep Learning Summary - 2. 신경망","img":"/post_image/thumbnail/dl-summary-2.png","summary":"딥러닝 신경망들을 정리 해 보자."}}},{"node":{"id":"6f4ef829-2b1e-5c62-9021-1b9650495b35","excerpt":"이번 시간에는 Deep Learning 에서 사용 하는 몇 가지 용어들에 대해서 정리 해 보는 시간을 가져 보겠습니다. 이 게시글에서는 기초 미분, 선형 대수, 통계, 확률론 그리고, 실제 어플리케이션을 만들 때 다루는 기초 딥러닝 용어 들을 카테고리 별로 나누어 정리 해 보도록 하겠습니다. 주의: 남 보라고 쓴 글이라기 보단, 저 보려고 쓴 글에 더 가깝습니다. 목차 1. 미분\n2. 선형 대수\n3. 확률과 통계…","frontmatter":{"date":"2020-09-13","tags":["Deep-Learning"],"path":"/deep-learning/dl-summary-1","title":"Deep Learning Summary - 1. 기초 용어 정리","img":"/post_image/thumbnail/dl-summary-1.jpg","summary":"Deep Learning 기초 용어들을 정리 해 보자."}}},{"node":{"id":"ee196dff-4c4c-5d59-bf3c-c4726466fd60","excerpt":"에서 모델의 가중치를 저장하기 위해선 3가지 함수만 알면 충분 합니다. : 객체를 디스크에 저장합니다.  모듈을 이용하여 객체를 직렬화 하며, 이 함수를 사용하여 모든 종류의 모델, Tensor 등을 저장할 수 있습니다. :  모듈을 이용하여 객체를 역직렬화하여 메모리에 할당합니다. : 역직렬화된 state_dict를 사용, 모델의 매개변수들을 불러옵니다. state_dict는 간단히 말해 각 체층을 매개변수 Tensor로 매핑한 Python…","frontmatter":{"date":"2020-04-26","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-save","title":"Pytorch에서 학습한 모델 저장 및 불러오기","img":"/post_image/pytorch-save.png","summary":"Pytorch 모델을 저장하고, 불러와 보기"}}},{"node":{"id":"cdf662e2-a4e9-58ba-84d0-e5847182d188","excerpt":"에서는 과 마찬가지로, 과 관련 된 를 제공합니다. 이를 이용해 손쉽게  네트워크를 구축 할 수 있습니다. Recurrent Neural Network 를 위한 는  입니다. 일단  시퀀스의 각 요소에 대해, 각 레이어에서는 다음 연산을 수행합니다.  Parameters : 의 사이즈에 해당 하는 수를 입력하면 됩니다. : 은닉층의 사이즈에 해당 하는 수를 입력하면 됩니다. : 의 은닉층 레이어 갯수를 나타냅니다. 기본 값은…","frontmatter":{"date":"2020-04-19","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-rnn","title":"Pytorch로 RNN, LSTM 구현하기","img":"https://lionbridge.ai/wp-content/uploads/2020/03/2020-02-21_difference-between-cnn-rnn-1.png","summary":"자연어 처리, 감성 분류 등에 사용되는 RNN, LSTM"}}}]}}}}},"staticQueryHashes":["234633779","63159454"]}