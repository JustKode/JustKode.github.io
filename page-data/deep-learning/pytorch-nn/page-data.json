{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/deep-learning/pytorch-nn","result":{"data":{"markdownRemark":{"html":"<h1>Neural Network in Pytorch</h1>\n<p>딥 러닝 프레임워크인 <code class=\"language-text\">Pytorch</code>에선 <strong>신경망 구축을 위한 API</strong>를 제공합니다. 이는 <code class=\"language-text\">torch.nn</code> 패키지를 <code class=\"language-text\">import</code> 하여 사용 가능합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn <span class=\"token keyword\">as</span> nn\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>functional <span class=\"token keyword\">as</span> F</code></pre></div>\n<br>\n<p>이번 시간의 <strong>목차</strong>는 다음과 같습니다.</p>\n<ul>\n<li><strong>신경망 클래스 선언</strong>하기</li>\n<li><strong>신경망 클래스의 멤버 변수, 멤버 함수</strong> 만들기</li>\n<li><strong>신경망 학습</strong>시키기</li>\n<li><strong>신경망 파라미터 확인</strong>하기</li>\n</ul>\n<h2>신경망 클래스 선언하기</h2>\n<p><strong>신경망 클래스</strong>를 선언하는 방법은 <code class=\"language-text\">nn.Module</code>을 <strong>상속</strong>받음으로써 가능합니다. 이는 <strong>신경망 클래스</strong>의 기본이 되는 클래스 입니다. 사용 방법은 다음과 같습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">DNN</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span>DNN<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n  <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">return</span> x</code></pre></div>\n<p><code class=\"language-text\">super(클래스명, self).__init__()</code>은 무조건 <strong>클래스 생성자</strong>에 입력 해 주어야 합니다.</p>\n<h2>신경망 클래스의 멤버 변수, 멤버 함수 만들기</h2>\n<p>신경망 클래스의 <strong>멤버 변수, 멤버 함수</strong>를 선언 하기 전에, 어떻게 신경망을 설계를 할 것인지 먼저 정해야 합니다.</p>\n<ul>\n<li><strong>가중치의 크기</strong>는 어느 정도로 할 것인지?</li>\n<li>신경망을 <strong>몇 층으로 설계</strong> 할 것인지?</li>\n<li><strong>Dropout</strong>은 몇 퍼센트로 할 것인지?</li>\n<li><strong>어떤 방식</strong>을 사용 하는 신경망인지? (CNN, RNN 등)</li>\n<li>기타 등등...</li>\n</ul>\n<p>이에 따라서 <strong>신경망 클래스</strong>의 <strong>멤버 변수</strong>를 어떻게 설정 할 것인지는 천지차이로 달라 질 수 있습니다.</p>\n<h3>상황 설정</h3>\n<p>일단 털, 날개의 유무를 통해 종을 분류하는 <strong>신경망</strong>을 만든다고 가정 해 보겠습니다. 그렇다면 <code class=\"language-text\">input</code>의 size는 (n, 2) 가 될 것입니다. 그리고 종의 종류가 3개라고 가정 하면, 최종 <code class=\"language-text\">output</code>은   <code class=\"language-text\">softmax</code> 함수를 사용하여 분류된 세가지 값 중에 하나 일 것입니다. 또한, <strong>은닉층을 한 개</strong> 놓는다고 가정하고, 활성화 함수로는 <strong>ReLU</strong>를 사용 하겠습니다. 그리고 <strong>Dropout</strong>을 이용하여, <strong>Overfitting</strong> 현상을 막아 보겠습니다. 대충 과정을 요약하면 이렇게 되겠군요.</p>\n<blockquote>\n<p>input -> affine1 -> ReLU -> dropout -> affine2 -> softmax</p>\n</blockquote>\n<p>한 번 구현해 볼까요?</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">DNN</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span>DNN<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># 필수</span>\n    self<span class=\"token punctuation\">.</span>w1 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">20</span><span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># Linear한 1차원 값의 행렬곱을 할 때 사용 한다.</span>\n    self<span class=\"token punctuation\">.</span>w2 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">20</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># y = xA + b, bias 값을 가지고 있다.bias 비활성화를 원하면 bias=False를 파라미터에 입력 하면 된다.</span>\n    self<span class=\"token punctuation\">.</span>relu <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>ReLU<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># ReLU 활성화 함수</span>\n    self<span class=\"token punctuation\">.</span>softmax <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Softmax<span class=\"token punctuation\">(</span>dim<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># Softmax 함수</span>\n    self<span class=\"token punctuation\">.</span>dropout <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Dropout<span class=\"token punctuation\">(</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># Dropout을 위함</span>\n    \n  <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\t<span class=\"token comment\"># 값을 도출하는 함수</span>\n    y <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>w1<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> self<span class=\"token punctuation\">.</span>bias1\n    y <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">)</span>\n    y <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>dropout<span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">)</span>\n    y <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>w2<span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> self<span class=\"token punctuation\">.</span>bias2\n    y <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>softmax<span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> y</code></pre></div>\n<br>\n<h2>신경망 학습 시키기</h2>\n<p>이제 <strong>학습</strong>을 시킬 차례입니다. 순서는 다음과 같습니다.</p>\n<ul>\n<li><strong>신경망 객체 선언</strong></li>\n<li><strong>loss function</strong> 설정</li>\n<li><strong>Optimizer</strong> 설정</li>\n<li><strong>역전파</strong>를 통한 <strong>반복 학습</strong></li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> DNN<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 객체 선언</span>\ncriterion <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>CrossEntropyLoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># loss function 설정</span>\noptim <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>optim<span class=\"token punctuation\">.</span>Adam<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">0.01</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># Optimizer 설정</span>\n\nx_data <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>Tensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\ny_data <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>LongTensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n    <span class=\"token number\">0</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># etc</span>\n    <span class=\"token number\">1</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># mammal</span>\n    <span class=\"token number\">2</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># birds</span>\n    <span class=\"token number\">0</span><span class=\"token punctuation\">,</span>\n    <span class=\"token number\">0</span><span class=\"token punctuation\">,</span>\n    <span class=\"token number\">2</span>\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">for</span> epoch <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">1000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  output <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>x_data<span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># model의 forward 함수 호출</span>\n  loss <span class=\"token operator\">=</span> criterion<span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">,</span> y_data<span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># loss function으로 값 계산</span>\n\n  optim<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># 변화도를 0으로 만듦</span>\n  loss<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 역전파</span>\n  optim<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># 역전파로 알아낸 변화도를 model에 적용</span>\n\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"progress:\"</span><span class=\"token punctuation\">,</span> epoch<span class=\"token punctuation\">,</span> <span class=\"token string\">\"loss=\"</span><span class=\"token punctuation\">,</span> loss<span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">for</span> x <span class=\"token keyword\">in</span> x_data<span class=\"token punctuation\">:</span>\n  y_pred <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># 결과</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>y_pred<span class=\"token punctuation\">.</span><span class=\"token builtin\">max</span><span class=\"token punctuation\">(</span>dim<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># Softmax로 나온 최고 값의 Item 반환</span></code></pre></div>\n<p>결과는 다음과 같습니다. 원래 분류값과 같습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">...\nprogress: 999 loss= 0.8048997521400452\n0\n1\n2\n0\n0\n2</code></pre></div>\n<br>\n<h2>신경망 파라미터 보기</h2>\n<p><code class=\"language-text\">nn.Module.parameters()</code> 함수를 이용하여, <strong>신경망 내 파라미터</strong>를 확인 할 수 있습니다. 이는 <code class=\"language-text\">iterable</code>한 객체를 반환 합니다. <code class=\"language-text\">nn.Linear</code> 같은 경우 <code class=\"language-text\">Bias</code> 값도 같이 가지고 있습니다.</p>\n<p><strong>In</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">for</span> p <span class=\"token keyword\">in</span> model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n  <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>p<span class=\"token punctuation\">)</span></code></pre></div>\n<p><strong>Out</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">Parameter containing:\ntensor([[ 1.7096, -1.7123],\n        [-0.6805,  0.0985],\n        [ 0.1440,  1.9562],\n        [ 1.8654,  0.4950],\n        [ 0.5621,  1.7321],\n        [ 1.0138,  1.2981],\n        [-0.5106, -0.3613],\n        [-0.7362,  1.9065],\n        [ 0.4817,  1.8496],\n        [-0.2222,  0.3298],\n        [-0.2911, -0.0461],\n        [ 0.0025, -0.4711],\n        [ 0.1412,  0.3408],\n        [ 1.4825, -1.5597],\n        [-0.3180,  1.9836],\n        [-1.2678, -1.1919],\n        [ 0.2207, -0.2972],\n        [ 1.6377, -1.6504],\n        [ 1.5434, -1.5627],\n        [-0.6680,  0.4963]], requires_grad=True)\nParameter containing:\ntensor([-0.0138, -0.3932, -0.1902, -0.0206, -0.0475, -0.0151, -0.3948,  0.5054,\n        -0.0722, -0.4243, -0.0933, -0.6166, -0.6943, -0.0642,  0.0584,  1.1297,\n        -0.6891, -0.0328, -0.0174, -0.5061], requires_grad=True)\nParameter containing:\ntensor([[-4.4640e-01, -9.9519e-02, -1.4001e+00, -1.3389e+00, -1.5929e+00,\n         -1.4062e+00, -1.8384e-01, -2.6385e-01, -8.5617e-01, -1.5454e-01,\n          2.1536e-01, -1.6306e-01, -1.1615e-01, -1.0810e+00, -8.3832e-01,\n          2.2192e-01, -1.3481e-01, -5.9061e-01, -1.2475e+00,  1.1666e-01],\n        [ 1.2246e+00, -1.6060e-03, -5.1957e-01,  5.3110e-01, -1.5496e-01,\n          6.3452e-02,  9.0574e-03, -2.1007e+00, -1.9217e-01,  1.7074e-01,\n          2.0107e-01,  8.5908e-02, -6.8430e-02,  1.3706e+00, -1.3454e+00,\n         -1.6898e+00, -7.8466e-02,  1.2772e+00,  1.5808e+00, -4.7245e-02],\n        [-7.1529e-01,  2.0121e-01,  6.2016e-01,  3.4047e-01,  5.1581e-01,\n          4.6319e-01, -7.1984e-02,  5.3509e-01,  4.9709e-01, -1.2095e-01,\n          1.3305e-01, -1.1810e-01, -1.4068e-01, -2.6007e-01,  6.0926e-01,\n         -9.6860e-01,  3.2639e-02, -5.9985e-01, -9.6767e-01, -8.5310e-02]],\n       requires_grad=True)\nParameter containing:\ntensor([ 0.0040, -0.0931, -0.2043], requires_grad=True)</code></pre></div>\n<h2>마치며</h2>\n<p>여러 가지 <code class=\"language-text\">Loss Function</code>들은 <a href=\"https://pytorch.org/docs/stable/nn.html#loss-functions\">해당 링크</a>에서 확인 할 수 있고, 다른 <code class=\"language-text\">Optimizer</code>들은 <a href=\"https://pytorch.org/docs/stable/optim.html\">해당 링크</a>에서 확인 할 수 있습니다. 다음 시간에는 <code class=\"language-text\">Pytorch</code>로 <code class=\"language-text\">CNN</code> 모델을 설계 하는 방법을 알아 보겠습니다.</p>\n<h3>Reference</h3>\n<ul>\n<li><a href=\"https://pytorch.org/docs/stable/nn.html\">Pytorch Document</a></li>\n<li><a href=\"https://github.com/graykode/DeepLearning-Study\">graykode/DeepLearning-Study</a></li>\n</ul>","id":"5b78f1ab-69ff-56fb-b9d3-f76b6905f5a1","frontmatter":{"date":"2020-04-04","path":"/deep-learning/pytorch-nn","title":"Pytorch의 Neural Network","tags":["Deep-Learning","Python"],"keyword":"Python, python, 파이썬, Pytorch, 파이토치, pytorch, 딥러닝, Neural Network, 신경망","summary":"Pytorch에서 신경망 구현하기","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png"}}},"pageContext":{}}}