{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/deep-learning/pytorch-save","result":{"data":{"markdownRemark":{"html":"<p><code class=\"language-text\">Pytorch</code> 에서 모델의 가중치를 저장하기 위해선 <strong>3가지 함수</strong>만 알면 충분 합니다.</p>\n<ul>\n<li><code class=\"language-text\">torch.save</code>: <strong>객체를 디스크에 저장</strong>합니다. <code class=\"language-text\">pickle</code> 모듈을 이용하여 객체를 직렬화 하며, 이 함수를 사용하여 모든 종류의 모델, Tensor 등을 저장할 수 있습니다.</li>\n<li><code class=\"language-text\">torch.load</code>: <code class=\"language-text\">pickle</code> 모듈을 이용하여 객체를 역직렬화하여 메모리에 할당합니다.</li>\n<li><code class=\"language-text\">torch.nn.Module.load_state_dict</code>: 역직렬화된 <em>state_dict</em>를 사용, 모델의 매개변수들을 불러옵니다. <em>state_dict</em>는 간단히 말해 각 체층을 매개변수 Tensor로 매핑한 Python 사전(dict) 객체입니다.</li>\n</ul>\n<p>간단한 <code class=\"language-text\">DNN</code> 모델을 통해 연습 해 보겠습니다.</p>\n<h3>Code</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn <span class=\"token keyword\">as</span> nn\n\nx_data <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>Tensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\ny_data <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>LongTensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n    <span class=\"token number\">0</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># etc</span>\n    <span class=\"token number\">1</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># mammal</span>\n    <span class=\"token number\">2</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># birds</span>\n    <span class=\"token number\">0</span><span class=\"token punctuation\">,</span>\n    <span class=\"token number\">0</span><span class=\"token punctuation\">,</span>\n    <span class=\"token number\">2</span>\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">DNN</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span>DNN<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>w1 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>bias1 <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">10</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n        self<span class=\"token punctuation\">.</span>w2 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>bias2 <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">3</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>relu <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>ReLU<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>softmax <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Softmax<span class=\"token punctuation\">(</span>dim<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        y <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>w1<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> self<span class=\"token punctuation\">.</span>bias1\n        y <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">)</span>\n\n        y <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>w2<span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> self<span class=\"token punctuation\">.</span>bias2\n        <span class=\"token keyword\">return</span> y\n\nmodel <span class=\"token operator\">=</span> DNN<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\ncriterion <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>CrossEntropyLoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\noptimizer <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>optim<span class=\"token punctuation\">.</span>SGD<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">0.01</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">for</span> epoch <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">1000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    output <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>x_data<span class=\"token punctuation\">)</span>\n\n    loss <span class=\"token operator\">=</span> criterion<span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">,</span> y_data<span class=\"token punctuation\">)</span>\n\n    optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    loss<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"progress:\"</span><span class=\"token punctuation\">,</span> epoch<span class=\"token punctuation\">,</span> <span class=\"token string\">\"loss=\"</span><span class=\"token punctuation\">,</span> loss<span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h2>torch.save(object, path)</h2>\n<p><strong>전체 모델을 저장</strong>하거나, <strong>모델의 <code class=\"language-text\">state_dict</code>를 저장</strong> 할 때 사용합니다.</p>\n<ul>\n<li><code class=\"language-text\">object</code>: 저장할 모델 객체</li>\n<li><code class=\"language-text\">path</code>: 저장할 위치 + 파일명</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">PATH <span class=\"token operator\">=</span> <span class=\"token string\">'./weights/'</span>\n\ntorch<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> PATH <span class=\"token operator\">+</span> <span class=\"token string\">'model.pt'</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 전체 모델 저장</span>\ntorch<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>state_dict<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> PATH <span class=\"token operator\">+</span> <span class=\"token string\">'model_state_dict.pt'</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 모델 객체의 state_dict 저장</span>\ntorch<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span>\n    <span class=\"token string\">'model'</span><span class=\"token punctuation\">:</span> model<span class=\"token punctuation\">.</span>state_dict<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">'optimizer'</span><span class=\"token punctuation\">:</span> optimizer<span class=\"token punctuation\">.</span>state_dict<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span> PATH <span class=\"token operator\">+</span> <span class=\"token string\">'all.tar'</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 여러 가지 값 저장, 학습 중 진행 상황 저장을 위해 epoch, loss 값 등 일반 scalar값 저장 가능</span></code></pre></div>\n<h2>torch.load(path)</h2>\n<p><strong>전체 모델을 불러</strong>오거나, <strong>모델의 <code class=\"language-text\">state_dict</code>를 불러</strong> 올 때 사용합니다.</p>\n<ul>\n<li><code class=\"language-text\">path</code>: 불러올 위치 + 파일명</li>\n</ul>\n<h2>torch.nn.Module.load<em>state</em>dict(dict):</h2>\n<p><code class=\"language-text\">state_dict</code>를 이용하여, 모델 객체 내의 매개 변수 값을 초기화 합니다.</p>\n<ul>\n<li><code class=\"language-text\">dict</code>: 불러올 매개 변수 값들이 담겨있는 <code class=\"language-text\">state_dict</code> 객체</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>PATH <span class=\"token operator\">+</span> <span class=\"token string\">'model.pt'</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 전체 모델을 통째로 불러옴, 클래스 선언 필수</span>\nmodel<span class=\"token punctuation\">.</span>load_state_dict<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>PATH <span class=\"token operator\">+</span> <span class=\"token string\">'model_state_dict.pt'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># state_dict를 불러 온 후, 모델에 저장</span>\n\ncheckpoint <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>PATH <span class=\"token operator\">+</span> <span class=\"token string\">'all.tar'</span><span class=\"token punctuation\">)</span>   <span class=\"token comment\"># dict 불러오기</span>\nmodel<span class=\"token punctuation\">.</span>load_state_dict<span class=\"token punctuation\">(</span>checkpoint<span class=\"token punctuation\">[</span><span class=\"token string\">'model'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\noptimizer<span class=\"token punctuation\">.</span>load_state_dict<span class=\"token punctuation\">(</span>checkpoint<span class=\"token punctuation\">[</span><span class=\"token string\">'optimizer'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>모델을 불러 온 이후에는 이 모델을 학습 할 껀지, 사용 할 껀지에 따라 각각 <code class=\"language-text\">model.train()</code>, <code class=\"language-text\">model.eval()</code> 둘 중에 하나를 사용 하면 됩니다.</p>\n<h2>다른 모델의 매개변수 사용하기</h2>\n<p>모델의 매개변수의 일부만 불러 사용하는 것은 <strong>전이학습</strong>을 이용할 때 자주 사용합니다. <code class=\"language-text\">state_dict</code>의 일부만 불러오거나, 적재하려는 모델보다 더 많은 키를 갖고 있는 <code class=\"language-text\">state_dict</code>를 불러 올때는, <code class=\"language-text\">load_state_dict()</code> 함수의 파라미터에 <code class=\"language-text\">strict=False</code>를 입력 해 주면 됩니다.</p>\n<p><strong>주의: 위 코드에서는 구현 하지 않은 클래스 입니다.</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">torch<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span>modelA<span class=\"token punctuation\">.</span>state_dict<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> PATH<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 저장하기</span>\n\nmodelB <span class=\"token operator\">=</span> TheModelBClass<span class=\"token punctuation\">(</span><span class=\"token operator\">*</span>args<span class=\"token punctuation\">,</span> <span class=\"token operator\">**</span>kwargs<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 불러오기</span>\nmodelB<span class=\"token punctuation\">.</span>load_state_dict<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>PATH<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> strict<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h2>GPU, CPU간 모델 불러오기</h2>\n<p><code class=\"language-text\">GPU</code> 에서 학습 한 모델과, <code class=\"language-text\">CPU</code> 에서 학습 한 모델 간 저장하는 방법은 같지만, 케이스에 따라 불러오는 과정이 다릅니다.</p>\n<p>저장하는 방법은 다음과 같습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">torch<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>state_dict<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> PATH <span class=\"token operator\">+</span> <span class=\"token string\">'model.pt'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h2>GPU에서 저장, CPU에서 불러오기</h2>\n<p><code class=\"language-text\">torch.load()</code> 함수의 <code class=\"language-text\">map_location</code> 인자에 <code class=\"language-text\">torch.device(&#39;cpu&#39;)</code> 를 전달 함으로써, 모델을 동적으로 CPU 장치에 할당합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">device <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>device<span class=\"token punctuation\">(</span><span class=\"token string\">'cpu'</span><span class=\"token punctuation\">)</span>\nmodel <span class=\"token operator\">=</span> DNN<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>load_state_dict<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>PATH<span class=\"token punctuation\">,</span> map_location<span class=\"token operator\">=</span>device<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h2>GPU에서 저장, GPU에서 불러오기</h2>\n<p><code class=\"language-text\">torch.load()</code> 로 초기화 한 모델의 <code class=\"language-text\">model.to(torch.device(&#39;cuda&#39;))</code> 를 호출하여, <code class=\"language-text\">CUDA Tensor</code>로 내부 매개변수를 형변환 해 주어야 합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">device <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>device<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda'</span><span class=\"token punctuation\">)</span>\nmodel <span class=\"token operator\">=</span> DNN<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>load_state_dict<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>PATH<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span></code></pre></div>\n<h2>CPU에서 저장, GPU에서 불러오기</h2>\n<p><code class=\"language-text\">torch.load()</code> 함수의 <code class=\"language-text\">map_location</code> 인자에 <em>cuda:device_id</em> 를 전달 함으로써, 모델을 동적으로 해당 GPU 장치에 할당합니다. 그 이후에 <code class=\"language-text\">model.to(torch.device(&#39;cuda&#39;))</code>를 호출, 모델 내의 <code class=\"language-text\">Tensor</code>를 <code class=\"language-text\">CUDA Tensor</code>로 변환 합니다. 모든 모델 입력에도 <code class=\"language-text\">.to(torch.device(&#39;cuda&#39;))</code>를 입력하여, <code class=\"language-text\">CUDA Tensor</code>로 변환하여야 합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">device <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>device<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda'</span><span class=\"token punctuation\">)</span>\nmodel <span class=\"token operator\">=</span> DNN<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>load_state_dict<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>PATH<span class=\"token punctuation\">,</span> map_location<span class=\"token operator\">=</span><span class=\"token string\">\"cuda:0\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 사용할 GPU 장치 번호 선택.</span>\nmodel<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># CUDA Tensor 형 변환</span></code></pre></div>","id":"3e5a78d9-ea4d-5a77-a082-281403358a61","frontmatter":{"date":"2020-04-26","path":"/deep-learning/pytorch-save","title":"Pytorch에서 학습한 모델 저장 및 불러오기","tags":["Deep-Learning","Python"],"keyword":"Python, python, 파이썬, Pytorch, 파이토치, pytorch, 딥러닝, pytorch save, pytorch weight save, pytorch model save","summary":"Pytorch 모델을 저장하고, 불러와 보기","img":"/post_image/pytorch-save.png","series":"Pytorch Basic"}}},"pageContext":{"series":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"3e5a78d9-ea4d-5a77-a082-281403358a61","excerpt":"에서 모델의 가중치를 저장하기 위해선 3가지 함수만 알면 충분 합니다. : 객체를 디스크에 저장합니다.  모듈을 이용하여 객체를 직렬화 하며, 이 함수를 사용하여 모든 종류의 모델, Tensor 등을 저장할 수 있습니다. :  모듈을 이용하여 객체를 역직렬화하여 메모리에 할당합니다. : 역직렬화된 state_dict를 사용, 모델의 매개변수들을 불러옵니다. state_dict는 간단히 말해 각 체층을 매개변수 Tensor로 매핑한 Python…","frontmatter":{"date":"2020-04-26","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-save","title":"Pytorch에서 학습한 모델 저장 및 불러오기","img":"/post_image/pytorch-save.png","summary":"Pytorch 모델을 저장하고, 불러와 보기"}}},{"node":{"id":"ce0ab714-91d8-5592-afc1-72bb2a4c56c4","excerpt":"에서는 과 마찬가지로, 과 관련 된 를 제공합니다. 이를 이용해 손쉽게  네트워크를 구축 할 수 있습니다. Recurrent Neural Network 를 위한 는  입니다. 일단  시퀀스의 각 요소에 대해, 각 레이어에서는 다음 연산을 수행합니다.  Parameters : 의 사이즈에 해당 하는 수를 입력하면 됩니다. : 은닉층의 사이즈에 해당 하는 수를 입력하면 됩니다. : 의 은닉층 레이어 갯수를 나타냅니다. 기본 값은…","frontmatter":{"date":"2020-04-19","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-rnn","title":"Pytorch로 RNN, LSTM 구현하기","img":"https://lionbridge.ai/wp-content/uploads/2020/03/2020-02-21_difference-between-cnn-rnn-1.png","summary":"자연어 처리, 감성 분류 등에 사용되는 RNN, LSTM"}}},{"node":{"id":"ddf2f489-870b-5d07-a72f-68cefd3da0c4","excerpt":"CNN In Pytorch 에는 을 개발 하기 위한 들이 있습니다. 다채널로 구현 되어 있는 CNN 신경망을 위한 Layers, Max pooling, Avg pooling 등, 이번 시간에는 여러 가지 을 위한 를 알아 보겠습니다. 또한,  데이터 또한 학습 해 보겠습니다. Convolution Layers  연산을 위한 레이어들은 다음과 같습니다. Conv1d (Text-CNN에서 많이 사용) Conv2d…","frontmatter":{"date":"2020-04-08","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-cnn","title":"Pytorch로 CNN 구현하기","img":"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/02/Plot-of-a-Subset-of-Images-from-the-MNIST-Dataset-1024x768.png","summary":"Pytorch로 MNIST 그림 식별을 해보자."}}},{"node":{"id":"5b78f1ab-69ff-56fb-b9d3-f76b6905f5a1","excerpt":"Neural Network in Pytorch 딥 러닝 프레임워크인 에선 신경망 구축을 위한 API…","frontmatter":{"date":"2020-04-04","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-nn","title":"Pytorch의 Neural Network","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png","summary":"Pytorch에서 신경망 구현하기"}}},{"node":{"id":"18eeab8b-863e-5ce3-b980-5e9e957c180a","excerpt":"Pytorch로 행렬을 미분해보자, autograd 딥러닝 이론에 대해서 공부 해 보신 분들은, 딥러닝의 핵심은 미분을 통해서 손실 함수(loss function)의 값을 최소화 하는 것 입니다. 그렇기 때문에 Deep Learning 연구 플랫폼인 에선, 당연하게도 행렬 미분을 위한 기능들을  객체에 내장 시켜 놓았습니다. 한 번 알아볼까요? autograd 활성화 시키기 requires_grad=True…","frontmatter":{"date":"2020-03-28","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-autograd","title":"Pytorch로 행렬을 미분해보자, autograd","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png","summary":"Pytorch로 행렬을 미분하는 방법"}}},{"node":{"id":"d3d26a0d-b647-5094-816f-d4c7a9c8e5d9","excerpt":"주의 필자인 저 또한 배운 것들을 정리 하면서 쓰는 글이기 때문에, 틀린 부분이 있을 수도 있습니다. 또한, 이 글은 에 대한 기본 적인 배경 지식을 필요로 합니다. 오타, 지적사항 발생 시, 댓글 혹은 이메일로 남겨 주시면 감사하겠습니다! Pytorch Pytorch는 두 가지 목표를 달성하기 위해 만들어진 오픈 소스 라이브러리이자, Python Package입니다. 두 가지 목표는 다음과 같습니다. Numpy가 기존에 하던 연산들을 GPU…","frontmatter":{"date":"2020-03-23","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-basic","title":"Pytorch에 대해서 알아보자.araboza","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png","summary":"딥러닝 플랫폼인 Pytorch에 대하여"}}}]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"69d057cf-22e0-5e3a-b7cd-1a4155a4bde7","excerpt":"이번 시간에는 Deep Learning의 Network들을 정리 해 보는 시간을 가져보겠습니다. DNN(Deep Neural Network) 심층 신경망(Deep Neural Network)는 딥러닝의 가장 기본이 되는 신경망이며, 입력층(input layer)과 출력층(output layer) 사이에 여러개의 은닉층(hidden layer…","frontmatter":{"date":"2020-09-19","tags":["Deep-Learning"],"path":"/deep-learning/dl-summary-2","title":"Deep Learning Summary - 2. 신경망","img":"/post_image/thumbnail/dl-summary-2.png","summary":"딥러닝 신경망들을 정리 해 보자."}}},{"node":{"id":"7b1ad58d-837a-5625-8622-35a72d134629","excerpt":"이번 시간에는 Deep Learning 에서 사용 하는 몇 가지 용어들에 대해서 정리 해 보는 시간을 가져 보겠습니다. 이 게시글에서는 기초 미분, 선형 대수, 통계, 확률론 그리고, 실제 어플리케이션을 만들 때 다루는 기초 딥러닝 용어 들을 카테고리 별로 나누어 정리 해 보도록 하겠습니다. 주의: 남 보라고 쓴 글이라기 보단, 저 보려고 쓴 글에 더 가깝습니다. 목차 1. 미분\n2. 선형 대수\n3. 확률과 통계…","frontmatter":{"date":"2020-09-13","tags":["Deep-Learning"],"path":"/deep-learning/dl-summary-1","title":"Deep Learning Summary - 1. 기초 용어 정리","img":"/post_image/thumbnail/dl-summary-1.jpg","summary":"Deep Learning 기초 용어들을 정리 해 보자."}}},{"node":{"id":"3e5a78d9-ea4d-5a77-a082-281403358a61","excerpt":"에서 모델의 가중치를 저장하기 위해선 3가지 함수만 알면 충분 합니다. : 객체를 디스크에 저장합니다.  모듈을 이용하여 객체를 직렬화 하며, 이 함수를 사용하여 모든 종류의 모델, Tensor 등을 저장할 수 있습니다. :  모듈을 이용하여 객체를 역직렬화하여 메모리에 할당합니다. : 역직렬화된 state_dict를 사용, 모델의 매개변수들을 불러옵니다. state_dict는 간단히 말해 각 체층을 매개변수 Tensor로 매핑한 Python…","frontmatter":{"date":"2020-04-26","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-save","title":"Pytorch에서 학습한 모델 저장 및 불러오기","img":"/post_image/pytorch-save.png","summary":"Pytorch 모델을 저장하고, 불러와 보기"}}},{"node":{"id":"ce0ab714-91d8-5592-afc1-72bb2a4c56c4","excerpt":"에서는 과 마찬가지로, 과 관련 된 를 제공합니다. 이를 이용해 손쉽게  네트워크를 구축 할 수 있습니다. Recurrent Neural Network 를 위한 는  입니다. 일단  시퀀스의 각 요소에 대해, 각 레이어에서는 다음 연산을 수행합니다.  Parameters : 의 사이즈에 해당 하는 수를 입력하면 됩니다. : 은닉층의 사이즈에 해당 하는 수를 입력하면 됩니다. : 의 은닉층 레이어 갯수를 나타냅니다. 기본 값은…","frontmatter":{"date":"2020-04-19","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-rnn","title":"Pytorch로 RNN, LSTM 구현하기","img":"https://lionbridge.ai/wp-content/uploads/2020/03/2020-02-21_difference-between-cnn-rnn-1.png","summary":"자연어 처리, 감성 분류 등에 사용되는 RNN, LSTM"}}}]}}}}}}