{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/deep-learning/pytorch-save","result":{"data":{"markdownRemark":{"html":"<p><code class=\"language-text\">Pytorch</code> 에서 모델의 가중치를 저장하기 위해선 <strong>3가지 함수</strong>만 알면 충분 합니다.</p>\n<ul>\n<li><code class=\"language-text\">torch.save</code>: <strong>객체를 디스크에 저장</strong>합니다. <code class=\"language-text\">pickle</code> 모듈을 이용하여 객체를 직렬화 하며, 이 함수를 사용하여 모든 종류의 모델, Tensor 등을 저장할 수 있습니다.</li>\n<li><code class=\"language-text\">torch.load</code>: <code class=\"language-text\">pickle</code> 모듈을 이용하여 객체를 역직렬화하여 메모리에 할당합니다.</li>\n<li><code class=\"language-text\">torch.nn.Module.load_state_dict</code>: 역직렬화된 <em>state_dict</em>를 사용, 모델의 매개변수들을 불러옵니다. <em>state_dict</em>는 간단히 말해 각 체층을 매개변수 Tensor로 매핑한 Python 사전(dict) 객체입니다.</li>\n</ul>\n<p>간단한 <code class=\"language-text\">DNN</code> 모델을 통해 연습 해 보겠습니다.</p>\n<h3>Code</h3>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">import</span> torch<span class=\"token punctuation\">.</span>nn <span class=\"token keyword\">as</span> nn\n\nx_data <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>Tensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\ny_data <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>LongTensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n    <span class=\"token number\">0</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># etc</span>\n    <span class=\"token number\">1</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># mammal</span>\n    <span class=\"token number\">2</span><span class=\"token punctuation\">,</span>  <span class=\"token comment\"># birds</span>\n    <span class=\"token number\">0</span><span class=\"token punctuation\">,</span>\n    <span class=\"token number\">0</span><span class=\"token punctuation\">,</span>\n    <span class=\"token number\">2</span>\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">DNN</span><span class=\"token punctuation\">(</span>nn<span class=\"token punctuation\">.</span>Module<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token builtin\">super</span><span class=\"token punctuation\">(</span>DNN<span class=\"token punctuation\">,</span> self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>__init__<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>w1 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>bias1 <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">10</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n        self<span class=\"token punctuation\">.</span>w2 <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>bias2 <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">3</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>relu <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>ReLU<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>softmax <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Softmax<span class=\"token punctuation\">(</span>dim<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">forward</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        y <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>w1<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> self<span class=\"token punctuation\">.</span>bias1\n        y <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">)</span>\n\n        y <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>w2<span class=\"token punctuation\">(</span>y<span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span> self<span class=\"token punctuation\">.</span>bias2\n        <span class=\"token keyword\">return</span> y\n\nmodel <span class=\"token operator\">=</span> DNN<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\ncriterion <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>CrossEntropyLoss<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\noptimizer <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>optim<span class=\"token punctuation\">.</span>SGD<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>parameters<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> lr<span class=\"token operator\">=</span><span class=\"token number\">0.01</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">for</span> epoch <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span><span class=\"token number\">1000</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    output <span class=\"token operator\">=</span> model<span class=\"token punctuation\">(</span>x_data<span class=\"token punctuation\">)</span>\n\n    loss <span class=\"token operator\">=</span> criterion<span class=\"token punctuation\">(</span>output<span class=\"token punctuation\">,</span> y_data<span class=\"token punctuation\">)</span>\n\n    optimizer<span class=\"token punctuation\">.</span>zero_grad<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    loss<span class=\"token punctuation\">.</span>backward<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    optimizer<span class=\"token punctuation\">.</span>step<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"progress:\"</span><span class=\"token punctuation\">,</span> epoch<span class=\"token punctuation\">,</span> <span class=\"token string\">\"loss=\"</span><span class=\"token punctuation\">,</span> loss<span class=\"token punctuation\">.</span>item<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h2>torch.save(object, path)</h2>\n<p><strong>전체 모델을 저장</strong>하거나, <strong>모델의 <code class=\"language-text\">state_dict</code>를 저장</strong> 할 때 사용합니다.</p>\n<ul>\n<li><code class=\"language-text\">object</code>: 저장할 모델 객체</li>\n<li><code class=\"language-text\">path</code>: 저장할 위치 + 파일명</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">PATH <span class=\"token operator\">=</span> <span class=\"token string\">'./weights/'</span>\n\ntorch<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">,</span> PATH <span class=\"token operator\">+</span> <span class=\"token string\">'model.pt'</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 전체 모델 저장</span>\ntorch<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>state_dict<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> PATH <span class=\"token operator\">+</span> <span class=\"token string\">'model_state_dict.pt'</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 모델 객체의 state_dict 저장</span>\ntorch<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span>\n    <span class=\"token string\">'model'</span><span class=\"token punctuation\">:</span> model<span class=\"token punctuation\">.</span>state_dict<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">'optimizer'</span><span class=\"token punctuation\">:</span> optimizer<span class=\"token punctuation\">.</span>state_dict<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span> PATH <span class=\"token operator\">+</span> <span class=\"token string\">'all.tar'</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 여러 가지 값 저장, 학습 중 진행 상황 저장을 위해 epoch, loss 값 등 일반 scalar값 저장 가능</span></code></pre></div>\n<h2>torch.load(path)</h2>\n<p><strong>전체 모델을 불러</strong>오거나, <strong>모델의 <code class=\"language-text\">state_dict</code>를 불러</strong> 올 때 사용합니다.</p>\n<ul>\n<li><code class=\"language-text\">path</code>: 불러올 위치 + 파일명</li>\n</ul>\n<h2>torch.nn.Module.load<em>state</em>dict(dict):</h2>\n<p><code class=\"language-text\">state_dict</code>를 이용하여, 모델 객체 내의 매개 변수 값을 초기화 합니다.</p>\n<ul>\n<li><code class=\"language-text\">dict</code>: 불러올 매개 변수 값들이 담겨있는 <code class=\"language-text\">state_dict</code> 객체</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">model <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>PATH <span class=\"token operator\">+</span> <span class=\"token string\">'model.pt'</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 전체 모델을 통째로 불러옴, 클래스 선언 필수</span>\nmodel<span class=\"token punctuation\">.</span>load_state_dict<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>PATH <span class=\"token operator\">+</span> <span class=\"token string\">'model_state_dict.pt'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># state_dict를 불러 온 후, 모델에 저장</span>\n\ncheckpoint <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>PATH <span class=\"token operator\">+</span> <span class=\"token string\">'all.tar'</span><span class=\"token punctuation\">)</span>   <span class=\"token comment\"># dict 불러오기</span>\nmodel<span class=\"token punctuation\">.</span>load_state_dict<span class=\"token punctuation\">(</span>checkpoint<span class=\"token punctuation\">[</span><span class=\"token string\">'model'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\noptimizer<span class=\"token punctuation\">.</span>load_state_dict<span class=\"token punctuation\">(</span>checkpoint<span class=\"token punctuation\">[</span><span class=\"token string\">'optimizer'</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>모델을 불러 온 이후에는 이 모델을 학습 할 껀지, 사용 할 껀지에 따라 각각 <code class=\"language-text\">model.train()</code>, <code class=\"language-text\">model.eval()</code> 둘 중에 하나를 사용 하면 됩니다.</p>\n<h2>다른 모델의 매개변수 사용하기</h2>\n<p>모델의 매개변수의 일부만 불러 사용하는 것은 <strong>전이학습</strong>을 이용할 때 자주 사용합니다. <code class=\"language-text\">state_dict</code>의 일부만 불러오거나, 적재하려는 모델보다 더 많은 키를 갖고 있는 <code class=\"language-text\">state_dict</code>를 불러 올때는, <code class=\"language-text\">load_state_dict()</code> 함수의 파라미터에 <code class=\"language-text\">strict=False</code>를 입력 해 주면 됩니다.</p>\n<p><strong>주의: 위 코드에서는 구현 하지 않은 클래스 입니다.</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">torch<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span>modelA<span class=\"token punctuation\">.</span>state_dict<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> PATH<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 저장하기</span>\n\nmodelB <span class=\"token operator\">=</span> TheModelBClass<span class=\"token punctuation\">(</span><span class=\"token operator\">*</span>args<span class=\"token punctuation\">,</span> <span class=\"token operator\">**</span>kwargs<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 불러오기</span>\nmodelB<span class=\"token punctuation\">.</span>load_state_dict<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>PATH<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> strict<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h2>GPU, CPU간 모델 불러오기</h2>\n<p><code class=\"language-text\">GPU</code> 에서 학습 한 모델과, <code class=\"language-text\">CPU</code> 에서 학습 한 모델 간 저장하는 방법은 같지만, 케이스에 따라 불러오는 과정이 다릅니다.</p>\n<p>저장하는 방법은 다음과 같습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">torch<span class=\"token punctuation\">.</span>save<span class=\"token punctuation\">(</span>model<span class=\"token punctuation\">.</span>state_dict<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> PATH <span class=\"token operator\">+</span> <span class=\"token string\">'model.pt'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h2>GPU에서 저장, CPU에서 불러오기</h2>\n<p><code class=\"language-text\">torch.load()</code> 함수의 <code class=\"language-text\">map_location</code> 인자에 <code class=\"language-text\">torch.device(&#39;cpu&#39;)</code> 를 전달 함으로써, 모델을 동적으로 CPU 장치에 할당합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">device <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>device<span class=\"token punctuation\">(</span><span class=\"token string\">'cpu'</span><span class=\"token punctuation\">)</span>\nmodel <span class=\"token operator\">=</span> DNN<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>load_state_dict<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>PATH<span class=\"token punctuation\">,</span> map_location<span class=\"token operator\">=</span>device<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h2>GPU에서 저장, GPU에서 불러오기</h2>\n<p><code class=\"language-text\">torch.load()</code> 로 초기화 한 모델의 <code class=\"language-text\">model.to(torch.device(&#39;cuda&#39;))</code> 를 호출하여, <code class=\"language-text\">CUDA Tensor</code>로 내부 매개변수를 형변환 해 주어야 합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">device <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>device<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda'</span><span class=\"token punctuation\">)</span>\nmodel <span class=\"token operator\">=</span> DNN<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>load_state_dict<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>PATH<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span></code></pre></div>\n<h2>CPU에서 저장, GPU에서 불러오기</h2>\n<p><code class=\"language-text\">torch.load()</code> 함수의 <code class=\"language-text\">map_location</code> 인자에 <em>cuda:device_id</em> 를 전달 함으로써, 모델을 동적으로 해당 GPU 장치에 할당합니다. 그 이후에 <code class=\"language-text\">model.to(torch.device(&#39;cuda&#39;))</code>를 호출, 모델 내의 <code class=\"language-text\">Tensor</code>를 <code class=\"language-text\">CUDA Tensor</code>로 변환 합니다. 모든 모델 입력에도 <code class=\"language-text\">.to(torch.device(&#39;cuda&#39;))</code>를 입력하여, <code class=\"language-text\">CUDA Tensor</code>로 변환하여야 합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">device <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>device<span class=\"token punctuation\">(</span><span class=\"token string\">'cuda'</span><span class=\"token punctuation\">)</span>\nmodel <span class=\"token operator\">=</span> DNN<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nmodel<span class=\"token punctuation\">.</span>load_state_dict<span class=\"token punctuation\">(</span>torch<span class=\"token punctuation\">.</span>load<span class=\"token punctuation\">(</span>PATH<span class=\"token punctuation\">,</span> map_location<span class=\"token operator\">=</span><span class=\"token string\">\"cuda:0\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 사용할 GPU 장치 번호 선택.</span>\nmodel<span class=\"token punctuation\">.</span>to<span class=\"token punctuation\">(</span>device<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># CUDA Tensor 형 변환</span></code></pre></div>","id":"ee196dff-4c4c-5d59-bf3c-c4726466fd60","frontmatter":{"date":"2020-04-26","path":"/deep-learning/pytorch-save","title":"Pytorch에서 학습한 모델 저장 및 불러오기","tags":["Deep-Learning","Python"],"keyword":"Python, python, 파이썬, Pytorch, 파이토치, pytorch, 딥러닝, pytorch save, pytorch weight save, pytorch model save","summary":"Pytorch 모델을 저장하고, 불러와 보기","img":"/post_image/pytorch-save.png","series":"Pytorch Basic"}}},"pageContext":{"series":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"ee196dff-4c4c-5d59-bf3c-c4726466fd60","excerpt":"에서 모델의 가중치를 저장하기 위해선 3가지 함수만 알면 충분 합니다. : 객체를 디스크에 저장합니다.  모듈을 이용하여 객체를 직렬화 하며, 이 함수를 사용하여 모든 종류의 모델, Tensor 등을 저장할 수 있습니다. :  모듈을 이용하여 객체를 역직렬화하여 메모리에 할당합니다. : 역직렬화된 state_dict를 사용, 모델의 매개변수들을 불러옵니다. state_dict는 간단히 말해 각 체층을 매개변수 Tensor로 매핑한 Python…","frontmatter":{"date":"2020-04-26","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-save","title":"Pytorch에서 학습한 모델 저장 및 불러오기","img":"/post_image/pytorch-save.png","summary":"Pytorch 모델을 저장하고, 불러와 보기"}}},{"node":{"id":"cdf662e2-a4e9-58ba-84d0-e5847182d188","excerpt":"에서는 과 마찬가지로, 과 관련 된 를 제공합니다. 이를 이용해 손쉽게  네트워크를 구축 할 수 있습니다. Recurrent Neural Network 를 위한 는  입니다. 일단  시퀀스의 각 요소에 대해, 각 레이어에서는 다음 연산을 수행합니다.  Parameters : 의 사이즈에 해당 하는 수를 입력하면 됩니다. : 은닉층의 사이즈에 해당 하는 수를 입력하면 됩니다. : 의 은닉층 레이어 갯수를 나타냅니다. 기본 값은…","frontmatter":{"date":"2020-04-19","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-rnn","title":"Pytorch로 RNN, LSTM 구현하기","img":"https://lionbridge.ai/wp-content/uploads/2020/03/2020-02-21_difference-between-cnn-rnn-1.png","summary":"자연어 처리, 감성 분류 등에 사용되는 RNN, LSTM"}}},{"node":{"id":"4cdc9d08-db94-566b-88e6-c01dc47394f4","excerpt":"CNN In Pytorch 에는 을 개발 하기 위한 들이 있습니다. 다채널로 구현 되어 있는 CNN 신경망을 위한 Layers, Max pooling, Avg pooling 등, 이번 시간에는 여러 가지 을 위한 를 알아 보겠습니다. 또한,  데이터 또한 학습 해 보겠습니다. Convolution Layers  연산을 위한 레이어들은 다음과 같습니다. Conv1d (Text-CNN에서 많이 사용) Conv2d…","frontmatter":{"date":"2020-04-08","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-cnn","title":"Pytorch로 CNN 구현하기","img":"/post_image/MnistExamples.png","summary":"Pytorch로 MNIST 그림 식별을 해보자."}}},{"node":{"id":"4ab26bed-547f-5562-b400-8d703fc2e734","excerpt":"Neural Network in Pytorch 딥 러닝 프레임워크인 에선 신경망 구축을 위한 API…","frontmatter":{"date":"2020-04-04","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-nn","title":"Pytorch의 Neural Network","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png","summary":"Pytorch에서 신경망 구현하기"}}},{"node":{"id":"1699f140-b9f3-5aaf-b462-9e99cda20297","excerpt":"Pytorch로 행렬을 미분해보자, autograd 딥러닝 이론에 대해서 공부 해 보신 분들은, 딥러닝의 핵심은 미분을 통해서 손실 함수(loss function)의 값을 최소화 하는 것을 알고 계실 겁니다. 그렇기 때문에 Deep Learning 연구 플랫폼인 에선, 당연하게도 행렬 미분을 위한 기능들을  객체에 내장 시켜 놓았습니다. 한 번 알아볼까요? autograd 활성화 시키기 requires_grad=True…","frontmatter":{"date":"2020-03-28","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-autograd","title":"Pytorch로 행렬을 미분해보자, autograd","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png","summary":"Pytorch로 행렬을 미분하는 방법"}}},{"node":{"id":"ad43d234-197a-52d8-89f2-d33c3a14f01f","excerpt":"주의 필자인 저 또한 배운 것들을 정리 하면서 쓰는 글이기 때문에, 틀린 부분이 있을 수도 있습니다. 또한, 이 글은 에 대한 기본 적인 배경 지식을 필요로 합니다. 오타, 지적사항 발생 시, 댓글 혹은 이메일로 남겨 주시면 감사하겠습니다! Pytorch Pytorch는 두 가지 목표를 달성하기 위해 만들어진 오픈 소스 라이브러리이자, Python Package입니다. 두 가지 목표는 다음과 같습니다. Numpy가 기존에 하던 연산들을 GPU…","frontmatter":{"date":"2020-03-23","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-basic","title":"Pytorch에 대해서 알아보자.araboza","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png","summary":"딥러닝 플랫폼인 Pytorch에 대하여"}}}]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"7e87fe30-83b7-55df-8ae3-9f21b97c5a73","excerpt":"Regularization 안녕하세요? 이번 시간에는 Regularization에 대해서 알아 보도록 하겠습니다. 우리가 모델을 만들 때, 많은 분들이 Overfitting을 경험 해 보셨을 것 입니다. 여러분들은 Overfitting을 경험할 때, 다음과 같은 것들을 시도 해 볼 것입니다. 모델의 크기를 줄인다. (네트워크의 깊이, 노드 갯수 등등) 데이터의 크기를 늘린다. 하지만, 이 외에도 다른 방법들이 있습니다. DropOut…","frontmatter":{"date":"2021-08-31","tags":["Data-Science","Deep-Learning"],"path":"/data-science/regularization","title":"Regularization: 모델의 과적합을 막는 방법","img":"/post_image/thumbnail/regularization.jpg","summary":"모델의 과적합을 막는 Regularization에 대해서 알아 보자."}}},{"node":{"id":"bc05c946-5ffd-5779-9e8b-0a3b4c81c348","excerpt":"Generative Adversarial Network 안녕하세요? 오늘은 GAN, Generative Adversarial Network에 대해서 알아 보도록 하겠습니다. Generative Adversarial Network…","frontmatter":{"date":"2021-08-07","tags":["Data-Science","Deep-Learning"],"path":"/data-science/just-data-science-12","title":"[찍먹 Data Science] 12. Generative Adversarial Network","img":"/post_image/thumbnail/just-data-science-12.jpg","summary":"서로가 적대적으로 학습하는 GAN에 대해서 알아 보자."}}},{"node":{"id":"47be900a-1c73-5e1d-bd91-6282bdeca3e2","excerpt":"Recurrent Neural Network 안녕하세요? 오늘은 RNN, Recurrent Neural Network에 대해서 알아 보도록 하겠습니다. RNN…","frontmatter":{"date":"2021-08-05","tags":["Data-Science","Deep-Learning"],"path":"/data-science/just-data-science-11","title":"[찍먹 Data Science] 11. Recurrent Neural Network","img":"/post_image/thumbnail/just-data-science-11.jpg","summary":"시계열 데이터를 처리하는 RNN을 알아보자."}}},{"node":{"id":"f17dd469-00dc-52fc-84c7-51bd9e60e9e7","excerpt":"Convolutional Neural Network 안녕하세요? 오늘은 CNN, Convolutional Neural Network에 대해서 알아 보도록 하겠습니다. 저번 시간에는 DNN…","frontmatter":{"date":"2021-07-31","tags":["Data-Science","Deep-Learning"],"path":"/data-science/just-data-science-10","title":"[찍먹 Data Science] 10. Convolutional Neural Network","img":"/post_image/thumbnail/just-data-science-10.jpg","summary":"국소적인 정보를 추출하는 CNN에 대해 알아 보자."}}}]}}}}},"staticQueryHashes":["234633779","63159454"]}