{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/deep-learning/pytorch-basic","result":{"data":{"markdownRemark":{"html":"<h2>주의</h2>\n<p>필자인 저 또한 배운 것들을 정리 하면서 쓰는 글이기 때문에, 틀린 부분이 있을 수도 있습니다. 또한, 이 글은 <code class=\"language-text\">Numpy</code>에 대한 기본 적인 배경 지식을 필요로 합니다. <strong>오타, 지적사항</strong> 발생 시, 댓글 혹은 이메일로 남겨 주시면 감사하겠습니다!</p>\n<h1>Pytorch</h1>\n<p><strong>Pytorch</strong>는 두 가지 목표를 달성하기 위해 만들어진 <strong>오픈 소스 라이브러리</strong>이자, <strong>Python Package</strong>입니다. 두 가지 목표는 다음과 같습니다.</p>\n<ul>\n<li><strong>Numpy</strong>가 기존에 하던 연산들을 <strong>GPU</strong>로 대체하기 위함.</li>\n<li><strong>높은 유연성과 속도</strong>를 제공하는 <strong>Deep Learning 연구 플랫폼</strong></li>\n</ul>\n<p><strong>Facebook 인공지능 연구 팀</strong>이 개발 하였으며, <strong>러닝 커브가 낮고, 코드 가독성</strong> 또한 좋은 편입니다. 실시간 결과 값을 <strong>시각화</strong>도 가능할 뿐더러, 기계 학습 과정을 보며, 수학적으로 이해하는 데 상당히 유용하죠. 자, 한번 <strong>Pytorch</strong>의 기본 문법을 알아 볼까요?</p>\n<p align=\"center\">\n\t<img src=\"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png\" width=\"30%\" />\n\t<div style=\"text-align: center; color: #aaaaaa; font-size: 12px;\">\n\t\tPytorch 로고\n\t</div>\n</p>\n<br>\n<h2>Installation</h2>\n<p><strong>Python3, pip</strong>이 설치된 상태에서 터미널에 해당 명령어를 입력 해 주세요.</p>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">pip install torch===1.4.0 torchvision===0.5.0 -f https://download.pytorch.org/whl/torch_stable.html</code></pre></div>\n<br>\n<h2>Tensor</h2>\n<p><code class=\"language-text\">Tensor</code> 객체는 <code class=\"language-text\">Numpy</code>의 <code class=\"language-text\">ndarray</code>와 상당히 유사한 구조를 가집니다. 애초에 <code class=\"language-text\">Pytorch</code>가 <code class=\"language-text\">Numpy</code>를 대체하기 위해서 나왔기 때문이죠, 일단 <code class=\"language-text\">torch</code> 모듈을 <code class=\"language-text\">import</code> 해 볼까요?</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch</code></pre></div>\n<br>\n<h3>Make some Tensor Objects</h3>\n<p>그 다음, <code class=\"language-text\">Tensor</code> 객체를 만드는 여러 가지 방법들을 알려 드리겠습니다.</p>\n<ul>\n<li><code class=\"language-text\">torch.empty(x, y)</code>: x * y 사이즈의 요소들의 값이 초기화 되지 않은 행렬 반환.</li>\n<li><code class=\"language-text\">torch.rand(x, y)</code>: x * y 사이즈의 요소들이 <strong>0 ~ 1 사이의 랜덤한 값</strong>으로 초기화 된 행렬 반환.</li>\n<li><code class=\"language-text\">torch.randn(x, y)</code>: x * y 사이즈의 요소들이 <strong>정규분포 그래프</strong> 상의 랜덤한 값으로 초기화 된 행렬 반환.</li>\n<li><code class=\"language-text\">torch.zeros(x, y, dtype=type)</code>: x * y 사이즈의 요소들이 <strong>0으로 초기화</strong> 된 행렬 반환, 요소들은 type에 맞게 초기화 된다.</li>\n<li><code class=\"language-text\">torch.ones(x, y, dtype=type)</code>: x * y 사이즈의 요소들이 <strong>1으로 초기화</strong> 된 행렬 반환, 요소들은 type에 맞게 초기화 된다.</li>\n<li><code class=\"language-text\">torch.tensor(iterable)</code>: <code class=\"language-text\">iterable</code>한 객체를 <code class=\"language-text\">Tensor</code> 객체로 변환한다.</li>\n<li><code class=\"language-text\">torch.zeros_like(tensor, dtype=type)</code>: 파라미터로 들어 간 <code class=\"language-text\">Tensor</code> 객체의 사이즈과 똑같은 행렬을 반환하며, 요소들은 <strong>0으로 초기화</strong> 되어 있다.</li>\n<li><code class=\"language-text\">torch.ones_like(tensor, dtype=type)</code>: 파라미터로 들어 간 <code class=\"language-text\">Tensor</code> 객체의 사이즈과 똑같은 행렬을 반환하며, 요소들은 <strong>1으로 초기화</strong> 되어 있다.</li>\n<li><code class=\"language-text\">torch.randn_like(tensor, dtype=type)</code>: 파라미터로 들어 간 <code class=\"language-text\">Tensor</code> 객체의 사이즈과 똑같은 행렬을 반환하며, 요소들은 <strong>정규분포 그래프</strong> 상의 랜덤한 값으로 초기화 되어 있다.</li>\n</ul>\n<br>\n<h4>Code implementation</h4>\n<ul>\n<li><strong>입력</strong></li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">empty_tensor <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>empty<span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span>\t\t\t\t\t\t\t\t\t\t\t<span class=\"token comment\"># 3 * 3의 빈 행렬 생성</span>\nrand_tensor <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>rand<span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span>\t\t\t\t\t\t\t\t\t\t\t\t<span class=\"token comment\"># 3 * 3의 요소들이 0 ~ 1의 랜덤 값으로 초기화된 행렬 생성</span>\nrandn_tensor <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>torch<span class=\"token punctuation\">.</span>double<span class=\"token punctuation\">)</span>\t\t\t\t\t\t<span class=\"token comment\"># 3 * 3의 요소들이 정규분포 그래프 값으로 초기화된 행렬 생성</span>\nzero_tensor <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>torch<span class=\"token punctuation\">.</span><span class=\"token builtin\">long</span><span class=\"token punctuation\">)</span>\t\t\t\t\t\t\t<span class=\"token comment\"># 3 * 3의 요소들이 0으로 초기화된 행렬 생성</span>\none_tensor <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones<span class=\"token punctuation\">(</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>torch<span class=\"token punctuation\">.</span>double<span class=\"token punctuation\">)</span>\t\t\t\t\t\t\t<span class=\"token comment\"># 3 * 3의 요소들이 1으로 초기화된 행렬 생성</span>\niterable_tensor <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>tensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\t\t\t\t\t\t\t\t\t<span class=\"token comment\"># list 객체를 Tensor 객체로 변환</span>\nzeros_like_tensor <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>zeros_like<span class=\"token punctuation\">(</span>iterable_tensor<span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>torch<span class=\"token punctuation\">.</span>double<span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># iterable_tensor와 사이즈가 같은, 요소들이 0으로 초기화된 행렬 생성</span>\nones_like_tensor <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>ones_like<span class=\"token punctuation\">(</span>iterable_tensor<span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>torch<span class=\"token punctuation\">.</span>double<span class=\"token punctuation\">)</span>\t\t<span class=\"token comment\"># iterable_tensor와 사이즈가 같은, 요소들이 1으로 초기화된 행렬 생성</span>\nrandn_like_tensor <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randn_like<span class=\"token punctuation\">(</span>iterable_tensor<span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span>torch<span class=\"token punctuation\">.</span>double<span class=\"token punctuation\">)</span>\t<span class=\"token comment\"># iterable_tensor와 사이즈가 같은, 요소들이 정규분포 그래프 값으로 초기화된 행렬 생성</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>empty_tensor<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>rand_tensor<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>randn_tensor<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>zero_tensor<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>one_tensor<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>iterable_tensor<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>zeros_like_tensor<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>ones_like_tensor<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>randn_like_tensor<span class=\"token punctuation\">)</span></code></pre></div>\n<br>\n<ul>\n<li><strong>출력</strong></li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">tensor([[1.2045e-35, 0.0000e+00, 4.4842e-44],\n        [0.0000e+00,        nan, 6.1657e-44],\n        [4.3722e-05, 5.2475e-08, 1.7662e-04]])\ntensor([[0.8250, 0.8530, 0.6120],\n        [0.4726, 0.9426, 0.7616],\n        [0.5276, 0.1977, 0.1966]])\ntensor([[-0.1279, -0.9227,  0.0434],\n        [-0.2085,  0.1541, -1.8450],\n        [-0.7687,  0.0956, -0.6723]], dtype=torch.float64)\ntensor([[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]])\ntensor([[1., 1., 1.],\n        [1., 1., 1.],\n        [1., 1., 1.]], dtype=torch.float64)\ntensor([1, 2, 3])\ntensor([0., 0., 0.], dtype=torch.float64)\ntensor([1., 1., 1.], dtype=torch.float64)\ntensor([ 0.5920, -0.2960, -0.2184], dtype=torch.float64)</code></pre></div>\n<br>\n<h3>Operations</h3>\n<p>당연히, <code class=\"language-text\">Tensor</code> 객체의 연산을 위한 <strong>연산자, 인덱싱</strong> 또한 준비 되어 있습니다.</p>\n<h4>Code Implementation</h4>\n<p>일단 기본적인 <strong>연산자</strong>는 다음과 같이 사용 할 수 있습니다. 또한, <code class=\"language-text\">Numpy</code>에서 사용 했던, <strong>차원별 인덱싱</strong>과 <strong>브로드캐스팅</strong>이 가능합니다.</p>\n<ul>\n<li><strong>입력</strong></li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">x_tensor <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>tensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span> <span class=\"token comment\"># 2 * 2 행렬 생성</span>\ny_tensor <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>tensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">6</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">7</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nz_tensor <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>tensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x_tensor<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>y_tensor<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x_tensor <span class=\"token operator\">+</span> y_tensor<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># Index가 일치하는 요소 끼리 덧셈</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x_tensor <span class=\"token operator\">-</span> y_tensor<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># Index가 일치하는 요소 끼리 뺄셈</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x_tensor <span class=\"token operator\">*</span> y_tensor<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># Index가 일치하는 요소 끼리 곱셈</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x_tensor @ y_tensor<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 행렬 곱</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x_tensor <span class=\"token operator\">*</span> <span class=\"token number\">3</span><span class=\"token punctuation\">)</span>         <span class=\"token comment\"># x_tensor 각 요소에 3을 곱해줌</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>x_tensor <span class=\"token operator\">+</span> z_tensor<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 일치하는 요소에 브로드캐스팅</span>\n\na_tensor <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>tensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">6</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nb_tensor <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>tensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">6</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>a_tensor<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>b_tensor<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>a_tensor @ b_tensor<span class=\"token punctuation\">)</span>  <span class=\"token comment\"># 2 * 3 행렬과 3 * 2 행렬 곱셈</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>a_tensor<span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>       <span class=\"token comment\"># 각 행의 1번째 열 추출</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>b_tensor<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">:</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>       <span class=\"token comment\"># 1번째 행의 모든 열 추출</span></code></pre></div>\n<br>\n<ul>\n<li><strong>출력</strong></li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">tensor([[1, 2],\n        [3, 4]])\ntensor([[5, 6],\n        [7, 8]])\ntensor([[ 6,  8],\n        [10, 12]])\ntensor([[-4, -4],\n        [-4, -4]])\ntensor([[ 5, 12],\n        [21, 32]])\ntensor([[19, 22],\n        [43, 50]])\ntensor([[ 3,  6],\n        [ 9, 12]])\ntensor([[[2, 4],\n         [6, 8]],\n\n        [[2, 4],\n         [6, 8]]])\ntensor([[1, 2, 3],\n        [4, 5, 6]])\ntensor([[1, 2],\n        [3, 4],\n        [5, 6]])\ntensor([[22, 28],\n        [49, 64]])\ntensor([2, 5])\ntensor([3, 4])</code></pre></div>\n<p><br><br></p>\n<h3>Tensor Resize</h3>\n<p><strong>딥러닝 모델</strong>을 설계하다 보면 <strong>행렬 사이즈를 재조정</strong> 해야 하는 경우가 상당히 많습니다. 이를 위한 <strong>몇 가지 함수</strong>를 소개 시켜 드리겠습니다.</p>\n<ul>\n<li><code class=\"language-text\">Tensor.size()</code>: <code class=\"language-text\">Tensor</code> 객체의 <strong>사이즈를 반환</strong> 한다.</li>\n<li><code class=\"language-text\">Tensor.view(size)</code>: 파라미터로 들어간 사이즈로 <code class=\"language-text\">Tensor</code> 객체의 <strong>사이즈를 변환</strong> 시켜 주며, 파라미터로 -1이 들어갈 시, <strong>행렬의 차원 수를 낮춰</strong> 리사이징 하며, (-1, n)이 들어가면 가장 <strong>하위 차원에서 n개씩 끊어 넣는 방식</strong>으로 리사이징 한다. <code class=\"language-text\">numpy</code>의 <code class=\"language-text\">resize</code>와 방법이 유사하다.</li>\n</ul>\n<h4>Code Implementation</h4>\n<ul>\n<li><strong>입력</strong></li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">a_tensor <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>tensor<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>      <span class=\"token comment\"># 4 * 4 행렬</span>\n                         <span class=\"token punctuation\">[</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> <span class=\"token number\">6</span><span class=\"token punctuation\">,</span> <span class=\"token number\">7</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n                         <span class=\"token punctuation\">[</span><span class=\"token number\">9</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">,</span> <span class=\"token number\">11</span><span class=\"token punctuation\">,</span> <span class=\"token number\">12</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n                         <span class=\"token punctuation\">[</span><span class=\"token number\">13</span><span class=\"token punctuation\">,</span> <span class=\"token number\">14</span><span class=\"token punctuation\">,</span> <span class=\"token number\">15</span><span class=\"token punctuation\">,</span> <span class=\"token number\">16</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>a_tensor<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>a_tensor<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nb_tensor <span class=\"token operator\">=</span> a_tensor<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span><span class=\"token number\">16</span><span class=\"token punctuation\">)</span>                <span class=\"token comment\"># 사이즈가 16 array</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>b_tensor<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>b_tensor<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nc_tensor <span class=\"token operator\">=</span> a_tensor<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">)</span>             <span class=\"token comment\"># 4 * 4 => 2 * 8</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>c_tensor<span class=\"token punctuation\">)</span>                             <span class=\"token comment\"># 6 * 4 일시 3 * 8 으로 리사이징 된다.</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>c_tensor<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\nd_tensor <span class=\"token operator\">=</span> a_tensor<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>                <span class=\"token comment\"># 4 * 4 => 16</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>d_tensor<span class=\"token punctuation\">)</span>                             <span class=\"token comment\"># 6 * 4 일시 24로 리사이징 된다.</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>d_tensor<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\ne_tensor <span class=\"token operator\">=</span> a_tensor<span class=\"token punctuation\">.</span>view<span class=\"token punctuation\">(</span><span class=\"token number\">8</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span>              <span class=\"token comment\"># 8 * 2 행렬로 사이즈 변환</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>e_tensor<span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>e_tensor<span class=\"token punctuation\">.</span>size<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<br>\n<ul>\n<li><strong>출력</strong></li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">tensor([[ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12],\n        [13, 14, 15, 16]])\ntorch.Size([4, 4])\ntensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])\ntorch.Size([16])\ntensor([[ 1,  2,  3,  4,  5,  6,  7,  8],\n        [ 9, 10, 11, 12, 13, 14, 15, 16]])\ntorch.Size([2, 8])\ntensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16])\ntorch.Size([16])\ntensor([[ 1,  2],\n        [ 3,  4],\n        [ 5,  6],\n        [ 7,  8],\n        [ 9, 10],\n        [11, 12],\n        [13, 14],\n        [15, 16]])\ntorch.Size([8, 2])</code></pre></div>\n<p><br><br></p>\n<h3>Numpy Bridges</h3>\n<p><code class=\"language-text\">Pytorch</code>는 <code class=\"language-text\">Numpy</code>연산을 GPU를 이용하여, 더욱 빠르게 하기 위해 탄생했습니다. 그렇기 때문에 <code class=\"language-text\">Numpy</code>와 <code class=\"language-text\">Pytorch</code>를 연동할 수 있도록, <code class=\"language-text\">Pytorch</code>에서는 <code class=\"language-text\">API</code>를 제공합니다.</p>\n<ul>\n<li><code class=\"language-text\">Tensor.numpy()</code>: <code class=\"language-text\">Tensor</code> 객체를 <code class=\"language-text\">numpy.ndarray</code> 객체로 변환 하여 반환합니다.</li>\n<li><code class=\"language-text\">torch.from_numpy(ndarray)</code>: <code class=\"language-text\">numpy.ndarray</code>를 <code class=\"language-text\">Tensor</code> 객체로 변환 하여 반환합니다.</li>\n</ul>\n<h4>Code Implementation</h4>\n<ul>\n<li><strong>입력</strong></li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">import</span> torch\n\na_matrix <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>   <span class=\"token comment\"># np.ndarray 객체 할당</span>\nb_matrix <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>from_numpy<span class=\"token punctuation\">(</span>a_matrix<span class=\"token punctuation\">)</span>   <span class=\"token comment\"># np.ndarray 객체를 이용하여 Tensor 객체 할당</span>\nc_matrix <span class=\"token operator\">=</span> b_matrix<span class=\"token punctuation\">.</span>numpy<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>             <span class=\"token comment\"># Tensor 객체를 이용하여 np.ndarray 객체 할당</span>\n\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>a_matrix<span class=\"token punctuation\">)</span>                         <span class=\"token comment\"># np.ndarray</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>b_matrix<span class=\"token punctuation\">)</span>                         <span class=\"token comment\"># Tensor</span>\n<span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>c_matrix<span class=\"token punctuation\">)</span>                         <span class=\"token comment\"># np.ndarray</span></code></pre></div>\n<br>\n<ul>\n<li><strong>출력</strong></li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"terminal\"><pre class=\"language-terminal\"><code class=\"language-terminal\">[[1 2]\n [3 4]]\ntensor([[1, 2],\n        [3, 4]])\n[[1 2]\n [3 4]]</code></pre></div>\n<p><br><br></p>\n<h2>마치며</h2>\n<p><code class=\"language-text\">Pytorch</code>는 <code class=\"language-text\">Numpy</code>와 <code class=\"language-text\">API</code> 형태가 상당히 유사합니다. 하지만, <code class=\"language-text\">Numpy</code>와 다른 점이 있다면, <code class=\"language-text\">Backpropagation</code>을 쉽게 구현할 수 있고, <code class=\"language-text\">loss function</code>, <code class=\"language-text\">optimizer</code> 등 많은 고수준 <code class=\"language-text\">API</code>를 제공합니다. 다음 시간에는 <strong>Pytorch로 미분하기, autograd</strong>에 대해서 알아 보겠습니다.</p>\n<h3>Reference</h3>\n<ul>\n<li><a href=\"https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html\">Pytorch Tutorial</a></li>\n</ul>","id":"897e816a-7e4e-5fc4-8936-e49103bc1359","frontmatter":{"date":"2020-03-23","path":"/deep-learning/pytorch-basic","title":"Pytorch에 대해서 알아보자.araboza","tags":["Deep-Learning","Python"],"keyword":"Python, python, 파이썬, Pytorch, 파이토치, pytorch, 딥러닝","summary":"딥러닝 플랫폼인 Pytorch에 대하여","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png","series":"Pytorch Basic"}}},"pageContext":{"series":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"a2b924e9-9836-58ae-b3ae-4e480268cbcf","excerpt":"에서 모델의 가중치를 저장하기 위해선 3가지 함수만 알면 충분 합니다. : 객체를 디스크에 저장합니다.  모듈을 이용하여 객체를 직렬화 하며, 이 함수를 사용하여 모든 종류의 모델, Tensor 등을 저장할 수 있습니다. :  모듈을 이용하여 객체를 역직렬화하여 메모리에 할당합니다. : 역직렬화된 state_dict를 사용, 모델의 매개변수들을 불러옵니다. state_dict는 간단히 말해 각 체층을 매개변수 Tensor로 매핑한 Python…","frontmatter":{"date":"2020-04-26","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-save","title":"Pytorch에서 학습한 모델 저장 및 불러오기","img":"/post_image/pytorch-save.png","summary":"Pytorch 모델을 저장하고, 불러와 보기"}}},{"node":{"id":"fd604893-7ec3-53e1-8173-e5ca5c9be50b","excerpt":"에서는 과 마찬가지로, 과 관련 된 를 제공합니다. 이를 이용해 손쉽게  네트워크를 구축 할 수 있습니다. Recurrent Neural Network 를 위한 는  입니다. 일단  시퀀스의 각 요소에 대해, 각 레이어에서는 다음 연산을 수행합니다.  Parameters : 의 사이즈에 해당 하는 수를 입력하면 됩니다. : 은닉층의 사이즈에 해당 하는 수를 입력하면 됩니다. : 의 은닉층 레이어 갯수를 나타냅니다. 기본 값은…","frontmatter":{"date":"2020-04-19","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-rnn","title":"Pytorch로 RNN, LSTM 구현하기","img":"https://lionbridge.ai/wp-content/uploads/2020/03/2020-02-21_difference-between-cnn-rnn-1.png","summary":"자연어 처리, 감성 분류 등에 사용되는 RNN, LSTM"}}},{"node":{"id":"21ec001c-601e-5e4b-8ebc-f2cb41862240","excerpt":"CNN In Pytorch 에는 을 개발 하기 위한 들이 있습니다. 다채널로 구현 되어 있는 CNN 신경망을 위한 Layers, Max pooling, Avg pooling 등, 이번 시간에는 여러 가지 을 위한 를 알아 보겠습니다. 또한,  데이터 또한 학습 해 보겠습니다. Convolution Layers  연산을 위한 레이어들은 다음과 같습니다. Conv1d (Text-CNN에서 많이 사용) Conv2d…","frontmatter":{"date":"2020-04-08","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-cnn","title":"Pytorch로 CNN 구현하기","img":"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/02/Plot-of-a-Subset-of-Images-from-the-MNIST-Dataset-1024x768.png","summary":"Pytorch로 MNIST 그림 식별을 해보자."}}},{"node":{"id":"845b1f8c-db78-5c2d-9289-3248d0069fee","excerpt":"Neural Network in Pytorch 딥 러닝 프레임워크인 에선 신경망 구축을 위한 API…","frontmatter":{"date":"2020-04-04","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-nn","title":"Pytorch의 Neural Network","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png","summary":"Pytorch에서 신경망 구현하기"}}},{"node":{"id":"2f7903e5-090d-578d-a8b7-3f696d116862","excerpt":"Pytorch로 행렬을 미분해보자, autograd 딥러닝 이론에 대해서 공부 해 보신 분들은, 딥러닝의 핵심은 미분을 통해서 손실 함수(loss function)의 값을 최소화 하는 것을 알고 계실 겁니다. 그렇기 때문에 Deep Learning 연구 플랫폼인 에선, 당연하게도 행렬 미분을 위한 기능들을  객체에 내장 시켜 놓았습니다. 한 번 알아볼까요? autograd 활성화 시키기 requires_grad=True…","frontmatter":{"date":"2020-03-28","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-autograd","title":"Pytorch로 행렬을 미분해보자, autograd","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png","summary":"Pytorch로 행렬을 미분하는 방법"}}},{"node":{"id":"897e816a-7e4e-5fc4-8936-e49103bc1359","excerpt":"주의 필자인 저 또한 배운 것들을 정리 하면서 쓰는 글이기 때문에, 틀린 부분이 있을 수도 있습니다. 또한, 이 글은 에 대한 기본 적인 배경 지식을 필요로 합니다. 오타, 지적사항 발생 시, 댓글 혹은 이메일로 남겨 주시면 감사하겠습니다! Pytorch Pytorch는 두 가지 목표를 달성하기 위해 만들어진 오픈 소스 라이브러리이자, Python Package입니다. 두 가지 목표는 다음과 같습니다. Numpy가 기존에 하던 연산들을 GPU…","frontmatter":{"date":"2020-03-23","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-basic","title":"Pytorch에 대해서 알아보자.araboza","img":"https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/challenge_thumbnails/000/822/053/datas/original.png","summary":"딥러닝 플랫폼인 Pytorch에 대하여"}}}]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"edbb5b70-1341-56dc-a484-10a4bee18986","excerpt":"Regularization 안녕하세요? 이번 시간에는 Regularization에 대해서 알아 보도록 하겠습니다. 우리가 모델을 만들 때, 많은 분들이 Overfitting을 경험 해 보셨을 것 입니다. 여러분들은 Overfitting을 경험할 때, 다음과 같은 것들을 시도 해 볼 것입니다. 모델의 크기를 줄인다. (네트워크의 깊이, 노드 갯수 등등) 데이터의 크기를 늘린다. 하지만, 이 외에도 다른 방법들이 있습니다. DropOut…","frontmatter":{"date":"2021-08-31","tags":["Data-Science","Deep-Learning"],"path":"/data-science/regularization","title":"Regularization: 모델의 과적합을 막는 방법","img":"/post_image/thumbnail/regularization.jpg","summary":"모델의 과적합을 막는 Regularization에 대해서 알아 보자."}}},{"node":{"id":"4bb77c07-a193-5691-a51c-fa1377db8569","excerpt":"Generative Adversarial Network 안녕하세요? 오늘은 GAN, Generative Adversarial Network에 대해서 알아 보도록 하겠습니다. Generative Adversarial Network…","frontmatter":{"date":"2021-08-07","tags":["Data-Science","Deep-Learning"],"path":"/data-science/just-data-science-12","title":"[찍먹 Data Science] 12. Generative Adversarial Network","img":"/post_image/thumbnail/just-data-science-12.jpg","summary":"서로가 적대적으로 학습하는 GAN에 대해서 알아 보자."}}},{"node":{"id":"8a0ee69e-b362-55d6-9ac0-8c9e8861f475","excerpt":"Recurrent Neural Network 안녕하세요? 오늘은 RNN, Recurrent Neural Network에 대해서 알아 보도록 하겠습니다. RNN…","frontmatter":{"date":"2021-08-05","tags":["Data-Science","Deep-Learning"],"path":"/data-science/just-data-science-11","title":"[찍먹 Data Science] 11. Recurrent Neural Network","img":"/post_image/thumbnail/just-data-science-11.jpg","summary":"시계열 데이터를 처리하는 RNN을 알아보자."}}},{"node":{"id":"ffe46d73-d05b-5d3f-a9ea-1f6b57ef50ba","excerpt":"Convolutional Neural Network 안녕하세요? 오늘은 CNN, Convolutional Neural Network에 대해서 알아 보도록 하겠습니다. 저번 시간에는 DNN…","frontmatter":{"date":"2021-07-31","tags":["Data-Science","Deep-Learning"],"path":"/data-science/just-data-science-10","title":"[찍먹 Data Science] 10. Convolutional Neural Network","img":"/post_image/thumbnail/just-data-science-10.jpg","summary":"국소적인 정보를 추출하는 CNN에 대해 알아 보자."}}}]}}}}},"staticQueryHashes":["234633779","63159454"]}