{"componentChunkName":"component---src-layouts-post-layout-tsx","path":"/deep-learning/word2vec-1","result":{"data":{"markdownRemark":null},"pageContext":{"series":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"40d74188-dcb9-56d3-88d2-8cd1aa282587","excerpt":"당신이 수 십만개의 언어를 수 십만 차원의 One-Hot Vector로 표현 할 것이 아니라면, 단어의 의미를 담을 수 있도록 저차원의 벡터로 임베딩(단어를 수치화 하는 과정) 하는 것은 필수적일 것입니다. Word2Vec…","frontmatter":{"date":"2020-07-01","tags":["Deep-Learning"],"path":"/deep-learning/word2vec-1","title":"[Word2Vec] 1. CBOW, Skip-gram에 대하여","img":"https://humboldt-wi.github.io/blog/img/seminar/topic_models/oprah.png","summary":"Word2Vec 모델인 CBOW, Skip-gram의 기본"}}}]}}},"categoryPosts":{"data":{"allMarkdownRemark":{"edges":[{"node":{"id":"40d74188-dcb9-56d3-88d2-8cd1aa282587","excerpt":"당신이 수 십만개의 언어를 수 십만 차원의 One-Hot Vector로 표현 할 것이 아니라면, 단어의 의미를 담을 수 있도록 저차원의 벡터로 임베딩(단어를 수치화 하는 과정) 하는 것은 필수적일 것입니다. Word2Vec…","frontmatter":{"date":"2020-07-01","tags":["Deep-Learning"],"path":"/deep-learning/word2vec-1","title":"[Word2Vec] 1. CBOW, Skip-gram에 대하여","img":"https://humboldt-wi.github.io/blog/img/seminar/topic_models/oprah.png","summary":"Word2Vec 모델인 CBOW, Skip-gram의 기본"}}},{"node":{"id":"3e5a78d9-ea4d-5a77-a082-281403358a61","excerpt":"에서 모델의 가중치를 저장하기 위해선 3가지 함수만 알면 충분 합니다. : 객체를 디스크에 저장합니다.  모듈을 이용하여 객체를 직렬화 하며, 이 함수를 사용하여 모든 종류의 모델, Tensor 등을 저장할 수 있습니다. :  모듈을 이용하여 객체를 역직렬화하여 메모리에 할당합니다. : 역직렬화된 state_dict를 사용, 모델의 매개변수들을 불러옵니다. state_dict는 간단히 말해 각 체층을 매개변수 Tensor로 매핑한 Python…","frontmatter":{"date":"2020-04-26","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-save","title":"Pytorch에서 학습한 모델 저장 및 불러오기","img":"/post_image/pytorch-save.png","summary":"Pytorch 모델을 저장하고, 불러와 보기"}}},{"node":{"id":"ce0ab714-91d8-5592-afc1-72bb2a4c56c4","excerpt":"에서는 과 마찬가지로, 과 관련 된 를 제공합니다. 이를 이용해 손쉽게  네트워크를 구축 할 수 있습니다. Recurrent Neural Network 를 위한 는  입니다. 일단  시퀀스의 각 요소에 대해, 각 레이어에서는 다음 연산을 수행합니다.  Parameters : 의 사이즈에 해당 하는 수를 입력하면 됩니다. : 은닉층의 사이즈에 해당 하는 수를 입력하면 됩니다. : 의 은닉층 레이어 갯수를 나타냅니다. 기본 값은…","frontmatter":{"date":"2020-04-19","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-rnn","title":"Pytorch로 RNN, LSTM 구현하기","img":"https://lionbridge.ai/wp-content/uploads/2020/03/2020-02-21_difference-between-cnn-rnn-1.png","summary":"자연어 처리, 감성 분류 등에 사용되는 RNN, LSTM"}}},{"node":{"id":"ddf2f489-870b-5d07-a72f-68cefd3da0c4","excerpt":"CNN In Pytorch 에는 을 개발 하기 위한 들이 있습니다. 다채널로 구현 되어 있는 CNN 신경망을 위한 Layers, Max pooling, Avg pooling 등, 이번 시간에는 여러 가지 을 위한 를 알아 보겠습니다. 또한,  데이터 또한 학습 해 보겠습니다. Convolution Layers  연산을 위한 레이어들은 다음과 같습니다. Conv1d (Text-CNN에서 많이 사용) Conv2d…","frontmatter":{"date":"2020-04-08","tags":["Deep-Learning","Python"],"path":"/deep-learning/pytorch-cnn","title":"Pytorch로 CNN 구현하기","img":"https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/02/Plot-of-a-Subset-of-Images-from-the-MNIST-Dataset-1024x768.png","summary":"Pytorch로 MNIST 그림 식별을 해보자."}}}]}}}}}}